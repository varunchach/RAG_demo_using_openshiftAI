{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28f5e67b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: llama-index-readers-docling in ./.venv/lib/python3.11/site-packages (0.4.0)\n",
      "Requirement already satisfied: docling-core<3,>=2.2.0 in ./.venv/lib/python3.11/site-packages (from llama-index-readers-docling) (2.44.2)\n",
      "Requirement already satisfied: docling<3,>=2.2.0 in ./.venv/lib/python3.11/site-packages (from llama-index-readers-docling) (2.44.0)\n",
      "Requirement already satisfied: llama-index-core<0.14,>=0.13.0 in ./.venv/lib/python3.11/site-packages (from llama-index-readers-docling) (0.13.2)\n",
      "Requirement already satisfied: numpy>=2.0 in ./.venv/lib/python3.11/site-packages (from llama-index-readers-docling) (2.2.6)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.0.0 in ./.venv/lib/python3.11/site-packages (from docling<3,>=2.2.0->llama-index-readers-docling) (2.11.7)\n",
      "Requirement already satisfied: docling-parse<5.0.0,>=4.0.0 in ./.venv/lib/python3.11/site-packages (from docling<3,>=2.2.0->llama-index-readers-docling) (4.1.0)\n",
      "Requirement already satisfied: docling-ibm-models<4,>=3.9.0 in ./.venv/lib/python3.11/site-packages (from docling<3,>=2.2.0->llama-index-readers-docling) (3.9.0)\n",
      "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in ./.venv/lib/python3.11/site-packages (from docling<3,>=2.2.0->llama-index-readers-docling) (1.2.0)\n",
      "Requirement already satisfied: pypdfium2!=4.30.1,<5.0.0,>=4.30.0 in ./.venv/lib/python3.11/site-packages (from docling<3,>=2.2.0->llama-index-readers-docling) (4.30.0)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.3.0 in ./.venv/lib/python3.11/site-packages (from docling<3,>=2.2.0->llama-index-readers-docling) (2.10.1)\n",
      "Requirement already satisfied: huggingface_hub<1,>=0.23 in ./.venv/lib/python3.11/site-packages (from docling<3,>=2.2.0->llama-index-readers-docling) (0.34.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.32.2 in ./.venv/lib/python3.11/site-packages (from docling<3,>=2.2.0->llama-index-readers-docling) (2.32.4)\n",
      "Requirement already satisfied: easyocr<2.0,>=1.7 in ./.venv/lib/python3.11/site-packages (from docling<3,>=2.2.0->llama-index-readers-docling) (1.7.2)\n",
      "Requirement already satisfied: certifi>=2024.7.4 in ./.venv/lib/python3.11/site-packages (from docling<3,>=2.2.0->llama-index-readers-docling) (2025.8.3)\n",
      "Requirement already satisfied: rtree<2.0.0,>=1.3.0 in ./.venv/lib/python3.11/site-packages (from docling<3,>=2.2.0->llama-index-readers-docling) (1.4.1)\n",
      "Requirement already satisfied: typer<0.17.0,>=0.12.5 in ./.venv/lib/python3.11/site-packages (from docling<3,>=2.2.0->llama-index-readers-docling) (0.16.0)\n",
      "Requirement already satisfied: python-docx<2.0.0,>=1.1.2 in ./.venv/lib/python3.11/site-packages (from docling<3,>=2.2.0->llama-index-readers-docling) (1.2.0)\n",
      "Requirement already satisfied: python-pptx<2.0.0,>=1.0.2 in ./.venv/lib/python3.11/site-packages (from docling<3,>=2.2.0->llama-index-readers-docling) (1.0.2)\n",
      "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in ./.venv/lib/python3.11/site-packages (from docling<3,>=2.2.0->llama-index-readers-docling) (4.13.4)\n",
      "Requirement already satisfied: pandas<3.0.0,>=2.1.4 in ./.venv/lib/python3.11/site-packages (from docling<3,>=2.2.0->llama-index-readers-docling) (2.3.1)\n",
      "Requirement already satisfied: marko<3.0.0,>=2.1.2 in ./.venv/lib/python3.11/site-packages (from docling<3,>=2.2.0->llama-index-readers-docling) (2.2.0)\n",
      "Requirement already satisfied: openpyxl<4.0.0,>=3.1.5 in ./.venv/lib/python3.11/site-packages (from docling<3,>=2.2.0->llama-index-readers-docling) (3.1.5)\n",
      "Requirement already satisfied: lxml<6.0.0,>=4.0.0 in ./.venv/lib/python3.11/site-packages (from docling<3,>=2.2.0->llama-index-readers-docling) (5.4.0)\n",
      "Requirement already satisfied: pillow<12.0.0,>=10.0.0 in ./.venv/lib/python3.11/site-packages (from docling<3,>=2.2.0->llama-index-readers-docling) (11.3.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.65.0 in ./.venv/lib/python3.11/site-packages (from docling<3,>=2.2.0->llama-index-readers-docling) (4.67.1)\n",
      "Requirement already satisfied: pluggy<2.0.0,>=1.0.0 in ./.venv/lib/python3.11/site-packages (from docling<3,>=2.2.0->llama-index-readers-docling) (1.6.0)\n",
      "Requirement already satisfied: pylatexenc<3.0,>=2.10 in ./.venv/lib/python3.11/site-packages (from docling<3,>=2.2.0->llama-index-readers-docling) (2.10)\n",
      "Requirement already satisfied: scipy<2.0.0,>=1.6.0 in ./.venv/lib/python3.11/site-packages (from docling<3,>=2.2.0->llama-index-readers-docling) (1.16.1)\n",
      "Requirement already satisfied: accelerate<2,>=1.0.0 in ./.venv/lib/python3.11/site-packages (from docling<3,>=2.2.0->llama-index-readers-docling) (1.10.0)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.11/site-packages (from accelerate<2,>=1.0.0->docling<3,>=2.2.0->llama-index-readers-docling) (25.0)\n",
      "Requirement already satisfied: psutil in ./.venv/lib/python3.11/site-packages (from accelerate<2,>=1.0.0->docling<3,>=2.2.0->llama-index-readers-docling) (7.0.0)\n",
      "Requirement already satisfied: pyyaml in ./.venv/lib/python3.11/site-packages (from accelerate<2,>=1.0.0->docling<3,>=2.2.0->llama-index-readers-docling) (6.0.2)\n",
      "Requirement already satisfied: torch>=2.0.0 in ./.venv/lib/python3.11/site-packages (from accelerate<2,>=1.0.0->docling<3,>=2.2.0->llama-index-readers-docling) (2.8.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in ./.venv/lib/python3.11/site-packages (from accelerate<2,>=1.0.0->docling<3,>=2.2.0->llama-index-readers-docling) (0.6.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in ./.venv/lib/python3.11/site-packages (from beautifulsoup4<5.0.0,>=4.12.3->docling<3,>=2.2.0->llama-index-readers-docling) (2.7)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in ./.venv/lib/python3.11/site-packages (from beautifulsoup4<5.0.0,>=4.12.3->docling<3,>=2.2.0->llama-index-readers-docling) (4.14.1)\n",
      "Requirement already satisfied: jsonschema<5.0.0,>=4.16.0 in ./.venv/lib/python3.11/site-packages (from docling-core<3,>=2.2.0->llama-index-readers-docling) (4.25.0)\n",
      "Requirement already satisfied: jsonref<2.0.0,>=1.1.0 in ./.venv/lib/python3.11/site-packages (from docling-core<3,>=2.2.0->llama-index-readers-docling) (1.1.0)\n",
      "Requirement already satisfied: tabulate<0.10.0,>=0.9.0 in ./.venv/lib/python3.11/site-packages (from docling-core<3,>=2.2.0->llama-index-readers-docling) (0.9.0)\n",
      "Requirement already satisfied: latex2mathml<4.0.0,>=3.77.0 in ./.venv/lib/python3.11/site-packages (from docling-core<3,>=2.2.0->llama-index-readers-docling) (3.78.0)\n",
      "Requirement already satisfied: semchunk<3.0.0,>=2.2.0 in ./.venv/lib/python3.11/site-packages (from docling-core[chunking]<3.0.0,>=2.42.0->docling<3,>=2.2.0->llama-index-readers-docling) (2.2.2)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.34.0 in ./.venv/lib/python3.11/site-packages (from docling-core[chunking]<3.0.0,>=2.42.0->docling<3,>=2.2.0->llama-index-readers-docling) (4.55.2)\n",
      "Requirement already satisfied: torchvision<1,>=0 in ./.venv/lib/python3.11/site-packages (from docling-ibm-models<4,>=3.9.0->docling<3,>=2.2.0->llama-index-readers-docling) (0.23.0)\n",
      "Requirement already satisfied: jsonlines<4.0.0,>=3.1.0 in ./.venv/lib/python3.11/site-packages (from docling-ibm-models<4,>=3.9.0->docling<3,>=2.2.0->llama-index-readers-docling) (3.1.0)\n",
      "Requirement already satisfied: opencv-python-headless<5.0.0.0,>=4.6.0.66 in ./.venv/lib/python3.11/site-packages (from docling-ibm-models<4,>=3.9.0->docling<3,>=2.2.0->llama-index-readers-docling) (4.12.0.88)\n",
      "Requirement already satisfied: scikit-image in ./.venv/lib/python3.11/site-packages (from easyocr<2.0,>=1.7->docling<3,>=2.2.0->llama-index-readers-docling) (0.25.2)\n",
      "Requirement already satisfied: python-bidi in ./.venv/lib/python3.11/site-packages (from easyocr<2.0,>=1.7->docling<3,>=2.2.0->llama-index-readers-docling) (0.6.6)\n",
      "Requirement already satisfied: Shapely in ./.venv/lib/python3.11/site-packages (from easyocr<2.0,>=1.7->docling<3,>=2.2.0->llama-index-readers-docling) (2.1.1)\n",
      "Requirement already satisfied: pyclipper in ./.venv/lib/python3.11/site-packages (from easyocr<2.0,>=1.7->docling<3,>=2.2.0->llama-index-readers-docling) (1.3.0.post6)\n",
      "Requirement already satisfied: ninja in ./.venv/lib/python3.11/site-packages (from easyocr<2.0,>=1.7->docling<3,>=2.2.0->llama-index-readers-docling) (1.13.0)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.11/site-packages (from huggingface_hub<1,>=0.23->docling<3,>=2.2.0->llama-index-readers-docling) (3.19.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.venv/lib/python3.11/site-packages (from huggingface_hub<1,>=0.23->docling<3,>=2.2.0->llama-index-readers-docling) (2025.3.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./.venv/lib/python3.11/site-packages (from huggingface_hub<1,>=0.23->docling<3,>=2.2.0->llama-index-readers-docling) (1.1.7)\n",
      "Requirement already satisfied: attrs>=19.2.0 in ./.venv/lib/python3.11/site-packages (from jsonlines<4.0.0,>=3.1.0->docling-ibm-models<4,>=3.9.0->docling<3,>=2.2.0->llama-index-readers-docling) (25.3.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in ./.venv/lib/python3.11/site-packages (from jsonschema<5.0.0,>=4.16.0->docling-core<3,>=2.2.0->llama-index-readers-docling) (2025.4.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in ./.venv/lib/python3.11/site-packages (from jsonschema<5.0.0,>=4.16.0->docling-core<3,>=2.2.0->llama-index-readers-docling) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in ./.venv/lib/python3.11/site-packages (from jsonschema<5.0.0,>=4.16.0->docling-core<3,>=2.2.0->llama-index-readers-docling) (0.27.0)\n",
      "Requirement already satisfied: aiohttp<4,>=3.8.6 in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-readers-docling) (3.12.15)\n",
      "Requirement already satisfied: aiosqlite in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-readers-docling) (0.21.0)\n",
      "Requirement already satisfied: banks<3,>=2.2.0 in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-readers-docling) (2.2.0)\n",
      "Requirement already satisfied: dataclasses-json in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-readers-docling) (0.6.7)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-readers-docling) (1.2.18)\n",
      "Requirement already satisfied: dirtyjson<2,>=1.0.8 in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-readers-docling) (1.0.8)\n",
      "Requirement already satisfied: httpx in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-readers-docling) (0.28.1)\n",
      "Requirement already satisfied: llama-index-workflows<2,>=1.0.1 in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-readers-docling) (1.3.0)\n",
      "Requirement already satisfied: nest-asyncio<2,>=1.5.8 in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-readers-docling) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-readers-docling) (3.5)\n",
      "Requirement already satisfied: nltk>3.8.1 in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-readers-docling) (3.9.1)\n",
      "Requirement already satisfied: platformdirs in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-readers-docling) (4.3.8)\n",
      "Requirement already satisfied: setuptools>=80.9.0 in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-readers-docling) (80.9.0)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.49 in ./.venv/lib/python3.11/site-packages (from sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.14,>=0.13.0->llama-index-readers-docling) (2.0.43)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-readers-docling) (9.1.2)\n",
      "Requirement already satisfied: tiktoken>=0.7.0 in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-readers-docling) (0.11.0)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-readers-docling) (0.9.0)\n",
      "Requirement already satisfied: wrapt in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-readers-docling) (1.17.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in ./.venv/lib/python3.11/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.14,>=0.13.0->llama-index-readers-docling) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in ./.venv/lib/python3.11/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.14,>=0.13.0->llama-index-readers-docling) (1.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.venv/lib/python3.11/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.14,>=0.13.0->llama-index-readers-docling) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.venv/lib/python3.11/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.14,>=0.13.0->llama-index-readers-docling) (6.6.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./.venv/lib/python3.11/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.14,>=0.13.0->llama-index-readers-docling) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./.venv/lib/python3.11/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.14,>=0.13.0->llama-index-readers-docling) (1.20.1)\n",
      "Requirement already satisfied: griffe in ./.venv/lib/python3.11/site-packages (from banks<3,>=2.2.0->llama-index-core<0.14,>=0.13.0->llama-index-readers-docling) (1.12.1)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.11/site-packages (from banks<3,>=2.2.0->llama-index-core<0.14,>=0.13.0->llama-index-readers-docling) (3.1.6)\n",
      "Requirement already satisfied: llama-index-instrumentation>=0.1.0 in ./.venv/lib/python3.11/site-packages (from llama-index-workflows<2,>=1.0.1->llama-index-core<0.14,>=0.13.0->llama-index-readers-docling) (0.4.0)\n",
      "Requirement already satisfied: et-xmlfile in ./.venv/lib/python3.11/site-packages (from openpyxl<4.0.0,>=3.1.5->docling<3,>=2.2.0->llama-index-readers-docling) (2.0.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.11/site-packages (from pandas<3.0.0,>=2.1.4->docling<3,>=2.2.0->llama-index-readers-docling) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.11/site-packages (from pandas<3.0.0,>=2.1.4->docling<3,>=2.2.0->llama-index-readers-docling) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.11/site-packages (from pandas<3.0.0,>=2.1.4->docling<3,>=2.2.0->llama-index-readers-docling) (2025.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./.venv/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.0.0->docling<3,>=2.2.0->llama-index-readers-docling) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in ./.venv/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.0.0->docling<3,>=2.2.0->llama-index-readers-docling) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in ./.venv/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.0.0->docling<3,>=2.2.0->llama-index-readers-docling) (0.4.1)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in ./.venv/lib/python3.11/site-packages (from pydantic-settings<3.0.0,>=2.3.0->docling<3,>=2.2.0->llama-index-readers-docling) (1.1.1)\n",
      "Requirement already satisfied: XlsxWriter>=0.5.7 in ./.venv/lib/python3.11/site-packages (from python-pptx<2.0.0,>=1.0.2->docling<3,>=2.2.0->llama-index-readers-docling) (3.2.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.venv/lib/python3.11/site-packages (from requests<3.0.0,>=2.32.2->docling<3,>=2.2.0->llama-index-readers-docling) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.11/site-packages (from requests<3.0.0,>=2.32.2->docling<3,>=2.2.0->llama-index-readers-docling) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.11/site-packages (from requests<3.0.0,>=2.32.2->docling<3,>=2.2.0->llama-index-readers-docling) (2.5.0)\n",
      "Requirement already satisfied: mpire[dill] in ./.venv/lib/python3.11/site-packages (from semchunk<3.0.0,>=2.2.0->docling-core[chunking]<3.0.0,>=2.42.0->docling<3,>=2.2.0->llama-index-readers-docling) (2.10.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in ./.venv/lib/python3.11/site-packages (from torch>=2.0.0->accelerate<2,>=1.0.0->docling<3,>=2.2.0->llama-index-readers-docling) (1.14.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.venv/lib/python3.11/site-packages (from transformers<5.0.0,>=4.34.0->docling-core[chunking]<3.0.0,>=2.42.0->docling<3,>=2.2.0->llama-index-readers-docling) (2025.7.34)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in ./.venv/lib/python3.11/site-packages (from transformers<5.0.0,>=4.34.0->docling-core[chunking]<3.0.0,>=2.42.0->docling<3,>=2.2.0->llama-index-readers-docling) (0.21.4)\n",
      "Requirement already satisfied: click>=8.0.0 in ./.venv/lib/python3.11/site-packages (from typer<0.17.0,>=0.12.5->docling<3,>=2.2.0->llama-index-readers-docling) (8.2.1)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in ./.venv/lib/python3.11/site-packages (from typer<0.17.0,>=0.12.5->docling<3,>=2.2.0->llama-index-readers-docling) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in ./.venv/lib/python3.11/site-packages (from typer<0.17.0,>=0.12.5->docling<3,>=2.2.0->llama-index-readers-docling) (14.1.0)\n",
      "Requirement already satisfied: joblib in ./.venv/lib/python3.11/site-packages (from nltk>3.8.1->llama-index-core<0.14,>=0.13.0->llama-index-readers-docling) (1.5.1)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas<3.0.0,>=2.1.4->docling<3,>=2.2.0->llama-index-readers-docling) (1.17.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in ./.venv/lib/python3.11/site-packages (from rich>=10.11.0->typer<0.17.0,>=0.12.5->docling<3,>=2.2.0->llama-index-readers-docling) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in ./.venv/lib/python3.11/site-packages (from rich>=10.11.0->typer<0.17.0,>=0.12.5->docling<3,>=2.2.0->llama-index-readers-docling) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in ./.venv/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<0.17.0,>=0.12.5->docling<3,>=2.2.0->llama-index-readers-docling) (0.1.2)\n",
      "Requirement already satisfied: greenlet>=1 in ./.venv/lib/python3.11/site-packages (from sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.14,>=0.13.0->llama-index-readers-docling) (3.2.4)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.11/site-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate<2,>=1.0.0->docling<3,>=2.2.0->llama-index-readers-docling) (1.3.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in ./.venv/lib/python3.11/site-packages (from typing-inspect>=0.8.0->llama-index-core<0.14,>=0.13.0->llama-index-readers-docling) (1.1.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in ./.venv/lib/python3.11/site-packages (from dataclasses-json->llama-index-core<0.14,>=0.13.0->llama-index-readers-docling) (3.26.1)\n",
      "Requirement already satisfied: colorama>=0.4 in ./.venv/lib/python3.11/site-packages (from griffe->banks<3,>=2.2.0->llama-index-core<0.14,>=0.13.0->llama-index-readers-docling) (0.4.6)\n",
      "Requirement already satisfied: anyio in ./.venv/lib/python3.11/site-packages (from httpx->llama-index-core<0.14,>=0.13.0->llama-index-readers-docling) (4.10.0)\n",
      "Requirement already satisfied: httpcore==1.* in ./.venv/lib/python3.11/site-packages (from httpx->llama-index-core<0.14,>=0.13.0->llama-index-readers-docling) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in ./.venv/lib/python3.11/site-packages (from httpcore==1.*->httpx->llama-index-core<0.14,>=0.13.0->llama-index-readers-docling) (0.16.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in ./.venv/lib/python3.11/site-packages (from anyio->httpx->llama-index-core<0.14,>=0.13.0->llama-index-readers-docling) (1.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.11/site-packages (from jinja2->banks<3,>=2.2.0->llama-index-core<0.14,>=0.13.0->llama-index-readers-docling) (3.0.2)\n",
      "Requirement already satisfied: multiprocess>=0.70.15 in ./.venv/lib/python3.11/site-packages (from mpire[dill]->semchunk<3.0.0,>=2.2.0->docling-core[chunking]<3.0.0,>=2.42.0->docling<3,>=2.2.0->llama-index-readers-docling) (0.70.16)\n",
      "Requirement already satisfied: dill>=0.3.8 in ./.venv/lib/python3.11/site-packages (from multiprocess>=0.70.15->mpire[dill]->semchunk<3.0.0,>=2.2.0->docling-core[chunking]<3.0.0,>=2.42.0->docling<3,>=2.2.0->llama-index-readers-docling) (0.3.8)\n",
      "Requirement already satisfied: imageio!=2.35.0,>=2.33 in ./.venv/lib/python3.11/site-packages (from scikit-image->easyocr<2.0,>=1.7->docling<3,>=2.2.0->llama-index-readers-docling) (2.37.0)\n",
      "Requirement already satisfied: tifffile>=2022.8.12 in ./.venv/lib/python3.11/site-packages (from scikit-image->easyocr<2.0,>=1.7->docling<3,>=2.2.0->llama-index-readers-docling) (2025.6.11)\n",
      "Requirement already satisfied: lazy-loader>=0.4 in ./.venv/lib/python3.11/site-packages (from scikit-image->easyocr<2.0,>=1.7->docling<3,>=2.2.0->llama-index-readers-docling) (0.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: llama-index-node-parser-docling in ./.venv/lib/python3.11/site-packages (0.4.0)\n",
      "Requirement already satisfied: docling-core<3,>=2.18.0 in ./.venv/lib/python3.11/site-packages (from llama-index-node-parser-docling) (2.44.2)\n",
      "Requirement already satisfied: llama-index-core<0.14,>=0.13.0 in ./.venv/lib/python3.11/site-packages (from llama-index-node-parser-docling) (0.13.2)\n",
      "Requirement already satisfied: jsonschema<5.0.0,>=4.16.0 in ./.venv/lib/python3.11/site-packages (from docling-core<3,>=2.18.0->llama-index-node-parser-docling) (4.25.0)\n",
      "Requirement already satisfied: pydantic!=2.10.0,!=2.10.1,!=2.10.2,<3.0.0,>=2.6.0 in ./.venv/lib/python3.11/site-packages (from docling-core<3,>=2.18.0->llama-index-node-parser-docling) (2.11.7)\n",
      "Requirement already satisfied: jsonref<2.0.0,>=1.1.0 in ./.venv/lib/python3.11/site-packages (from docling-core<3,>=2.18.0->llama-index-node-parser-docling) (1.1.0)\n",
      "Requirement already satisfied: tabulate<0.10.0,>=0.9.0 in ./.venv/lib/python3.11/site-packages (from docling-core<3,>=2.18.0->llama-index-node-parser-docling) (0.9.0)\n",
      "Requirement already satisfied: pandas<3.0.0,>=2.1.4 in ./.venv/lib/python3.11/site-packages (from docling-core<3,>=2.18.0->llama-index-node-parser-docling) (2.3.1)\n",
      "Requirement already satisfied: pillow<12.0.0,>=10.0.0 in ./.venv/lib/python3.11/site-packages (from docling-core<3,>=2.18.0->llama-index-node-parser-docling) (11.3.0)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=5.1 in ./.venv/lib/python3.11/site-packages (from docling-core<3,>=2.18.0->llama-index-node-parser-docling) (6.0.2)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.12.2 in ./.venv/lib/python3.11/site-packages (from docling-core<3,>=2.18.0->llama-index-node-parser-docling) (4.14.1)\n",
      "Requirement already satisfied: typer<0.17.0,>=0.12.5 in ./.venv/lib/python3.11/site-packages (from docling-core<3,>=2.18.0->llama-index-node-parser-docling) (0.16.0)\n",
      "Requirement already satisfied: latex2mathml<4.0.0,>=3.77.0 in ./.venv/lib/python3.11/site-packages (from docling-core<3,>=2.18.0->llama-index-node-parser-docling) (3.78.0)\n",
      "Requirement already satisfied: attrs>=22.2.0 in ./.venv/lib/python3.11/site-packages (from jsonschema<5.0.0,>=4.16.0->docling-core<3,>=2.18.0->llama-index-node-parser-docling) (25.3.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in ./.venv/lib/python3.11/site-packages (from jsonschema<5.0.0,>=4.16.0->docling-core<3,>=2.18.0->llama-index-node-parser-docling) (2025.4.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in ./.venv/lib/python3.11/site-packages (from jsonschema<5.0.0,>=4.16.0->docling-core<3,>=2.18.0->llama-index-node-parser-docling) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in ./.venv/lib/python3.11/site-packages (from jsonschema<5.0.0,>=4.16.0->docling-core<3,>=2.18.0->llama-index-node-parser-docling) (0.27.0)\n",
      "Requirement already satisfied: aiohttp<4,>=3.8.6 in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-node-parser-docling) (3.12.15)\n",
      "Requirement already satisfied: aiosqlite in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-node-parser-docling) (0.21.0)\n",
      "Requirement already satisfied: banks<3,>=2.2.0 in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-node-parser-docling) (2.2.0)\n",
      "Requirement already satisfied: dataclasses-json in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-node-parser-docling) (0.6.7)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-node-parser-docling) (1.2.18)\n",
      "Requirement already satisfied: dirtyjson<2,>=1.0.8 in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-node-parser-docling) (1.0.8)\n",
      "Requirement already satisfied: filetype<2,>=1.2.0 in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-node-parser-docling) (1.2.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-node-parser-docling) (2025.3.0)\n",
      "Requirement already satisfied: httpx in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-node-parser-docling) (0.28.1)\n",
      "Requirement already satisfied: llama-index-workflows<2,>=1.0.1 in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-node-parser-docling) (1.3.0)\n",
      "Requirement already satisfied: nest-asyncio<2,>=1.5.8 in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-node-parser-docling) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-node-parser-docling) (3.5)\n",
      "Requirement already satisfied: nltk>3.8.1 in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-node-parser-docling) (3.9.1)\n",
      "Requirement already satisfied: numpy in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-node-parser-docling) (2.2.6)\n",
      "Requirement already satisfied: platformdirs in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-node-parser-docling) (4.3.8)\n",
      "Requirement already satisfied: requests>=2.31.0 in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-node-parser-docling) (2.32.4)\n",
      "Requirement already satisfied: setuptools>=80.9.0 in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-node-parser-docling) (80.9.0)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.49 in ./.venv/lib/python3.11/site-packages (from sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.14,>=0.13.0->llama-index-node-parser-docling) (2.0.43)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-node-parser-docling) (9.1.2)\n",
      "Requirement already satisfied: tiktoken>=0.7.0 in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-node-parser-docling) (0.11.0)\n",
      "Requirement already satisfied: tqdm<5,>=4.66.1 in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-node-parser-docling) (4.67.1)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-node-parser-docling) (0.9.0)\n",
      "Requirement already satisfied: wrapt in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-node-parser-docling) (1.17.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in ./.venv/lib/python3.11/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.14,>=0.13.0->llama-index-node-parser-docling) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in ./.venv/lib/python3.11/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.14,>=0.13.0->llama-index-node-parser-docling) (1.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.venv/lib/python3.11/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.14,>=0.13.0->llama-index-node-parser-docling) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.venv/lib/python3.11/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.14,>=0.13.0->llama-index-node-parser-docling) (6.6.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./.venv/lib/python3.11/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.14,>=0.13.0->llama-index-node-parser-docling) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./.venv/lib/python3.11/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.14,>=0.13.0->llama-index-node-parser-docling) (1.20.1)\n",
      "Requirement already satisfied: griffe in ./.venv/lib/python3.11/site-packages (from banks<3,>=2.2.0->llama-index-core<0.14,>=0.13.0->llama-index-node-parser-docling) (1.12.1)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.11/site-packages (from banks<3,>=2.2.0->llama-index-core<0.14,>=0.13.0->llama-index-node-parser-docling) (3.1.6)\n",
      "Requirement already satisfied: llama-index-instrumentation>=0.1.0 in ./.venv/lib/python3.11/site-packages (from llama-index-workflows<2,>=1.0.1->llama-index-core<0.14,>=0.13.0->llama-index-node-parser-docling) (0.4.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.11/site-packages (from pandas<3.0.0,>=2.1.4->docling-core<3,>=2.18.0->llama-index-node-parser-docling) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.11/site-packages (from pandas<3.0.0,>=2.1.4->docling-core<3,>=2.18.0->llama-index-node-parser-docling) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.11/site-packages (from pandas<3.0.0,>=2.1.4->docling-core<3,>=2.18.0->llama-index-node-parser-docling) (2025.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./.venv/lib/python3.11/site-packages (from pydantic!=2.10.0,!=2.10.1,!=2.10.2,<3.0.0,>=2.6.0->docling-core<3,>=2.18.0->llama-index-node-parser-docling) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in ./.venv/lib/python3.11/site-packages (from pydantic!=2.10.0,!=2.10.1,!=2.10.2,<3.0.0,>=2.6.0->docling-core<3,>=2.18.0->llama-index-node-parser-docling) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in ./.venv/lib/python3.11/site-packages (from pydantic!=2.10.0,!=2.10.1,!=2.10.2,<3.0.0,>=2.6.0->docling-core<3,>=2.18.0->llama-index-node-parser-docling) (0.4.1)\n",
      "Requirement already satisfied: click>=8.0.0 in ./.venv/lib/python3.11/site-packages (from typer<0.17.0,>=0.12.5->docling-core<3,>=2.18.0->llama-index-node-parser-docling) (8.2.1)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in ./.venv/lib/python3.11/site-packages (from typer<0.17.0,>=0.12.5->docling-core<3,>=2.18.0->llama-index-node-parser-docling) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in ./.venv/lib/python3.11/site-packages (from typer<0.17.0,>=0.12.5->docling-core<3,>=2.18.0->llama-index-node-parser-docling) (14.1.0)\n",
      "Requirement already satisfied: idna>=2.0 in ./.venv/lib/python3.11/site-packages (from yarl<2.0,>=1.17.0->aiohttp<4,>=3.8.6->llama-index-core<0.14,>=0.13.0->llama-index-node-parser-docling) (3.10)\n",
      "Requirement already satisfied: joblib in ./.venv/lib/python3.11/site-packages (from nltk>3.8.1->llama-index-core<0.14,>=0.13.0->llama-index-node-parser-docling) (1.5.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in ./.venv/lib/python3.11/site-packages (from nltk>3.8.1->llama-index-core<0.14,>=0.13.0->llama-index-node-parser-docling) (2025.7.34)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas<3.0.0,>=2.1.4->docling-core<3,>=2.18.0->llama-index-node-parser-docling) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.venv/lib/python3.11/site-packages (from requests>=2.31.0->llama-index-core<0.14,>=0.13.0->llama-index-node-parser-docling) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.11/site-packages (from requests>=2.31.0->llama-index-core<0.14,>=0.13.0->llama-index-node-parser-docling) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.11/site-packages (from requests>=2.31.0->llama-index-core<0.14,>=0.13.0->llama-index-node-parser-docling) (2025.8.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in ./.venv/lib/python3.11/site-packages (from rich>=10.11.0->typer<0.17.0,>=0.12.5->docling-core<3,>=2.18.0->llama-index-node-parser-docling) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in ./.venv/lib/python3.11/site-packages (from rich>=10.11.0->typer<0.17.0,>=0.12.5->docling-core<3,>=2.18.0->llama-index-node-parser-docling) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in ./.venv/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<0.17.0,>=0.12.5->docling-core<3,>=2.18.0->llama-index-node-parser-docling) (0.1.2)\n",
      "Requirement already satisfied: greenlet>=1 in ./.venv/lib/python3.11/site-packages (from sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.14,>=0.13.0->llama-index-node-parser-docling) (3.2.4)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in ./.venv/lib/python3.11/site-packages (from typing-inspect>=0.8.0->llama-index-core<0.14,>=0.13.0->llama-index-node-parser-docling) (1.1.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in ./.venv/lib/python3.11/site-packages (from dataclasses-json->llama-index-core<0.14,>=0.13.0->llama-index-node-parser-docling) (3.26.1)\n",
      "Requirement already satisfied: packaging>=17.0 in ./.venv/lib/python3.11/site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.14,>=0.13.0->llama-index-node-parser-docling) (25.0)\n",
      "Requirement already satisfied: colorama>=0.4 in ./.venv/lib/python3.11/site-packages (from griffe->banks<3,>=2.2.0->llama-index-core<0.14,>=0.13.0->llama-index-node-parser-docling) (0.4.6)\n",
      "Requirement already satisfied: anyio in ./.venv/lib/python3.11/site-packages (from httpx->llama-index-core<0.14,>=0.13.0->llama-index-node-parser-docling) (4.10.0)\n",
      "Requirement already satisfied: httpcore==1.* in ./.venv/lib/python3.11/site-packages (from httpx->llama-index-core<0.14,>=0.13.0->llama-index-node-parser-docling) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in ./.venv/lib/python3.11/site-packages (from httpcore==1.*->httpx->llama-index-core<0.14,>=0.13.0->llama-index-node-parser-docling) (0.16.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in ./.venv/lib/python3.11/site-packages (from anyio->httpx->llama-index-core<0.14,>=0.13.0->llama-index-node-parser-docling) (1.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.11/site-packages (from jinja2->banks<3,>=2.2.0->llama-index-core<0.14,>=0.13.0->llama-index-node-parser-docling) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: llama-index-vector-stores-milvus in ./.venv/lib/python3.11/site-packages (0.9.0)\n",
      "Requirement already satisfied: llama-index-core<0.14,>=0.13.0 in ./.venv/lib/python3.11/site-packages (from llama-index-vector-stores-milvus) (0.13.2)\n",
      "Requirement already satisfied: pymilvus<3,>=2.5.10 in ./.venv/lib/python3.11/site-packages (from llama-index-vector-stores-milvus) (2.6.0)\n",
      "Requirement already satisfied: aiohttp<4,>=3.8.6 in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-vector-stores-milvus) (3.12.15)\n",
      "Requirement already satisfied: aiosqlite in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-vector-stores-milvus) (0.21.0)\n",
      "Requirement already satisfied: banks<3,>=2.2.0 in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-vector-stores-milvus) (2.2.0)\n",
      "Requirement already satisfied: dataclasses-json in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-vector-stores-milvus) (0.6.7)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-vector-stores-milvus) (1.2.18)\n",
      "Requirement already satisfied: dirtyjson<2,>=1.0.8 in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-vector-stores-milvus) (1.0.8)\n",
      "Requirement already satisfied: filetype<2,>=1.2.0 in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-vector-stores-milvus) (1.2.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-vector-stores-milvus) (2025.3.0)\n",
      "Requirement already satisfied: httpx in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-vector-stores-milvus) (0.28.1)\n",
      "Requirement already satisfied: llama-index-workflows<2,>=1.0.1 in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-vector-stores-milvus) (1.3.0)\n",
      "Requirement already satisfied: nest-asyncio<2,>=1.5.8 in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-vector-stores-milvus) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-vector-stores-milvus) (3.5)\n",
      "Requirement already satisfied: nltk>3.8.1 in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-vector-stores-milvus) (3.9.1)\n",
      "Requirement already satisfied: numpy in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-vector-stores-milvus) (2.2.6)\n",
      "Requirement already satisfied: pillow>=9.0.0 in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-vector-stores-milvus) (11.3.0)\n",
      "Requirement already satisfied: platformdirs in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-vector-stores-milvus) (4.3.8)\n",
      "Requirement already satisfied: pydantic>=2.8.0 in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-vector-stores-milvus) (2.11.7)\n",
      "Requirement already satisfied: pyyaml>=6.0.1 in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-vector-stores-milvus) (6.0.2)\n",
      "Requirement already satisfied: requests>=2.31.0 in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-vector-stores-milvus) (2.32.4)\n",
      "Requirement already satisfied: setuptools>=80.9.0 in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-vector-stores-milvus) (80.9.0)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.49 in ./.venv/lib/python3.11/site-packages (from sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.14,>=0.13.0->llama-index-vector-stores-milvus) (2.0.43)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-vector-stores-milvus) (9.1.2)\n",
      "Requirement already satisfied: tiktoken>=0.7.0 in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-vector-stores-milvus) (0.11.0)\n",
      "Requirement already satisfied: tqdm<5,>=4.66.1 in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-vector-stores-milvus) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-vector-stores-milvus) (4.14.1)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-vector-stores-milvus) (0.9.0)\n",
      "Requirement already satisfied: wrapt in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-vector-stores-milvus) (1.17.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in ./.venv/lib/python3.11/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.14,>=0.13.0->llama-index-vector-stores-milvus) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in ./.venv/lib/python3.11/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.14,>=0.13.0->llama-index-vector-stores-milvus) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.venv/lib/python3.11/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.14,>=0.13.0->llama-index-vector-stores-milvus) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.venv/lib/python3.11/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.14,>=0.13.0->llama-index-vector-stores-milvus) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.venv/lib/python3.11/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.14,>=0.13.0->llama-index-vector-stores-milvus) (6.6.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./.venv/lib/python3.11/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.14,>=0.13.0->llama-index-vector-stores-milvus) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./.venv/lib/python3.11/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.14,>=0.13.0->llama-index-vector-stores-milvus) (1.20.1)\n",
      "Requirement already satisfied: griffe in ./.venv/lib/python3.11/site-packages (from banks<3,>=2.2.0->llama-index-core<0.14,>=0.13.0->llama-index-vector-stores-milvus) (1.12.1)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.11/site-packages (from banks<3,>=2.2.0->llama-index-core<0.14,>=0.13.0->llama-index-vector-stores-milvus) (3.1.6)\n",
      "Requirement already satisfied: llama-index-instrumentation>=0.1.0 in ./.venv/lib/python3.11/site-packages (from llama-index-workflows<2,>=1.0.1->llama-index-core<0.14,>=0.13.0->llama-index-vector-stores-milvus) (0.4.0)\n",
      "Requirement already satisfied: grpcio!=1.68.0,!=1.68.1,!=1.69.0,!=1.70.0,!=1.70.1,!=1.71.0,!=1.72.1,!=1.73.0,>=1.66.2 in ./.venv/lib/python3.11/site-packages (from pymilvus<3,>=2.5.10->llama-index-vector-stores-milvus) (1.74.0)\n",
      "Requirement already satisfied: protobuf>=5.27.2 in ./.venv/lib/python3.11/site-packages (from pymilvus<3,>=2.5.10->llama-index-vector-stores-milvus) (6.32.0)\n",
      "Requirement already satisfied: python-dotenv<2.0.0,>=1.0.1 in ./.venv/lib/python3.11/site-packages (from pymilvus<3,>=2.5.10->llama-index-vector-stores-milvus) (1.1.1)\n",
      "Requirement already satisfied: ujson>=2.0.0 in ./.venv/lib/python3.11/site-packages (from pymilvus<3,>=2.5.10->llama-index-vector-stores-milvus) (5.10.0)\n",
      "Requirement already satisfied: pandas>=1.2.4 in ./.venv/lib/python3.11/site-packages (from pymilvus<3,>=2.5.10->llama-index-vector-stores-milvus) (2.3.1)\n",
      "Requirement already satisfied: milvus-lite>=2.4.0 in ./.venv/lib/python3.11/site-packages (from pymilvus<3,>=2.5.10->llama-index-vector-stores-milvus) (2.5.1)\n",
      "Requirement already satisfied: idna>=2.0 in ./.venv/lib/python3.11/site-packages (from yarl<2.0,>=1.17.0->aiohttp<4,>=3.8.6->llama-index-core<0.14,>=0.13.0->llama-index-vector-stores-milvus) (3.10)\n",
      "Requirement already satisfied: click in ./.venv/lib/python3.11/site-packages (from nltk>3.8.1->llama-index-core<0.14,>=0.13.0->llama-index-vector-stores-milvus) (8.2.1)\n",
      "Requirement already satisfied: joblib in ./.venv/lib/python3.11/site-packages (from nltk>3.8.1->llama-index-core<0.14,>=0.13.0->llama-index-vector-stores-milvus) (1.5.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in ./.venv/lib/python3.11/site-packages (from nltk>3.8.1->llama-index-core<0.14,>=0.13.0->llama-index-vector-stores-milvus) (2025.7.34)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.11/site-packages (from pandas>=1.2.4->pymilvus<3,>=2.5.10->llama-index-vector-stores-milvus) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.11/site-packages (from pandas>=1.2.4->pymilvus<3,>=2.5.10->llama-index-vector-stores-milvus) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.11/site-packages (from pandas>=1.2.4->pymilvus<3,>=2.5.10->llama-index-vector-stores-milvus) (2025.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./.venv/lib/python3.11/site-packages (from pydantic>=2.8.0->llama-index-core<0.14,>=0.13.0->llama-index-vector-stores-milvus) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in ./.venv/lib/python3.11/site-packages (from pydantic>=2.8.0->llama-index-core<0.14,>=0.13.0->llama-index-vector-stores-milvus) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in ./.venv/lib/python3.11/site-packages (from pydantic>=2.8.0->llama-index-core<0.14,>=0.13.0->llama-index-vector-stores-milvus) (0.4.1)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas>=1.2.4->pymilvus<3,>=2.5.10->llama-index-vector-stores-milvus) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.venv/lib/python3.11/site-packages (from requests>=2.31.0->llama-index-core<0.14,>=0.13.0->llama-index-vector-stores-milvus) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.11/site-packages (from requests>=2.31.0->llama-index-core<0.14,>=0.13.0->llama-index-vector-stores-milvus) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.11/site-packages (from requests>=2.31.0->llama-index-core<0.14,>=0.13.0->llama-index-vector-stores-milvus) (2025.8.3)\n",
      "Requirement already satisfied: greenlet>=1 in ./.venv/lib/python3.11/site-packages (from sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.14,>=0.13.0->llama-index-vector-stores-milvus) (3.2.4)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in ./.venv/lib/python3.11/site-packages (from typing-inspect>=0.8.0->llama-index-core<0.14,>=0.13.0->llama-index-vector-stores-milvus) (1.1.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in ./.venv/lib/python3.11/site-packages (from dataclasses-json->llama-index-core<0.14,>=0.13.0->llama-index-vector-stores-milvus) (3.26.1)\n",
      "Requirement already satisfied: packaging>=17.0 in ./.venv/lib/python3.11/site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.14,>=0.13.0->llama-index-vector-stores-milvus) (25.0)\n",
      "Requirement already satisfied: colorama>=0.4 in ./.venv/lib/python3.11/site-packages (from griffe->banks<3,>=2.2.0->llama-index-core<0.14,>=0.13.0->llama-index-vector-stores-milvus) (0.4.6)\n",
      "Requirement already satisfied: anyio in ./.venv/lib/python3.11/site-packages (from httpx->llama-index-core<0.14,>=0.13.0->llama-index-vector-stores-milvus) (4.10.0)\n",
      "Requirement already satisfied: httpcore==1.* in ./.venv/lib/python3.11/site-packages (from httpx->llama-index-core<0.14,>=0.13.0->llama-index-vector-stores-milvus) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in ./.venv/lib/python3.11/site-packages (from httpcore==1.*->httpx->llama-index-core<0.14,>=0.13.0->llama-index-vector-stores-milvus) (0.16.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in ./.venv/lib/python3.11/site-packages (from anyio->httpx->llama-index-core<0.14,>=0.13.0->llama-index-vector-stores-milvus) (1.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.11/site-packages (from jinja2->banks<3,>=2.2.0->llama-index-core<0.14,>=0.13.0->llama-index-vector-stores-milvus) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: llama-index-embeddings-huggingface in ./.venv/lib/python3.11/site-packages (0.6.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.0 in ./.venv/lib/python3.11/site-packages (from huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (0.34.4)\n",
      "Requirement already satisfied: llama-index-core<0.14,>=0.13.0 in ./.venv/lib/python3.11/site-packages (from llama-index-embeddings-huggingface) (0.13.2)\n",
      "Requirement already satisfied: sentence-transformers>=2.6.1 in ./.venv/lib/python3.11/site-packages (from llama-index-embeddings-huggingface) (5.1.0)\n",
      "Requirement already satisfied: aiohttp<4,>=3.8.6 in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-embeddings-huggingface) (3.12.15)\n",
      "Requirement already satisfied: aiosqlite in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-embeddings-huggingface) (0.21.0)\n",
      "Requirement already satisfied: banks<3,>=2.2.0 in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-embeddings-huggingface) (2.2.0)\n",
      "Requirement already satisfied: dataclasses-json in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-embeddings-huggingface) (0.6.7)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-embeddings-huggingface) (1.2.18)\n",
      "Requirement already satisfied: dirtyjson<2,>=1.0.8 in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-embeddings-huggingface) (1.0.8)\n",
      "Requirement already satisfied: filetype<2,>=1.2.0 in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-embeddings-huggingface) (1.2.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-embeddings-huggingface) (2025.3.0)\n",
      "Requirement already satisfied: httpx in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-embeddings-huggingface) (0.28.1)\n",
      "Requirement already satisfied: llama-index-workflows<2,>=1.0.1 in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-embeddings-huggingface) (1.3.0)\n",
      "Requirement already satisfied: nest-asyncio<2,>=1.5.8 in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-embeddings-huggingface) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-embeddings-huggingface) (3.5)\n",
      "Requirement already satisfied: nltk>3.8.1 in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-embeddings-huggingface) (3.9.1)\n",
      "Requirement already satisfied: numpy in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-embeddings-huggingface) (2.2.6)\n",
      "Requirement already satisfied: pillow>=9.0.0 in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-embeddings-huggingface) (11.3.0)\n",
      "Requirement already satisfied: platformdirs in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-embeddings-huggingface) (4.3.8)\n",
      "Requirement already satisfied: pydantic>=2.8.0 in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-embeddings-huggingface) (2.11.7)\n",
      "Requirement already satisfied: pyyaml>=6.0.1 in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-embeddings-huggingface) (6.0.2)\n",
      "Requirement already satisfied: requests>=2.31.0 in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-embeddings-huggingface) (2.32.4)\n",
      "Requirement already satisfied: setuptools>=80.9.0 in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-embeddings-huggingface) (80.9.0)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.49 in ./.venv/lib/python3.11/site-packages (from sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.14,>=0.13.0->llama-index-embeddings-huggingface) (2.0.43)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-embeddings-huggingface) (9.1.2)\n",
      "Requirement already satisfied: tiktoken>=0.7.0 in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-embeddings-huggingface) (0.11.0)\n",
      "Requirement already satisfied: tqdm<5,>=4.66.1 in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-embeddings-huggingface) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-embeddings-huggingface) (4.14.1)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-embeddings-huggingface) (0.9.0)\n",
      "Requirement already satisfied: wrapt in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-embeddings-huggingface) (1.17.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in ./.venv/lib/python3.11/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.14,>=0.13.0->llama-index-embeddings-huggingface) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in ./.venv/lib/python3.11/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.14,>=0.13.0->llama-index-embeddings-huggingface) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.venv/lib/python3.11/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.14,>=0.13.0->llama-index-embeddings-huggingface) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.venv/lib/python3.11/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.14,>=0.13.0->llama-index-embeddings-huggingface) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.venv/lib/python3.11/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.14,>=0.13.0->llama-index-embeddings-huggingface) (6.6.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./.venv/lib/python3.11/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.14,>=0.13.0->llama-index-embeddings-huggingface) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./.venv/lib/python3.11/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.14,>=0.13.0->llama-index-embeddings-huggingface) (1.20.1)\n",
      "Requirement already satisfied: griffe in ./.venv/lib/python3.11/site-packages (from banks<3,>=2.2.0->llama-index-core<0.14,>=0.13.0->llama-index-embeddings-huggingface) (1.12.1)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.11/site-packages (from banks<3,>=2.2.0->llama-index-core<0.14,>=0.13.0->llama-index-embeddings-huggingface) (3.1.6)\n",
      "Requirement already satisfied: llama-index-instrumentation>=0.1.0 in ./.venv/lib/python3.11/site-packages (from llama-index-workflows<2,>=1.0.1->llama-index-core<0.14,>=0.13.0->llama-index-embeddings-huggingface) (0.4.0)\n",
      "Requirement already satisfied: idna>=2.0 in ./.venv/lib/python3.11/site-packages (from yarl<2.0,>=1.17.0->aiohttp<4,>=3.8.6->llama-index-core<0.14,>=0.13.0->llama-index-embeddings-huggingface) (3.10)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.11/site-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (3.19.1)\n",
      "Requirement already satisfied: packaging>=20.9 in ./.venv/lib/python3.11/site-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (25.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./.venv/lib/python3.11/site-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (1.1.7)\n",
      "Requirement already satisfied: click in ./.venv/lib/python3.11/site-packages (from nltk>3.8.1->llama-index-core<0.14,>=0.13.0->llama-index-embeddings-huggingface) (8.2.1)\n",
      "Requirement already satisfied: joblib in ./.venv/lib/python3.11/site-packages (from nltk>3.8.1->llama-index-core<0.14,>=0.13.0->llama-index-embeddings-huggingface) (1.5.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in ./.venv/lib/python3.11/site-packages (from nltk>3.8.1->llama-index-core<0.14,>=0.13.0->llama-index-embeddings-huggingface) (2025.7.34)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./.venv/lib/python3.11/site-packages (from pydantic>=2.8.0->llama-index-core<0.14,>=0.13.0->llama-index-embeddings-huggingface) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in ./.venv/lib/python3.11/site-packages (from pydantic>=2.8.0->llama-index-core<0.14,>=0.13.0->llama-index-embeddings-huggingface) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in ./.venv/lib/python3.11/site-packages (from pydantic>=2.8.0->llama-index-core<0.14,>=0.13.0->llama-index-embeddings-huggingface) (0.4.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.venv/lib/python3.11/site-packages (from requests>=2.31.0->llama-index-core<0.14,>=0.13.0->llama-index-embeddings-huggingface) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.11/site-packages (from requests>=2.31.0->llama-index-core<0.14,>=0.13.0->llama-index-embeddings-huggingface) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.11/site-packages (from requests>=2.31.0->llama-index-core<0.14,>=0.13.0->llama-index-embeddings-huggingface) (2025.8.3)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in ./.venv/lib/python3.11/site-packages (from sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (4.55.2)\n",
      "Requirement already satisfied: torch>=1.11.0 in ./.venv/lib/python3.11/site-packages (from sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (2.8.0)\n",
      "Requirement already satisfied: scikit-learn in ./.venv/lib/python3.11/site-packages (from sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (1.7.1)\n",
      "Requirement already satisfied: scipy in ./.venv/lib/python3.11/site-packages (from sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (1.16.1)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in ./.venv/lib/python3.11/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (0.21.4)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in ./.venv/lib/python3.11/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (0.6.2)\n",
      "Requirement already satisfied: greenlet>=1 in ./.venv/lib/python3.11/site-packages (from sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.14,>=0.13.0->llama-index-embeddings-huggingface) (3.2.4)\n",
      "Requirement already satisfied: sympy>=1.13.3 in ./.venv/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (1.14.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.11/site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (1.3.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in ./.venv/lib/python3.11/site-packages (from typing-inspect>=0.8.0->llama-index-core<0.14,>=0.13.0->llama-index-embeddings-huggingface) (1.1.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in ./.venv/lib/python3.11/site-packages (from dataclasses-json->llama-index-core<0.14,>=0.13.0->llama-index-embeddings-huggingface) (3.26.1)\n",
      "Requirement already satisfied: colorama>=0.4 in ./.venv/lib/python3.11/site-packages (from griffe->banks<3,>=2.2.0->llama-index-core<0.14,>=0.13.0->llama-index-embeddings-huggingface) (0.4.6)\n",
      "Requirement already satisfied: anyio in ./.venv/lib/python3.11/site-packages (from httpx->llama-index-core<0.14,>=0.13.0->llama-index-embeddings-huggingface) (4.10.0)\n",
      "Requirement already satisfied: httpcore==1.* in ./.venv/lib/python3.11/site-packages (from httpx->llama-index-core<0.14,>=0.13.0->llama-index-embeddings-huggingface) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in ./.venv/lib/python3.11/site-packages (from httpcore==1.*->httpx->llama-index-core<0.14,>=0.13.0->llama-index-embeddings-huggingface) (0.16.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in ./.venv/lib/python3.11/site-packages (from anyio->httpx->llama-index-core<0.14,>=0.13.0->llama-index-embeddings-huggingface) (1.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.11/site-packages (from jinja2->banks<3,>=2.2.0->llama-index-core<0.14,>=0.13.0->llama-index-embeddings-huggingface) (3.0.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./.venv/lib/python3.11/site-packages (from scikit-learn->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (3.6.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: llama-index-llms-huggingface in ./.venv/lib/python3.11/site-packages (0.6.0)\n",
      "Requirement already satisfied: llama-index-core<0.14,>=0.13.0 in ./.venv/lib/python3.11/site-packages (from llama-index-llms-huggingface) (0.13.2)\n",
      "Requirement already satisfied: torch<3,>=2.1.2 in ./.venv/lib/python3.11/site-packages (from llama-index-llms-huggingface) (2.8.0)\n",
      "Requirement already satisfied: transformers<5,>=4.37.0 in ./.venv/lib/python3.11/site-packages (from transformers[torch]<5,>=4.37.0->llama-index-llms-huggingface) (4.55.2)\n",
      "Requirement already satisfied: aiohttp<4,>=3.8.6 in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface) (3.12.15)\n",
      "Requirement already satisfied: aiosqlite in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface) (0.21.0)\n",
      "Requirement already satisfied: banks<3,>=2.2.0 in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface) (2.2.0)\n",
      "Requirement already satisfied: dataclasses-json in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface) (0.6.7)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface) (1.2.18)\n",
      "Requirement already satisfied: dirtyjson<2,>=1.0.8 in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface) (1.0.8)\n",
      "Requirement already satisfied: filetype<2,>=1.2.0 in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface) (1.2.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface) (2025.3.0)\n",
      "Requirement already satisfied: httpx in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface) (0.28.1)\n",
      "Requirement already satisfied: llama-index-workflows<2,>=1.0.1 in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface) (1.3.0)\n",
      "Requirement already satisfied: nest-asyncio<2,>=1.5.8 in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface) (3.5)\n",
      "Requirement already satisfied: nltk>3.8.1 in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface) (3.9.1)\n",
      "Requirement already satisfied: numpy in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface) (2.2.6)\n",
      "Requirement already satisfied: pillow>=9.0.0 in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface) (11.3.0)\n",
      "Requirement already satisfied: platformdirs in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface) (4.3.8)\n",
      "Requirement already satisfied: pydantic>=2.8.0 in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface) (2.11.7)\n",
      "Requirement already satisfied: pyyaml>=6.0.1 in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface) (6.0.2)\n",
      "Requirement already satisfied: requests>=2.31.0 in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface) (2.32.4)\n",
      "Requirement already satisfied: setuptools>=80.9.0 in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface) (80.9.0)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.49 in ./.venv/lib/python3.11/site-packages (from sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface) (2.0.43)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface) (9.1.2)\n",
      "Requirement already satisfied: tiktoken>=0.7.0 in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface) (0.11.0)\n",
      "Requirement already satisfied: tqdm<5,>=4.66.1 in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface) (4.14.1)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface) (0.9.0)\n",
      "Requirement already satisfied: wrapt in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface) (1.17.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in ./.venv/lib/python3.11/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in ./.venv/lib/python3.11/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.venv/lib/python3.11/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.venv/lib/python3.11/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.venv/lib/python3.11/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface) (6.6.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./.venv/lib/python3.11/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./.venv/lib/python3.11/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface) (1.20.1)\n",
      "Requirement already satisfied: griffe in ./.venv/lib/python3.11/site-packages (from banks<3,>=2.2.0->llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface) (1.12.1)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.11/site-packages (from banks<3,>=2.2.0->llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface) (3.1.6)\n",
      "Requirement already satisfied: llama-index-instrumentation>=0.1.0 in ./.venv/lib/python3.11/site-packages (from llama-index-workflows<2,>=1.0.1->llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface) (0.4.0)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.11/site-packages (from torch<3,>=2.1.2->llama-index-llms-huggingface) (3.19.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in ./.venv/lib/python3.11/site-packages (from torch<3,>=2.1.2->llama-index-llms-huggingface) (1.14.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in ./.venv/lib/python3.11/site-packages (from transformers<5,>=4.37.0->transformers[torch]<5,>=4.37.0->llama-index-llms-huggingface) (0.34.4)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.11/site-packages (from transformers<5,>=4.37.0->transformers[torch]<5,>=4.37.0->llama-index-llms-huggingface) (25.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.venv/lib/python3.11/site-packages (from transformers<5,>=4.37.0->transformers[torch]<5,>=4.37.0->llama-index-llms-huggingface) (2025.7.34)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in ./.venv/lib/python3.11/site-packages (from transformers<5,>=4.37.0->transformers[torch]<5,>=4.37.0->llama-index-llms-huggingface) (0.21.4)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in ./.venv/lib/python3.11/site-packages (from transformers<5,>=4.37.0->transformers[torch]<5,>=4.37.0->llama-index-llms-huggingface) (0.6.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./.venv/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers<5,>=4.37.0->transformers[torch]<5,>=4.37.0->llama-index-llms-huggingface) (1.1.7)\n",
      "Requirement already satisfied: accelerate>=0.26.0 in ./.venv/lib/python3.11/site-packages (from transformers[torch]<5,>=4.37.0->llama-index-llms-huggingface) (1.10.0)\n",
      "Requirement already satisfied: idna>=2.0 in ./.venv/lib/python3.11/site-packages (from yarl<2.0,>=1.17.0->aiohttp<4,>=3.8.6->llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface) (3.10)\n",
      "Requirement already satisfied: psutil in ./.venv/lib/python3.11/site-packages (from accelerate>=0.26.0->transformers[torch]<5,>=4.37.0->llama-index-llms-huggingface) (7.0.0)\n",
      "Requirement already satisfied: click in ./.venv/lib/python3.11/site-packages (from nltk>3.8.1->llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface) (8.2.1)\n",
      "Requirement already satisfied: joblib in ./.venv/lib/python3.11/site-packages (from nltk>3.8.1->llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface) (1.5.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./.venv/lib/python3.11/site-packages (from pydantic>=2.8.0->llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in ./.venv/lib/python3.11/site-packages (from pydantic>=2.8.0->llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in ./.venv/lib/python3.11/site-packages (from pydantic>=2.8.0->llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface) (0.4.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.venv/lib/python3.11/site-packages (from requests>=2.31.0->llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.11/site-packages (from requests>=2.31.0->llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.11/site-packages (from requests>=2.31.0->llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface) (2025.8.3)\n",
      "Requirement already satisfied: greenlet>=1 in ./.venv/lib/python3.11/site-packages (from sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface) (3.2.4)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.11/site-packages (from sympy>=1.13.3->torch<3,>=2.1.2->llama-index-llms-huggingface) (1.3.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in ./.venv/lib/python3.11/site-packages (from typing-inspect>=0.8.0->llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface) (1.1.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in ./.venv/lib/python3.11/site-packages (from dataclasses-json->llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface) (3.26.1)\n",
      "Requirement already satisfied: colorama>=0.4 in ./.venv/lib/python3.11/site-packages (from griffe->banks<3,>=2.2.0->llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface) (0.4.6)\n",
      "Requirement already satisfied: anyio in ./.venv/lib/python3.11/site-packages (from httpx->llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface) (4.10.0)\n",
      "Requirement already satisfied: httpcore==1.* in ./.venv/lib/python3.11/site-packages (from httpx->llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in ./.venv/lib/python3.11/site-packages (from httpcore==1.*->httpx->llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface) (0.16.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in ./.venv/lib/python3.11/site-packages (from anyio->httpx->llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface) (1.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.11/site-packages (from jinja2->banks<3,>=2.2.0->llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: llama-index-postprocessor-flag-embedding-reranker in ./.venv/lib/python3.11/site-packages (0.4.0)\n",
      "Requirement already satisfied: llama-index-core<0.14,>=0.13.0 in ./.venv/lib/python3.11/site-packages (from llama-index-postprocessor-flag-embedding-reranker) (0.13.2)\n",
      "Requirement already satisfied: aiohttp<4,>=3.8.6 in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-postprocessor-flag-embedding-reranker) (3.12.15)\n",
      "Requirement already satisfied: aiosqlite in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-postprocessor-flag-embedding-reranker) (0.21.0)\n",
      "Requirement already satisfied: banks<3,>=2.2.0 in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-postprocessor-flag-embedding-reranker) (2.2.0)\n",
      "Requirement already satisfied: dataclasses-json in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-postprocessor-flag-embedding-reranker) (0.6.7)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-postprocessor-flag-embedding-reranker) (1.2.18)\n",
      "Requirement already satisfied: dirtyjson<2,>=1.0.8 in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-postprocessor-flag-embedding-reranker) (1.0.8)\n",
      "Requirement already satisfied: filetype<2,>=1.2.0 in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-postprocessor-flag-embedding-reranker) (1.2.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-postprocessor-flag-embedding-reranker) (2025.3.0)\n",
      "Requirement already satisfied: httpx in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-postprocessor-flag-embedding-reranker) (0.28.1)\n",
      "Requirement already satisfied: llama-index-workflows<2,>=1.0.1 in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-postprocessor-flag-embedding-reranker) (1.3.0)\n",
      "Requirement already satisfied: nest-asyncio<2,>=1.5.8 in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-postprocessor-flag-embedding-reranker) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-postprocessor-flag-embedding-reranker) (3.5)\n",
      "Requirement already satisfied: nltk>3.8.1 in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-postprocessor-flag-embedding-reranker) (3.9.1)\n",
      "Requirement already satisfied: numpy in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-postprocessor-flag-embedding-reranker) (2.2.6)\n",
      "Requirement already satisfied: pillow>=9.0.0 in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-postprocessor-flag-embedding-reranker) (11.3.0)\n",
      "Requirement already satisfied: platformdirs in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-postprocessor-flag-embedding-reranker) (4.3.8)\n",
      "Requirement already satisfied: pydantic>=2.8.0 in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-postprocessor-flag-embedding-reranker) (2.11.7)\n",
      "Requirement already satisfied: pyyaml>=6.0.1 in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-postprocessor-flag-embedding-reranker) (6.0.2)\n",
      "Requirement already satisfied: requests>=2.31.0 in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-postprocessor-flag-embedding-reranker) (2.32.4)\n",
      "Requirement already satisfied: setuptools>=80.9.0 in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-postprocessor-flag-embedding-reranker) (80.9.0)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.49 in ./.venv/lib/python3.11/site-packages (from sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.14,>=0.13.0->llama-index-postprocessor-flag-embedding-reranker) (2.0.43)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-postprocessor-flag-embedding-reranker) (9.1.2)\n",
      "Requirement already satisfied: tiktoken>=0.7.0 in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-postprocessor-flag-embedding-reranker) (0.11.0)\n",
      "Requirement already satisfied: tqdm<5,>=4.66.1 in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-postprocessor-flag-embedding-reranker) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-postprocessor-flag-embedding-reranker) (4.14.1)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-postprocessor-flag-embedding-reranker) (0.9.0)\n",
      "Requirement already satisfied: wrapt in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-postprocessor-flag-embedding-reranker) (1.17.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in ./.venv/lib/python3.11/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.14,>=0.13.0->llama-index-postprocessor-flag-embedding-reranker) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in ./.venv/lib/python3.11/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.14,>=0.13.0->llama-index-postprocessor-flag-embedding-reranker) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.venv/lib/python3.11/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.14,>=0.13.0->llama-index-postprocessor-flag-embedding-reranker) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.venv/lib/python3.11/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.14,>=0.13.0->llama-index-postprocessor-flag-embedding-reranker) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.venv/lib/python3.11/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.14,>=0.13.0->llama-index-postprocessor-flag-embedding-reranker) (6.6.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./.venv/lib/python3.11/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.14,>=0.13.0->llama-index-postprocessor-flag-embedding-reranker) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./.venv/lib/python3.11/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.14,>=0.13.0->llama-index-postprocessor-flag-embedding-reranker) (1.20.1)\n",
      "Requirement already satisfied: griffe in ./.venv/lib/python3.11/site-packages (from banks<3,>=2.2.0->llama-index-core<0.14,>=0.13.0->llama-index-postprocessor-flag-embedding-reranker) (1.12.1)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.11/site-packages (from banks<3,>=2.2.0->llama-index-core<0.14,>=0.13.0->llama-index-postprocessor-flag-embedding-reranker) (3.1.6)\n",
      "Requirement already satisfied: llama-index-instrumentation>=0.1.0 in ./.venv/lib/python3.11/site-packages (from llama-index-workflows<2,>=1.0.1->llama-index-core<0.14,>=0.13.0->llama-index-postprocessor-flag-embedding-reranker) (0.4.0)\n",
      "Requirement already satisfied: idna>=2.0 in ./.venv/lib/python3.11/site-packages (from yarl<2.0,>=1.17.0->aiohttp<4,>=3.8.6->llama-index-core<0.14,>=0.13.0->llama-index-postprocessor-flag-embedding-reranker) (3.10)\n",
      "Requirement already satisfied: click in ./.venv/lib/python3.11/site-packages (from nltk>3.8.1->llama-index-core<0.14,>=0.13.0->llama-index-postprocessor-flag-embedding-reranker) (8.2.1)\n",
      "Requirement already satisfied: joblib in ./.venv/lib/python3.11/site-packages (from nltk>3.8.1->llama-index-core<0.14,>=0.13.0->llama-index-postprocessor-flag-embedding-reranker) (1.5.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in ./.venv/lib/python3.11/site-packages (from nltk>3.8.1->llama-index-core<0.14,>=0.13.0->llama-index-postprocessor-flag-embedding-reranker) (2025.7.34)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./.venv/lib/python3.11/site-packages (from pydantic>=2.8.0->llama-index-core<0.14,>=0.13.0->llama-index-postprocessor-flag-embedding-reranker) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in ./.venv/lib/python3.11/site-packages (from pydantic>=2.8.0->llama-index-core<0.14,>=0.13.0->llama-index-postprocessor-flag-embedding-reranker) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in ./.venv/lib/python3.11/site-packages (from pydantic>=2.8.0->llama-index-core<0.14,>=0.13.0->llama-index-postprocessor-flag-embedding-reranker) (0.4.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.venv/lib/python3.11/site-packages (from requests>=2.31.0->llama-index-core<0.14,>=0.13.0->llama-index-postprocessor-flag-embedding-reranker) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.11/site-packages (from requests>=2.31.0->llama-index-core<0.14,>=0.13.0->llama-index-postprocessor-flag-embedding-reranker) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.11/site-packages (from requests>=2.31.0->llama-index-core<0.14,>=0.13.0->llama-index-postprocessor-flag-embedding-reranker) (2025.8.3)\n",
      "Requirement already satisfied: greenlet>=1 in ./.venv/lib/python3.11/site-packages (from sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.14,>=0.13.0->llama-index-postprocessor-flag-embedding-reranker) (3.2.4)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in ./.venv/lib/python3.11/site-packages (from typing-inspect>=0.8.0->llama-index-core<0.14,>=0.13.0->llama-index-postprocessor-flag-embedding-reranker) (1.1.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in ./.venv/lib/python3.11/site-packages (from dataclasses-json->llama-index-core<0.14,>=0.13.0->llama-index-postprocessor-flag-embedding-reranker) (3.26.1)\n",
      "Requirement already satisfied: packaging>=17.0 in ./.venv/lib/python3.11/site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.14,>=0.13.0->llama-index-postprocessor-flag-embedding-reranker) (25.0)\n",
      "Requirement already satisfied: colorama>=0.4 in ./.venv/lib/python3.11/site-packages (from griffe->banks<3,>=2.2.0->llama-index-core<0.14,>=0.13.0->llama-index-postprocessor-flag-embedding-reranker) (0.4.6)\n",
      "Requirement already satisfied: anyio in ./.venv/lib/python3.11/site-packages (from httpx->llama-index-core<0.14,>=0.13.0->llama-index-postprocessor-flag-embedding-reranker) (4.10.0)\n",
      "Requirement already satisfied: httpcore==1.* in ./.venv/lib/python3.11/site-packages (from httpx->llama-index-core<0.14,>=0.13.0->llama-index-postprocessor-flag-embedding-reranker) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in ./.venv/lib/python3.11/site-packages (from httpcore==1.*->httpx->llama-index-core<0.14,>=0.13.0->llama-index-postprocessor-flag-embedding-reranker) (0.16.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in ./.venv/lib/python3.11/site-packages (from anyio->httpx->llama-index-core<0.14,>=0.13.0->llama-index-postprocessor-flag-embedding-reranker) (1.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.11/site-packages (from jinja2->banks<3,>=2.2.0->llama-index-core<0.14,>=0.13.0->llama-index-postprocessor-flag-embedding-reranker) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: llama-index-postprocessor-flag-embedding-reranker in ./.venv/lib/python3.11/site-packages (0.4.0)\n",
      "Requirement already satisfied: llama-index-core<0.14,>=0.13.0 in ./.venv/lib/python3.11/site-packages (from llama-index-postprocessor-flag-embedding-reranker) (0.13.2)\n",
      "Requirement already satisfied: aiohttp<4,>=3.8.6 in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-postprocessor-flag-embedding-reranker) (3.12.15)\n",
      "Requirement already satisfied: aiosqlite in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-postprocessor-flag-embedding-reranker) (0.21.0)\n",
      "Requirement already satisfied: banks<3,>=2.2.0 in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-postprocessor-flag-embedding-reranker) (2.2.0)\n",
      "Requirement already satisfied: dataclasses-json in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-postprocessor-flag-embedding-reranker) (0.6.7)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-postprocessor-flag-embedding-reranker) (1.2.18)\n",
      "Requirement already satisfied: dirtyjson<2,>=1.0.8 in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-postprocessor-flag-embedding-reranker) (1.0.8)\n",
      "Requirement already satisfied: filetype<2,>=1.2.0 in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-postprocessor-flag-embedding-reranker) (1.2.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-postprocessor-flag-embedding-reranker) (2025.3.0)\n",
      "Requirement already satisfied: httpx in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-postprocessor-flag-embedding-reranker) (0.28.1)\n",
      "Requirement already satisfied: llama-index-workflows<2,>=1.0.1 in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-postprocessor-flag-embedding-reranker) (1.3.0)\n",
      "Requirement already satisfied: nest-asyncio<2,>=1.5.8 in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-postprocessor-flag-embedding-reranker) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-postprocessor-flag-embedding-reranker) (3.5)\n",
      "Requirement already satisfied: nltk>3.8.1 in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-postprocessor-flag-embedding-reranker) (3.9.1)\n",
      "Requirement already satisfied: numpy in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-postprocessor-flag-embedding-reranker) (2.2.6)\n",
      "Requirement already satisfied: pillow>=9.0.0 in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-postprocessor-flag-embedding-reranker) (11.3.0)\n",
      "Requirement already satisfied: platformdirs in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-postprocessor-flag-embedding-reranker) (4.3.8)\n",
      "Requirement already satisfied: pydantic>=2.8.0 in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-postprocessor-flag-embedding-reranker) (2.11.7)\n",
      "Requirement already satisfied: pyyaml>=6.0.1 in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-postprocessor-flag-embedding-reranker) (6.0.2)\n",
      "Requirement already satisfied: requests>=2.31.0 in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-postprocessor-flag-embedding-reranker) (2.32.4)\n",
      "Requirement already satisfied: setuptools>=80.9.0 in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-postprocessor-flag-embedding-reranker) (80.9.0)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.49 in ./.venv/lib/python3.11/site-packages (from sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.14,>=0.13.0->llama-index-postprocessor-flag-embedding-reranker) (2.0.43)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-postprocessor-flag-embedding-reranker) (9.1.2)\n",
      "Requirement already satisfied: tiktoken>=0.7.0 in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-postprocessor-flag-embedding-reranker) (0.11.0)\n",
      "Requirement already satisfied: tqdm<5,>=4.66.1 in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-postprocessor-flag-embedding-reranker) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-postprocessor-flag-embedding-reranker) (4.14.1)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-postprocessor-flag-embedding-reranker) (0.9.0)\n",
      "Requirement already satisfied: wrapt in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-postprocessor-flag-embedding-reranker) (1.17.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in ./.venv/lib/python3.11/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.14,>=0.13.0->llama-index-postprocessor-flag-embedding-reranker) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in ./.venv/lib/python3.11/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.14,>=0.13.0->llama-index-postprocessor-flag-embedding-reranker) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.venv/lib/python3.11/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.14,>=0.13.0->llama-index-postprocessor-flag-embedding-reranker) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.venv/lib/python3.11/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.14,>=0.13.0->llama-index-postprocessor-flag-embedding-reranker) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.venv/lib/python3.11/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.14,>=0.13.0->llama-index-postprocessor-flag-embedding-reranker) (6.6.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./.venv/lib/python3.11/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.14,>=0.13.0->llama-index-postprocessor-flag-embedding-reranker) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./.venv/lib/python3.11/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.14,>=0.13.0->llama-index-postprocessor-flag-embedding-reranker) (1.20.1)\n",
      "Requirement already satisfied: griffe in ./.venv/lib/python3.11/site-packages (from banks<3,>=2.2.0->llama-index-core<0.14,>=0.13.0->llama-index-postprocessor-flag-embedding-reranker) (1.12.1)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.11/site-packages (from banks<3,>=2.2.0->llama-index-core<0.14,>=0.13.0->llama-index-postprocessor-flag-embedding-reranker) (3.1.6)\n",
      "Requirement already satisfied: llama-index-instrumentation>=0.1.0 in ./.venv/lib/python3.11/site-packages (from llama-index-workflows<2,>=1.0.1->llama-index-core<0.14,>=0.13.0->llama-index-postprocessor-flag-embedding-reranker) (0.4.0)\n",
      "Requirement already satisfied: idna>=2.0 in ./.venv/lib/python3.11/site-packages (from yarl<2.0,>=1.17.0->aiohttp<4,>=3.8.6->llama-index-core<0.14,>=0.13.0->llama-index-postprocessor-flag-embedding-reranker) (3.10)\n",
      "Requirement already satisfied: click in ./.venv/lib/python3.11/site-packages (from nltk>3.8.1->llama-index-core<0.14,>=0.13.0->llama-index-postprocessor-flag-embedding-reranker) (8.2.1)\n",
      "Requirement already satisfied: joblib in ./.venv/lib/python3.11/site-packages (from nltk>3.8.1->llama-index-core<0.14,>=0.13.0->llama-index-postprocessor-flag-embedding-reranker) (1.5.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in ./.venv/lib/python3.11/site-packages (from nltk>3.8.1->llama-index-core<0.14,>=0.13.0->llama-index-postprocessor-flag-embedding-reranker) (2025.7.34)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./.venv/lib/python3.11/site-packages (from pydantic>=2.8.0->llama-index-core<0.14,>=0.13.0->llama-index-postprocessor-flag-embedding-reranker) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in ./.venv/lib/python3.11/site-packages (from pydantic>=2.8.0->llama-index-core<0.14,>=0.13.0->llama-index-postprocessor-flag-embedding-reranker) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in ./.venv/lib/python3.11/site-packages (from pydantic>=2.8.0->llama-index-core<0.14,>=0.13.0->llama-index-postprocessor-flag-embedding-reranker) (0.4.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.venv/lib/python3.11/site-packages (from requests>=2.31.0->llama-index-core<0.14,>=0.13.0->llama-index-postprocessor-flag-embedding-reranker) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.11/site-packages (from requests>=2.31.0->llama-index-core<0.14,>=0.13.0->llama-index-postprocessor-flag-embedding-reranker) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.11/site-packages (from requests>=2.31.0->llama-index-core<0.14,>=0.13.0->llama-index-postprocessor-flag-embedding-reranker) (2025.8.3)\n",
      "Requirement already satisfied: greenlet>=1 in ./.venv/lib/python3.11/site-packages (from sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.14,>=0.13.0->llama-index-postprocessor-flag-embedding-reranker) (3.2.4)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in ./.venv/lib/python3.11/site-packages (from typing-inspect>=0.8.0->llama-index-core<0.14,>=0.13.0->llama-index-postprocessor-flag-embedding-reranker) (1.1.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in ./.venv/lib/python3.11/site-packages (from dataclasses-json->llama-index-core<0.14,>=0.13.0->llama-index-postprocessor-flag-embedding-reranker) (3.26.1)\n",
      "Requirement already satisfied: packaging>=17.0 in ./.venv/lib/python3.11/site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.14,>=0.13.0->llama-index-postprocessor-flag-embedding-reranker) (25.0)\n",
      "Requirement already satisfied: colorama>=0.4 in ./.venv/lib/python3.11/site-packages (from griffe->banks<3,>=2.2.0->llama-index-core<0.14,>=0.13.0->llama-index-postprocessor-flag-embedding-reranker) (0.4.6)\n",
      "Requirement already satisfied: anyio in ./.venv/lib/python3.11/site-packages (from httpx->llama-index-core<0.14,>=0.13.0->llama-index-postprocessor-flag-embedding-reranker) (4.10.0)\n",
      "Requirement already satisfied: httpcore==1.* in ./.venv/lib/python3.11/site-packages (from httpx->llama-index-core<0.14,>=0.13.0->llama-index-postprocessor-flag-embedding-reranker) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in ./.venv/lib/python3.11/site-packages (from httpcore==1.*->httpx->llama-index-core<0.14,>=0.13.0->llama-index-postprocessor-flag-embedding-reranker) (0.16.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in ./.venv/lib/python3.11/site-packages (from anyio->httpx->llama-index-core<0.14,>=0.13.0->llama-index-postprocessor-flag-embedding-reranker) (1.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.11/site-packages (from jinja2->banks<3,>=2.2.0->llama-index-core<0.14,>=0.13.0->llama-index-postprocessor-flag-embedding-reranker) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: llama-index-llms-huggingface-api in ./.venv/lib/python3.11/site-packages (0.6.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.31.0 in ./.venv/lib/python3.11/site-packages (from llama-index-llms-huggingface-api) (0.34.4)\n",
      "Requirement already satisfied: llama-index-core<0.14,>=0.13.0 in ./.venv/lib/python3.11/site-packages (from llama-index-llms-huggingface-api) (0.13.2)\n",
      "Requirement already satisfied: aiohttp<4,>=3.8.6 in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface-api) (3.12.15)\n",
      "Requirement already satisfied: aiosqlite in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface-api) (0.21.0)\n",
      "Requirement already satisfied: banks<3,>=2.2.0 in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface-api) (2.2.0)\n",
      "Requirement already satisfied: dataclasses-json in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface-api) (0.6.7)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface-api) (1.2.18)\n",
      "Requirement already satisfied: dirtyjson<2,>=1.0.8 in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface-api) (1.0.8)\n",
      "Requirement already satisfied: filetype<2,>=1.2.0 in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface-api) (1.2.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface-api) (2025.3.0)\n",
      "Requirement already satisfied: httpx in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface-api) (0.28.1)\n",
      "Requirement already satisfied: llama-index-workflows<2,>=1.0.1 in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface-api) (1.3.0)\n",
      "Requirement already satisfied: nest-asyncio<2,>=1.5.8 in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface-api) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface-api) (3.5)\n",
      "Requirement already satisfied: nltk>3.8.1 in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface-api) (3.9.1)\n",
      "Requirement already satisfied: numpy in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface-api) (2.2.6)\n",
      "Requirement already satisfied: pillow>=9.0.0 in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface-api) (11.3.0)\n",
      "Requirement already satisfied: platformdirs in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface-api) (4.3.8)\n",
      "Requirement already satisfied: pydantic>=2.8.0 in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface-api) (2.11.7)\n",
      "Requirement already satisfied: pyyaml>=6.0.1 in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface-api) (6.0.2)\n",
      "Requirement already satisfied: requests>=2.31.0 in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface-api) (2.32.4)\n",
      "Requirement already satisfied: setuptools>=80.9.0 in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface-api) (80.9.0)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.49 in ./.venv/lib/python3.11/site-packages (from sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface-api) (2.0.43)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface-api) (9.1.2)\n",
      "Requirement already satisfied: tiktoken>=0.7.0 in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface-api) (0.11.0)\n",
      "Requirement already satisfied: tqdm<5,>=4.66.1 in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface-api) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface-api) (4.14.1)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface-api) (0.9.0)\n",
      "Requirement already satisfied: wrapt in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface-api) (1.17.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in ./.venv/lib/python3.11/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface-api) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in ./.venv/lib/python3.11/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface-api) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.venv/lib/python3.11/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface-api) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.venv/lib/python3.11/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface-api) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.venv/lib/python3.11/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface-api) (6.6.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./.venv/lib/python3.11/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface-api) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./.venv/lib/python3.11/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface-api) (1.20.1)\n",
      "Requirement already satisfied: griffe in ./.venv/lib/python3.11/site-packages (from banks<3,>=2.2.0->llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface-api) (1.12.1)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.11/site-packages (from banks<3,>=2.2.0->llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface-api) (3.1.6)\n",
      "Requirement already satisfied: llama-index-instrumentation>=0.1.0 in ./.venv/lib/python3.11/site-packages (from llama-index-workflows<2,>=1.0.1->llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface-api) (0.4.0)\n",
      "Requirement already satisfied: idna>=2.0 in ./.venv/lib/python3.11/site-packages (from yarl<2.0,>=1.17.0->aiohttp<4,>=3.8.6->llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface-api) (3.10)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.11/site-packages (from huggingface-hub>=0.31.0->llama-index-llms-huggingface-api) (3.19.1)\n",
      "Requirement already satisfied: packaging>=20.9 in ./.venv/lib/python3.11/site-packages (from huggingface-hub>=0.31.0->llama-index-llms-huggingface-api) (25.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./.venv/lib/python3.11/site-packages (from huggingface-hub>=0.31.0->llama-index-llms-huggingface-api) (1.1.7)\n",
      "Requirement already satisfied: click in ./.venv/lib/python3.11/site-packages (from nltk>3.8.1->llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface-api) (8.2.1)\n",
      "Requirement already satisfied: joblib in ./.venv/lib/python3.11/site-packages (from nltk>3.8.1->llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface-api) (1.5.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in ./.venv/lib/python3.11/site-packages (from nltk>3.8.1->llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface-api) (2025.7.34)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./.venv/lib/python3.11/site-packages (from pydantic>=2.8.0->llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface-api) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in ./.venv/lib/python3.11/site-packages (from pydantic>=2.8.0->llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface-api) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in ./.venv/lib/python3.11/site-packages (from pydantic>=2.8.0->llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface-api) (0.4.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.venv/lib/python3.11/site-packages (from requests>=2.31.0->llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface-api) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.11/site-packages (from requests>=2.31.0->llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface-api) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.11/site-packages (from requests>=2.31.0->llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface-api) (2025.8.3)\n",
      "Requirement already satisfied: greenlet>=1 in ./.venv/lib/python3.11/site-packages (from sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface-api) (3.2.4)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in ./.venv/lib/python3.11/site-packages (from typing-inspect>=0.8.0->llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface-api) (1.1.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in ./.venv/lib/python3.11/site-packages (from dataclasses-json->llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface-api) (3.26.1)\n",
      "Requirement already satisfied: colorama>=0.4 in ./.venv/lib/python3.11/site-packages (from griffe->banks<3,>=2.2.0->llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface-api) (0.4.6)\n",
      "Requirement already satisfied: anyio in ./.venv/lib/python3.11/site-packages (from httpx->llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface-api) (4.10.0)\n",
      "Requirement already satisfied: httpcore==1.* in ./.venv/lib/python3.11/site-packages (from httpx->llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface-api) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in ./.venv/lib/python3.11/site-packages (from httpcore==1.*->httpx->llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface-api) (0.16.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in ./.venv/lib/python3.11/site-packages (from anyio->httpx->llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface-api) (1.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.11/site-packages (from jinja2->banks<3,>=2.2.0->llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface-api) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement bitsandbytes==0.43.1 (from versions: 0.31.8, 0.32.0, 0.32.1, 0.32.2, 0.32.3, 0.33.0, 0.33.1, 0.34.0, 0.35.0, 0.35.1, 0.35.2, 0.35.3, 0.35.4, 0.36.0, 0.36.0.post1, 0.36.0.post2, 0.37.0, 0.37.1, 0.37.2, 0.38.0, 0.38.0.post1, 0.38.0.post2, 0.38.1, 0.39.0, 0.39.1, 0.40.0, 0.40.0.post1, 0.40.0.post2, 0.40.0.post3, 0.40.0.post4, 0.40.1, 0.40.1.post1, 0.40.2, 0.41.0, 0.41.1, 0.41.2, 0.41.2.post1, 0.41.2.post2, 0.41.3, 0.41.3.post1, 0.41.3.post2, 0.42.0)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for bitsandbytes==0.43.1\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: ctransformers in ./.venv/lib/python3.11/site-packages (0.2.27)\n",
      "Requirement already satisfied: huggingface-hub in ./.venv/lib/python3.11/site-packages (from ctransformers) (0.34.4)\n",
      "Requirement already satisfied: py-cpuinfo<10.0.0,>=9.0.0 in ./.venv/lib/python3.11/site-packages (from ctransformers) (9.0.0)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.11/site-packages (from huggingface-hub->ctransformers) (3.19.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.venv/lib/python3.11/site-packages (from huggingface-hub->ctransformers) (2025.3.0)\n",
      "Requirement already satisfied: packaging>=20.9 in ./.venv/lib/python3.11/site-packages (from huggingface-hub->ctransformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.11/site-packages (from huggingface-hub->ctransformers) (6.0.2)\n",
      "Requirement already satisfied: requests in ./.venv/lib/python3.11/site-packages (from huggingface-hub->ctransformers) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in ./.venv/lib/python3.11/site-packages (from huggingface-hub->ctransformers) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.venv/lib/python3.11/site-packages (from huggingface-hub->ctransformers) (4.14.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./.venv/lib/python3.11/site-packages (from huggingface-hub->ctransformers) (1.1.7)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.venv/lib/python3.11/site-packages (from requests->huggingface-hub->ctransformers) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.11/site-packages (from requests->huggingface-hub->ctransformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.11/site-packages (from requests->huggingface-hub->ctransformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.11/site-packages (from requests->huggingface-hub->ctransformers) (2025.8.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: llama-cpp-python in ./.venv/lib/python3.11/site-packages (0.3.16)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in ./.venv/lib/python3.11/site-packages (from llama-cpp-python) (4.14.1)\n",
      "Requirement already satisfied: numpy>=1.20.0 in ./.venv/lib/python3.11/site-packages (from llama-cpp-python) (2.2.6)\n",
      "Requirement already satisfied: diskcache>=5.6.1 in ./.venv/lib/python3.11/site-packages (from llama-cpp-python) (5.6.3)\n",
      "Requirement already satisfied: jinja2>=2.11.3 in ./.venv/lib/python3.11/site-packages (from llama-cpp-python) (3.1.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.11/site-packages (from jinja2>=2.11.3->llama-cpp-python) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: torch in ./.venv/lib/python3.11/site-packages (2.8.0)\n",
      "Requirement already satisfied: torchvision in ./.venv/lib/python3.11/site-packages (0.23.0)\n",
      "Requirement already satisfied: torchaudio in ./.venv/lib/python3.11/site-packages (2.8.0)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.11/site-packages (from torch) (3.19.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in ./.venv/lib/python3.11/site-packages (from torch) (4.14.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in ./.venv/lib/python3.11/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in ./.venv/lib/python3.11/site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.11/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in ./.venv/lib/python3.11/site-packages (from torch) (2025.3.0)\n",
      "Requirement already satisfied: numpy in ./.venv/lib/python3.11/site-packages (from torchvision) (2.2.6)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in ./.venv/lib/python3.11/site-packages (from torchvision) (11.3.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.11/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.11/site-packages (from jinja2->torch) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: accelerate in ./.venv/lib/python3.11/site-packages (1.10.0)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in ./.venv/lib/python3.11/site-packages (from accelerate) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.11/site-packages (from accelerate) (25.0)\n",
      "Requirement already satisfied: psutil in ./.venv/lib/python3.11/site-packages (from accelerate) (7.0.0)\n",
      "Requirement already satisfied: pyyaml in ./.venv/lib/python3.11/site-packages (from accelerate) (6.0.2)\n",
      "Requirement already satisfied: torch>=2.0.0 in ./.venv/lib/python3.11/site-packages (from accelerate) (2.8.0)\n",
      "Requirement already satisfied: huggingface_hub>=0.21.0 in ./.venv/lib/python3.11/site-packages (from accelerate) (0.34.4)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in ./.venv/lib/python3.11/site-packages (from accelerate) (0.6.2)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.11/site-packages (from huggingface_hub>=0.21.0->accelerate) (3.19.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.venv/lib/python3.11/site-packages (from huggingface_hub>=0.21.0->accelerate) (2025.3.0)\n",
      "Requirement already satisfied: requests in ./.venv/lib/python3.11/site-packages (from huggingface_hub>=0.21.0->accelerate) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in ./.venv/lib/python3.11/site-packages (from huggingface_hub>=0.21.0->accelerate) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.venv/lib/python3.11/site-packages (from huggingface_hub>=0.21.0->accelerate) (4.14.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./.venv/lib/python3.11/site-packages (from huggingface_hub>=0.21.0->accelerate) (1.1.7)\n",
      "Requirement already satisfied: sympy>=1.13.3 in ./.venv/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (1.14.0)\n",
      "Requirement already satisfied: networkx in ./.venv/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (3.5)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.11/site-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.11/site-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.venv/lib/python3.11/site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.11/site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.11/site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.11/site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2025.8.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: transformers in ./.venv/lib/python3.11/site-packages (4.55.2)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.11/site-packages (from transformers) (3.19.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in ./.venv/lib/python3.11/site-packages (from transformers) (0.34.4)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.venv/lib/python3.11/site-packages (from transformers) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.11/site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.11/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.venv/lib/python3.11/site-packages (from transformers) (2025.7.34)\n",
      "Requirement already satisfied: requests in ./.venv/lib/python3.11/site-packages (from transformers) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in ./.venv/lib/python3.11/site-packages (from transformers) (0.21.4)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in ./.venv/lib/python3.11/site-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./.venv/lib/python3.11/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.venv/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.venv/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.14.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./.venv/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.7)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.venv/lib/python3.11/site-packages (from requests->transformers) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.11/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.11/site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.11/site-packages (from requests->transformers) (2025.8.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "zsh:1: 2.4.2 not found\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: einops in ./.venv/lib/python3.11/site-packages (0.8.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: huggingface_hub in ./.venv/lib/python3.11/site-packages (0.34.4)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.11/site-packages (from huggingface_hub) (3.19.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.venv/lib/python3.11/site-packages (from huggingface_hub) (2025.3.0)\n",
      "Requirement already satisfied: packaging>=20.9 in ./.venv/lib/python3.11/site-packages (from huggingface_hub) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.11/site-packages (from huggingface_hub) (6.0.2)\n",
      "Requirement already satisfied: requests in ./.venv/lib/python3.11/site-packages (from huggingface_hub) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in ./.venv/lib/python3.11/site-packages (from huggingface_hub) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.venv/lib/python3.11/site-packages (from huggingface_hub) (4.14.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./.venv/lib/python3.11/site-packages (from huggingface_hub) (1.1.7)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.venv/lib/python3.11/site-packages (from requests->huggingface_hub) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.11/site-packages (from requests->huggingface_hub) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.11/site-packages (from requests->huggingface_hub) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.11/site-packages (from requests->huggingface_hub) (2025.8.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting git+https://github.com/FlagOpen/FlagEmbedding.git\n",
      "  Cloning https://github.com/FlagOpen/FlagEmbedding.git to /private/var/folders/_x/v81sxk6j38zdg2y882m0qmj40000gn/T/pip-req-build-exs9pjuv\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/FlagOpen/FlagEmbedding.git /private/var/folders/_x/v81sxk6j38zdg2y882m0qmj40000gn/T/pip-req-build-exs9pjuv\n",
      "  Resolved https://github.com/FlagOpen/FlagEmbedding.git to commit 7780cf3135661d34657d788ed95207a53937e13c\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: torch>=1.6.0 in ./.venv/lib/python3.11/site-packages (from FlagEmbedding==1.3.5) (2.8.0)\n",
      "Requirement already satisfied: transformers>=4.44.2 in ./.venv/lib/python3.11/site-packages (from FlagEmbedding==1.3.5) (4.55.2)\n",
      "Requirement already satisfied: datasets>=2.19.0 in ./.venv/lib/python3.11/site-packages (from FlagEmbedding==1.3.5) (4.0.0)\n",
      "Requirement already satisfied: accelerate>=0.20.1 in ./.venv/lib/python3.11/site-packages (from FlagEmbedding==1.3.5) (1.10.0)\n",
      "Requirement already satisfied: sentence_transformers in ./.venv/lib/python3.11/site-packages (from FlagEmbedding==1.3.5) (5.1.0)\n",
      "Requirement already satisfied: peft in ./.venv/lib/python3.11/site-packages (from FlagEmbedding==1.3.5) (0.17.0)\n",
      "Requirement already satisfied: ir-datasets in ./.venv/lib/python3.11/site-packages (from FlagEmbedding==1.3.5) (0.5.11)\n",
      "Requirement already satisfied: sentencepiece in ./.venv/lib/python3.11/site-packages (from FlagEmbedding==1.3.5) (0.2.1)\n",
      "Requirement already satisfied: protobuf in ./.venv/lib/python3.11/site-packages (from FlagEmbedding==1.3.5) (6.32.0)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in ./.venv/lib/python3.11/site-packages (from accelerate>=0.20.1->FlagEmbedding==1.3.5) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.11/site-packages (from accelerate>=0.20.1->FlagEmbedding==1.3.5) (25.0)\n",
      "Requirement already satisfied: psutil in ./.venv/lib/python3.11/site-packages (from accelerate>=0.20.1->FlagEmbedding==1.3.5) (7.0.0)\n",
      "Requirement already satisfied: pyyaml in ./.venv/lib/python3.11/site-packages (from accelerate>=0.20.1->FlagEmbedding==1.3.5) (6.0.2)\n",
      "Requirement already satisfied: huggingface_hub>=0.21.0 in ./.venv/lib/python3.11/site-packages (from accelerate>=0.20.1->FlagEmbedding==1.3.5) (0.34.4)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in ./.venv/lib/python3.11/site-packages (from accelerate>=0.20.1->FlagEmbedding==1.3.5) (0.6.2)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.11/site-packages (from datasets>=2.19.0->FlagEmbedding==1.3.5) (3.19.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in ./.venv/lib/python3.11/site-packages (from datasets>=2.19.0->FlagEmbedding==1.3.5) (21.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in ./.venv/lib/python3.11/site-packages (from datasets>=2.19.0->FlagEmbedding==1.3.5) (0.3.8)\n",
      "Requirement already satisfied: pandas in ./.venv/lib/python3.11/site-packages (from datasets>=2.19.0->FlagEmbedding==1.3.5) (2.3.1)\n",
      "Requirement already satisfied: requests>=2.32.2 in ./.venv/lib/python3.11/site-packages (from datasets>=2.19.0->FlagEmbedding==1.3.5) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in ./.venv/lib/python3.11/site-packages (from datasets>=2.19.0->FlagEmbedding==1.3.5) (4.67.1)\n",
      "Requirement already satisfied: xxhash in ./.venv/lib/python3.11/site-packages (from datasets>=2.19.0->FlagEmbedding==1.3.5) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in ./.venv/lib/python3.11/site-packages (from datasets>=2.19.0->FlagEmbedding==1.3.5) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in ./.venv/lib/python3.11/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.19.0->FlagEmbedding==1.3.5) (2025.3.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in ./.venv/lib/python3.11/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.19.0->FlagEmbedding==1.3.5) (3.12.15)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in ./.venv/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.19.0->FlagEmbedding==1.3.5) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in ./.venv/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.19.0->FlagEmbedding==1.3.5) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.venv/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.19.0->FlagEmbedding==1.3.5) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.venv/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.19.0->FlagEmbedding==1.3.5) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.venv/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.19.0->FlagEmbedding==1.3.5) (6.6.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./.venv/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.19.0->FlagEmbedding==1.3.5) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./.venv/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.19.0->FlagEmbedding==1.3.5) (1.20.1)\n",
      "Requirement already satisfied: idna>=2.0 in ./.venv/lib/python3.11/site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.19.0->FlagEmbedding==1.3.5) (3.10)\n",
      "Requirement already satisfied: typing-extensions>=4.2 in ./.venv/lib/python3.11/site-packages (from aiosignal>=1.4.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.19.0->FlagEmbedding==1.3.5) (4.14.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./.venv/lib/python3.11/site-packages (from huggingface_hub>=0.21.0->accelerate>=0.20.1->FlagEmbedding==1.3.5) (1.1.7)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.venv/lib/python3.11/site-packages (from requests>=2.32.2->datasets>=2.19.0->FlagEmbedding==1.3.5) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.11/site-packages (from requests>=2.32.2->datasets>=2.19.0->FlagEmbedding==1.3.5) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.11/site-packages (from requests>=2.32.2->datasets>=2.19.0->FlagEmbedding==1.3.5) (2025.8.3)\n",
      "Requirement already satisfied: sympy>=1.13.3 in ./.venv/lib/python3.11/site-packages (from torch>=1.6.0->FlagEmbedding==1.3.5) (1.14.0)\n",
      "Requirement already satisfied: networkx in ./.venv/lib/python3.11/site-packages (from torch>=1.6.0->FlagEmbedding==1.3.5) (3.5)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.11/site-packages (from torch>=1.6.0->FlagEmbedding==1.3.5) (3.1.6)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.11/site-packages (from sympy>=1.13.3->torch>=1.6.0->FlagEmbedding==1.3.5) (1.3.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.venv/lib/python3.11/site-packages (from transformers>=4.44.2->FlagEmbedding==1.3.5) (2025.7.34)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in ./.venv/lib/python3.11/site-packages (from transformers>=4.44.2->FlagEmbedding==1.3.5) (0.21.4)\n",
      "Requirement already satisfied: beautifulsoup4>=4.4.1 in ./.venv/lib/python3.11/site-packages (from ir-datasets->FlagEmbedding==1.3.5) (4.13.4)\n",
      "Requirement already satisfied: inscriptis>=2.2.0 in ./.venv/lib/python3.11/site-packages (from ir-datasets->FlagEmbedding==1.3.5) (2.6.0)\n",
      "Requirement already satisfied: lxml>=4.5.2 in ./.venv/lib/python3.11/site-packages (from ir-datasets->FlagEmbedding==1.3.5) (5.4.0)\n",
      "Requirement already satisfied: trec-car-tools>=2.5.4 in ./.venv/lib/python3.11/site-packages (from ir-datasets->FlagEmbedding==1.3.5) (2.6)\n",
      "Requirement already satisfied: lz4>=3.1.10 in ./.venv/lib/python3.11/site-packages (from ir-datasets->FlagEmbedding==1.3.5) (4.4.4)\n",
      "Requirement already satisfied: warc3-wet>=0.2.3 in ./.venv/lib/python3.11/site-packages (from ir-datasets->FlagEmbedding==1.3.5) (0.2.5)\n",
      "Requirement already satisfied: warc3-wet-clueweb09>=0.2.5 in ./.venv/lib/python3.11/site-packages (from ir-datasets->FlagEmbedding==1.3.5) (0.2.5)\n",
      "Requirement already satisfied: zlib-state>=0.1.3 in ./.venv/lib/python3.11/site-packages (from ir-datasets->FlagEmbedding==1.3.5) (0.1.9)\n",
      "Requirement already satisfied: ijson>=3.1.3 in ./.venv/lib/python3.11/site-packages (from ir-datasets->FlagEmbedding==1.3.5) (3.4.0)\n",
      "Requirement already satisfied: unlzw3>=0.2.1 in ./.venv/lib/python3.11/site-packages (from ir-datasets->FlagEmbedding==1.3.5) (0.2.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in ./.venv/lib/python3.11/site-packages (from beautifulsoup4>=4.4.1->ir-datasets->FlagEmbedding==1.3.5) (2.7)\n",
      "Requirement already satisfied: cbor>=1.0.0 in ./.venv/lib/python3.11/site-packages (from trec-car-tools>=2.5.4->ir-datasets->FlagEmbedding==1.3.5) (1.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.11/site-packages (from jinja2->torch>=1.6.0->FlagEmbedding==1.3.5) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.11/site-packages (from pandas->datasets>=2.19.0->FlagEmbedding==1.3.5) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.11/site-packages (from pandas->datasets>=2.19.0->FlagEmbedding==1.3.5) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.11/site-packages (from pandas->datasets>=2.19.0->FlagEmbedding==1.3.5) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.19.0->FlagEmbedding==1.3.5) (1.17.0)\n",
      "Requirement already satisfied: scikit-learn in ./.venv/lib/python3.11/site-packages (from sentence_transformers->FlagEmbedding==1.3.5) (1.7.1)\n",
      "Requirement already satisfied: scipy in ./.venv/lib/python3.11/site-packages (from sentence_transformers->FlagEmbedding==1.3.5) (1.16.1)\n",
      "Requirement already satisfied: Pillow in ./.venv/lib/python3.11/site-packages (from sentence_transformers->FlagEmbedding==1.3.5) (11.3.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in ./.venv/lib/python3.11/site-packages (from scikit-learn->sentence_transformers->FlagEmbedding==1.3.5) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./.venv/lib/python3.11/site-packages (from scikit-learn->sentence_transformers->FlagEmbedding==1.3.5) (3.6.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/varunraste/Downloads/RAG_demo/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m  Warning: 'huggingface-cli login' is deprecated. Use 'hf auth login' instead.\u001b[0m\n",
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `hf`CLI if you want to set the git credential as well.\n",
      "Token is valid (permission: fineGrained).\n",
      "The token `rag_demo_trial` has been saved to /Users/varunraste/.cache/huggingface/stored_tokens\n",
      "Your token has been saved to /Users/varunraste/.cache/huggingface/token\n",
      "Login successful.\n",
      "The current active token is: `rag_demo_trial`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_load_from_file_impl: using device Metal (Apple M3) - 16383 MiB free\n",
      "llama_model_loader: loaded meta data with 37 key-value pairs and 291 tensors from ./models/granite-7b-lab-Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = Instructlab Granite 7b Lab\n",
      "llama_model_loader: - kv   3:                           general.finetune str              = lab\n",
      "llama_model_loader: - kv   4:                           general.basename str              = instructlab-granite\n",
      "llama_model_loader: - kv   5:                         general.size_label str              = 7B\n",
      "llama_model_loader: - kv   6:                            general.license str              = apache-2.0\n",
      "llama_model_loader: - kv   7:                   general.base_model.count u32              = 1\n",
      "llama_model_loader: - kv   8:                  general.base_model.0.name str              = Granite 7b Base\n",
      "llama_model_loader: - kv   9:          general.base_model.0.organization str              = Ibm\n",
      "llama_model_loader: - kv  10:              general.base_model.0.repo_url str              = https://huggingface.co/ibm/granite-7b...\n",
      "llama_model_loader: - kv  11:                               general.tags arr[str,6]       = [\"granite\", \"ibm\", \"lab\", \"labrador\",...\n",
      "llama_model_loader: - kv  12:                          general.languages arr[str,1]       = [\"en\"]\n",
      "llama_model_loader: - kv  13:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv  14:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv  15:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv  16:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv  17:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  18:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv  19:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  20:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  21:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  22:                           llama.vocab_size u32              = 32008\n",
      "llama_model_loader: - kv  23:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  24:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  25:                         tokenizer.ggml.pre str              = default\n",
      "llama_model_loader: - kv  26:                      tokenizer.ggml.tokens arr[str,32008]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  27:                      tokenizer.ggml.scores arr[f32,32008]   = [-1000.000000, -1000.000000, -1000.00...\n",
      "llama_model_loader: - kv  28:                  tokenizer.ggml.token_type arr[i32,32008]   = [3, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  29:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  30:                tokenizer.ggml.eos_token_id u32              = 32000\n",
      "llama_model_loader: - kv  31:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  32:            tokenizer.ggml.padding_token_id u32              = 32001\n",
      "llama_model_loader: - kv  33:               tokenizer.ggml.add_bos_token bool             = false\n",
      "llama_model_loader: - kv  34:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  35:                    tokenizer.chat_template str              = {% for message in messages %}{% if me...\n",
      "llama_model_loader: - kv  36:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "print_info: file format = GGUF V3 (latest)\n",
      "print_info: file type   = Q4_K - Medium\n",
      "print_info: file size   = 3.80 GiB (4.84 BPW) \n",
      "init_tokenizer: initializing tokenizer for type 1\n",
      "load: control token:  32004 '<|system|>' is not marked as EOG\n",
      "load: control token:  32003 '<|assistant|>' is not marked as EOG\n",
      "load: control token:  32002 '<|user|>' is not marked as EOG\n",
      "load: control token:  32001 '<|pad|>' is not marked as EOG\n",
      "load: control token:      1 '<s>' is not marked as EOG\n",
      "load: control token:      0 '<unk>' is not marked as EOG\n",
      "load: control token:      2 '</s>' is not marked as EOG\n",
      "load: printing all EOG tokens:\n",
      "load:   - 32000 ('<|endoftext|>')\n",
      "load: special tokens cache size = 8\n",
      "load: token to piece cache size = 0.1684 MB\n",
      "print_info: arch             = llama\n",
      "print_info: vocab_only       = 0\n",
      "print_info: n_ctx_train      = 4096\n",
      "print_info: n_embd           = 4096\n",
      "print_info: n_layer          = 32\n",
      "print_info: n_head           = 32\n",
      "print_info: n_head_kv        = 32\n",
      "print_info: n_rot            = 128\n",
      "print_info: n_swa            = 0\n",
      "print_info: is_swa_any       = 0\n",
      "print_info: n_embd_head_k    = 128\n",
      "print_info: n_embd_head_v    = 128\n",
      "print_info: n_gqa            = 1\n",
      "print_info: n_embd_k_gqa     = 4096\n",
      "print_info: n_embd_v_gqa     = 4096\n",
      "print_info: f_norm_eps       = 0.0e+00\n",
      "print_info: f_norm_rms_eps   = 1.0e-05\n",
      "print_info: f_clamp_kqv      = 0.0e+00\n",
      "print_info: f_max_alibi_bias = 0.0e+00\n",
      "print_info: f_logit_scale    = 0.0e+00\n",
      "print_info: f_attn_scale     = 0.0e+00\n",
      "print_info: n_ff             = 11008\n",
      "print_info: n_expert         = 0\n",
      "print_info: n_expert_used    = 0\n",
      "print_info: causal attn      = 1\n",
      "print_info: pooling type     = 0\n",
      "print_info: rope type        = 0\n",
      "print_info: rope scaling     = linear\n",
      "print_info: freq_base_train  = 10000.0\n",
      "print_info: freq_scale_train = 1\n",
      "print_info: n_ctx_orig_yarn  = 4096\n",
      "print_info: rope_finetuned   = unknown\n",
      "print_info: model type       = 7B\n",
      "print_info: model params     = 6.74 B\n",
      "print_info: general.name     = Instructlab Granite 7b Lab\n",
      "print_info: vocab type       = SPM\n",
      "print_info: n_vocab          = 32008\n",
      "print_info: n_merges         = 0\n",
      "print_info: BOS token        = 1 '<s>'\n",
      "print_info: EOS token        = 32000 '<|endoftext|>'\n",
      "print_info: EOT token        = 32000 '<|endoftext|>'\n",
      "print_info: UNK token        = 0 '<unk>'\n",
      "print_info: PAD token        = 32001 '<|pad|>'\n",
      "print_info: LF token         = 13 '<0x0A>'\n",
      "print_info: EOG token        = 32000 '<|endoftext|>'\n",
      "print_info: max token length = 48\n",
      "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
      "load_tensors: layer   0 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer   1 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer   2 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer   3 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer   4 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer   5 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer   6 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer   7 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer   8 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer   9 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  10 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  11 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  12 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  13 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  14 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  15 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  16 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  17 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  18 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  19 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  20 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  21 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  22 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  23 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  24 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  25 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  26 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  27 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  28 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  29 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  30 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  31 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  32 assigned to device Metal, is_swa = 0\n",
      "load_tensors: tensor 'token_embd.weight' (q4_K) (and 0 others) cannot be used with preferred buffer type CPU_REPACK, using CPU instead\n",
      "ggml_backend_metal_log_allocated_size: allocated buffer, size =  3891.30 MiB, ( 3891.38 / 16384.02)\n",
      "load_tensors: offloading 32 repeating layers to GPU\n",
      "load_tensors: offloading output layer to GPU\n",
      "load_tensors: offloaded 33/33 layers to GPU\n",
      "load_tensors: Metal_Mapped model buffer size =  3891.29 MiB\n",
      "load_tensors:   CPU_Mapped model buffer size =    70.33 MiB\n",
      "..................................................................................................\n",
      "llama_context: constructing llama_context\n",
      "llama_context: n_seq_max     = 1\n",
      "llama_context: n_ctx         = 4096\n",
      "llama_context: n_ctx_per_seq = 4096\n",
      "llama_context: n_batch       = 512\n",
      "llama_context: n_ubatch      = 512\n",
      "llama_context: causal_attn   = 1\n",
      "llama_context: flash_attn    = 0\n",
      "llama_context: kv_unified    = false\n",
      "llama_context: freq_base     = 10000.0\n",
      "llama_context: freq_scale    = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M3\n",
      "ggml_metal_init: picking default device: Apple M3\n",
      "ggml_metal_load_library: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M3\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction   = true\n",
      "ggml_metal_init: simdgroup matrix mul. = true\n",
      "ggml_metal_init: has residency sets    = true\n",
      "ggml_metal_init: has bfloat            = true\n",
      "ggml_metal_init: use bfloat            = false\n",
      "ggml_metal_init: hasUnifiedMemory      = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 17179.89 MB\n",
      "ggml_metal_init: loaded kernel_add                                    0x1698faee0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_fuse_2                             0x16747c680 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_fuse_3                             0x16747ad10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_fuse_4                             0x16747b3c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_fuse_5                             0x16747d890 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_fuse_6                             0x16747e540 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_fuse_7                             0x16747e0f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_fuse_8                             0x16747f630 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_row_c4                             0x16747fe80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_row_c4_fuse_2                      0x167480960 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_row_c4_fuse_3                      0x1665f2410 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_row_c4_fuse_4                      0x1665f2f60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_row_c4_fuse_5                      0x4048054f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_row_c4_fuse_6                      0x1665f3970 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_row_c4_fuse_7                      0x1674811a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_row_c4_fuse_8                      0x16747f190 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sub                                    0x404805d50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sub_row_c4                             0x1665f41e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul                                    0x1698fb1a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_row_c4                             0x167f662c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_div                                    0x4048061a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_div_row_c4                             0x404806720 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_id                                 0x1698ffd40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_f32                             0x4048081d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_f16                             0x404808a40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_i32                             0x404809290 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_i16                             0x404809ae0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_scale                                  0x167f67160 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_scale_4                                0x404806da0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_clamp                                  0x1674815d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_tanh                                   0x1665f4750 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_relu                                   0x167481890 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sigmoid                                0x1665f4a10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu                                   0x16747c000 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_4                                 0x167482d90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_erf                               0x40480a240 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_erf_4                             0x167483850 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_quick                             0x167484210 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_quick_4                           0x167484c20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_silu                                   0x167485790 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_silu_4                                 0x167486250 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_elu                                    0x40480b5d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_abs                                    0x40480c090 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sgn                                    0x167486c30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_step                                   0x167486ef0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_hardswish                              0x167487ce0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_hardsigmoid                            0x167f649e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_exp                                    0x167f690b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f16                           0x167f681d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f16_4                         0x167f686e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f32                           0x1665f5060 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f32_4                         0x40480c5c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                          0x40480cda0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x40480de60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_f32                           0x167f69950 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                           0x40480e6e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                          0x167489090 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                          0x1665f5990 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_0                          0x167489940 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_1                          0x40480ef30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q8_0                          0x40480f780 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_mxfp4                         0x40480ff90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                          0x167f6b230 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                          0x404810810 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                          0x1665ee4b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                          0x16748a300 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                          0x167f6baf0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x1665ee8a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x1665eeb60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x404811040 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x404811520 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x16748a9d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x16748af10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x404811fa0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x167f6c640 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x404812800 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_i32                           0x4048130a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_rows_f32                           0x404813910 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_rows_f16                           0x404814130 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)\n",
      "ggml_metal_init: loaded kernel_set_rows_q8_0                          0x404814970 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_rows_q4_0                          0x169708300 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_rows_q4_1                          0x167f6cec0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_rows_q5_0                          0x167f6d700 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_rows_q5_1                          0x4048151b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_rows_iq4_nl                        0x4048159b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rms_norm                               0x4048161f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rms_norm_mul                           0x167f6c000 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rms_norm_mul_add                       0x16748bef0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_l2_norm                                0x404816a50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_group_norm                             0x167f6e790 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_norm                                   0x16748c1f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_ssm_conv_f32                           0x16748cf40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_ssm_scan_f32                           0x167f6de30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_ssm_scan_f32_group                     0x404817810 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rwkv_wkv6_f32                          0x16748b7c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rwkv_wkv7_f32                          0x16748d630 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x16748e860 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f32_f32_c4                      0x16748c6f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x16748f930 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f32_c4                      0x16748fbf0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x167490a20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x167491300 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x169708e30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x404817300 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x1697087f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x167f6f870 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x169709bd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x404817b70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_mxfp4_f32                       0x4048188c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x167f700d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x16748efd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x167491720 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x4048183a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x167493150 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x167f70c10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x1674935d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x167492ce0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x167f71510 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x167494790 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x1697094c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x16970b0e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x169709ff0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x16970bf40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x167493d80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x1674955c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x167f71df0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x167f6eee0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x167f72630 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x167f73570 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x167f73e70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x167f74790 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x167f75000 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x167f75930 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_mxfp4_f32_r1_2              0x167f76230 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_mxfp4_f32_r1_3              0x1674961d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_mxfp4_f32_r1_4              0x10ce15ee0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_mxfp4_f32_r1_5              0x404819d90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x404819040 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x167495d50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x167494ff0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x16970cb50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x16970d480 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x16970dd70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x16970e670 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x167f76f40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x167f77780 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x1674975c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x167497e30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x167f77ff0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x167f788a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x16970b820 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x40481ab60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x40481af80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x40481bf10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x40481c750 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x40481cf60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x40481d7d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x40481dfe0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x167f77290 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x16970f7c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x169710130 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x167f792f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x167f78fc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x40481e850 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x167f7b1d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x167f7baa0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x169710a00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x1697113a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x169711cc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x40481f430 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x16970f190 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x169712900 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x167496c90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x40481b8c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_mxfp4_f32                    0x167498a90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x169713200 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x167496fe0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x1697123d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x169714030 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x40481fd50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x404820670 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x1674993f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x167499d00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x16749b080 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x404821ba0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x16749b5b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x4048226d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x169714980 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x169713990 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x167f7a810 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x167f7cbd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x167f7d430 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x167f7dc60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x167f7e460 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x1697150f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x1697153b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_mxfp4_f32                       0x169716580 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_mxfp4_f32                       0x169715e90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x167f7ecd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x167f7f500 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x167f7fd30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x16749cbc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x169716c60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x169717690 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x16749d420 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x16749c330 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x16749c670 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x16749e930 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x16749f220 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x16749f960 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x167f80590 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x167f80ea0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_map0_f16                     0x1674a0b50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_map1_f32                     0x167f81780 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_f32_f16                      0x16749be70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_f16_f16                      0x169718430 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f16                     0x404822d30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f16                     0x169718d00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f16                     0x167f81fe0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f16                     0x169717a70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f16                     0x169719b30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_mxfp4_f16                    0x167f82bf0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f16                     0x404823270 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f16                     0x167f82770 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f16                     0x167f83a70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f16                     0x404822990 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f16                     0x167f84390 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f16                  0x4048247a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f16                   0x167f833c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f16                  0x404820ea0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f16                    0x1697193d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f16                    0x169719690 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f16                    0x1674a1410 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f16                    0x16971b2a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f16                   0x1674a1cf0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f16                   0x16971bba0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_norm_f32                          0x1674a0630 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_norm_f16                          0x16971a8c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_multi_f32                         0x167f851e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_multi_f16                         0x16971a4e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_vision_f32                        0x16971c4b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_vision_f16                        0x167f85a30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_neox_f32                          0x167f862b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_neox_f16                          0x167f86b40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_f16                             0x4048263d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_f32                             0x4048268b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_ext_f16                         0x404827980 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_ext_f32                         0x16971d130 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13ce30750 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x1674a35c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_upscale_f32                            0x167f876c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pad_f32                                0x404824fc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x1674a25f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x404828dc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_arange_f32                             0x404827f40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x16971dde0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x167f87ab0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_leaky_relu_f32                         0x167f88760 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x16971f150 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x16971eb00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x40482a400 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x167f88f90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x167f89f30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h192                0x16971fa20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_hk192_hv128         0x169720930 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x404829a60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_hk576_hv512         0x167f8aba0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x169721230 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x169721b00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x167f8a570 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x167f8b490 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x1674a4510 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h192               0x167f8b820 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_hk192_hv128        0x1674a4a60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x167f8d5a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_hk576_hv512        0x1697202d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x169721e80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x404829fe0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x1697227d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x169723a10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x169723450 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h192               0x1697248f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_hk192_hv128        0x169724d60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x169725b10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_hk576_hv512        0x167f8cf70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x167f8e450 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x169723ea0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x169727050 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x1697274d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x1697279a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h192               0x1674a50c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_hk192_hv128        0x1674a60e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x1674a5520 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_hk576_hv512        0x1674a6f80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x40482be70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x40482ce20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x1674a6570 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x1674a6950 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x1697285d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h192               0x1697295f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_hk192_hv128        0x1674a7e30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x1674a7800 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_hk576_hv512        0x1674a8890 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x1674a9050 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x1674a81c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x167f8df20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x40482d4e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x40482c400 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h192               0x40482eeb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_hk192_hv128        0x40482fd20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x167f8ecc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_hk576_hv512        0x16972a320 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h64             0x16972acf0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h64            0x4048302e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h64            0x16972b080 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h64            0x167f90030 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h64            0x1674ab590 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h64            0x167f8fa00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h96             0x167f8f320 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h96            0x16972c080 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h96            0x16972cd60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h96            0x16972d6d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h96            0x16972dbb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h96            0x167f915a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x167f924c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x16972f0b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x404830ca0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x16972dfa0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x167f91b50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x16972ff90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h192            0x404831030 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h192           0x1697308a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h192           0x167f92990 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h192           0x1674aaf60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h192           0x1674ab920 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h192           0x1674ac830 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_hk192_hv128      0x4048319d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_hk192_hv128      0x169730d10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_hk192_hv128      0x1697311c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_hk192_hv128      0x167f92ce0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_hk192_hv128      0x1697323e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_hk192_hv128      0x167f93260 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x404831d60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x404832b00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x4048326a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x167f93780 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x404831420 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x404833570 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_hk576_hv512      0x404833ac0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_hk576_hv512      0x167f94040 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_hk576_hv512      0x167f94960 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_hk576_hv512      0x169732cf0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_hk576_hv512      0x169733160 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_hk576_hv512      0x1674acb30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_f32                                0x169731aa0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_i32                                0x40482daf0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                            0x1674ad0d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                            0x167f94550 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f32                            0x1674aded0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                            0x404835250 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x167f96190 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x404834960 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x4048362c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x404835e40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x167f969b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x404837360 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x167f97220 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x167f97a60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x167f982d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x167f98b10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x167f99370 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x4048376d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x167f99bd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x167f9a430 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x167f9ac40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x404837be0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_concat                                 0x1674af2d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sqr                                    0x1674ae220 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sqrt                                   0x13ce26e70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sin                                    0x169733620 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cos                                    0x4048387a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_neg                                    0x169734630 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_reglu                                  0x1697359c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_geglu                                  0x1697361d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_swiglu                                 0x404838c20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_swiglu_oai                             0x1697369f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_geglu_erf                              0x1674af860 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_geglu_quick                            0x169737220 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sum_rows                               0x169737670 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mean                                   0x1674b0dc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_argmax                                 0x167f9b790 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x1674b1690 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x167f9c040 | th_max = 1024 | th_width =   32\n",
      "set_abort_callback: call\n",
      "llama_context:        CPU  output buffer size =     0.12 MiB\n",
      "create_memory: n_ctx = 4096 (padded)\n",
      "llama_kv_cache_unified: layer   0: dev = Metal\n",
      "llama_kv_cache_unified: layer   1: dev = Metal\n",
      "llama_kv_cache_unified: layer   2: dev = Metal\n",
      "llama_kv_cache_unified: layer   3: dev = Metal\n",
      "llama_kv_cache_unified: layer   4: dev = Metal\n",
      "llama_kv_cache_unified: layer   5: dev = Metal\n",
      "llama_kv_cache_unified: layer   6: dev = Metal\n",
      "llama_kv_cache_unified: layer   7: dev = Metal\n",
      "llama_kv_cache_unified: layer   8: dev = Metal\n",
      "llama_kv_cache_unified: layer   9: dev = Metal\n",
      "llama_kv_cache_unified: layer  10: dev = Metal\n",
      "llama_kv_cache_unified: layer  11: dev = Metal\n",
      "llama_kv_cache_unified: layer  12: dev = Metal\n",
      "llama_kv_cache_unified: layer  13: dev = Metal\n",
      "llama_kv_cache_unified: layer  14: dev = Metal\n",
      "llama_kv_cache_unified: layer  15: dev = Metal\n",
      "llama_kv_cache_unified: layer  16: dev = Metal\n",
      "llama_kv_cache_unified: layer  17: dev = Metal\n",
      "llama_kv_cache_unified: layer  18: dev = Metal\n",
      "llama_kv_cache_unified: layer  19: dev = Metal\n",
      "llama_kv_cache_unified: layer  20: dev = Metal\n",
      "llama_kv_cache_unified: layer  21: dev = Metal\n",
      "llama_kv_cache_unified: layer  22: dev = Metal\n",
      "llama_kv_cache_unified: layer  23: dev = Metal\n",
      "llama_kv_cache_unified: layer  24: dev = Metal\n",
      "llama_kv_cache_unified: layer  25: dev = Metal\n",
      "llama_kv_cache_unified: layer  26: dev = Metal\n",
      "llama_kv_cache_unified: layer  27: dev = Metal\n",
      "llama_kv_cache_unified: layer  28: dev = Metal\n",
      "llama_kv_cache_unified: layer  29: dev = Metal\n",
      "llama_kv_cache_unified: layer  30: dev = Metal\n",
      "llama_kv_cache_unified: layer  31: dev = Metal\n",
      "llama_kv_cache_unified:      Metal KV buffer size =  2048.00 MiB\n",
      "llama_kv_cache_unified: size = 2048.00 MiB (  4096 cells,  32 layers,  1/1 seqs), K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\n",
      "llama_context: enumerating backends\n",
      "llama_context: backend_ptrs.size() = 3\n",
      "llama_context: max_nodes = 2328\n",
      "llama_context: worst-case: n_tokens = 512, n_seqs = 1, n_outputs = 0\n",
      "graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  1, n_outputs =  512\n",
      "graph_reserve: reserving a graph for ubatch with n_tokens =    1, n_seqs =  1, n_outputs =    1\n",
      "graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  1, n_outputs =  512\n",
      "llama_context:      Metal compute buffer size =   312.01 MiB\n",
      "llama_context:        CPU compute buffer size =    32.01 MiB\n",
      "llama_context: graph nodes  = 1126\n",
      "llama_context: graph splits = 2\n",
      "Metal : EMBED_LIBRARY = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | REPACK = 1 | \n",
      "Model metadata: {'tokenizer.chat_template': \"{% for message in messages %}{% if message['role'] == 'system' %}{{'<|system|>'+ '\\n' + message['content'] + '\\n'}}{% elif message['role'] == 'user' %}{{'<|user|>' + '\\n' + message['content'] + '\\n'}}{% elif message['role'] == 'assistant' %}{{'<|assistant|>' + '\\n' + message['content'] + '<|endoftext|>' + ('' if loop.last else '\\n')}}{% endif %}{% endfor %}\", 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.add_bos_token': 'false', 'tokenizer.ggml.padding_token_id': '32001', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '32000', 'tokenizer.ggml.pre': 'default', 'llama.embedding_length': '4096', 'llama.vocab_size': '32008', 'llama.attention.head_count_kv': '32', 'general.size_label': '7B', 'general.base_model.0.name': 'Granite 7b Base', 'llama.block_count': '32', 'general.base_model.0.organization': 'Ibm', 'general.base_model.count': '1', 'general.quantization_version': '2', 'llama.rope.dimension_count': '128', 'general.license': 'apache-2.0', 'general.base_model.0.repo_url': 'https://huggingface.co/ibm/granite-7b-base', 'llama.attention.head_count': '32', 'llama.context_length': '4096', 'general.file_type': '15', 'general.finetune': 'lab', 'llama.feed_forward_length': '11008', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'general.architecture': 'llama', 'general.basename': 'instructlab-granite', 'llama.rope.freq_base': '10000.000000', 'tokenizer.ggml.bos_token_id': '1', 'general.type': 'model', 'tokenizer.ggml.model': 'llama', 'general.name': 'Instructlab Granite 7b Lab'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {% for message in messages %}{% if message['role'] == 'system' %}{{'<|system|>'+ '\n",
      "' + message['content'] + '\n",
      "'}}{% elif message['role'] == 'user' %}{{'<|user|>' + '\n",
      "' + message['content'] + '\n",
      "'}}{% elif message['role'] == 'assistant' %}{{'<|assistant|>' + '\n",
      "' + message['content'] + '<|endoftext|>' + ('' if loop.last else '\n",
      "')}}{% endif %}{% endfor %}\n",
      "Using chat eos_token: <|endoftext|>\n",
      "Using chat bos_token: <s>\n",
      "<All keys matched successfully>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports OK\n"
     ]
    }
   ],
   "source": [
    "# # Install core dependencies\n",
    "%pip install -q --upgrade pip wheel setuptools\n",
    "\n",
    "\n",
    "# Nomic client (for direct API usage if needed)\n",
    "%pip install -q nomic\n",
    "\n",
    "# Ensure Docling base library is present\n",
    "%pip install -q \"docling>=2.0.0\"\n",
    "\n",
    "\n",
    "%pip install llama-index-readers-docling\n",
    "\n",
    "%pip install llama-index-node-parser-docling\n",
    "\n",
    "%pip install llama-index-vector-stores-milvus\n",
    "\n",
    "%pip install llama-index-embeddings-huggingface\n",
    "\n",
    "%pip install llama-index-llms-huggingface\n",
    "\n",
    "%pip install llama-index-postprocessor-flag-embedding-reranker\n",
    "\n",
    "%pip install llama-index-postprocessor-flag-embedding-reranker\n",
    "\n",
    "%pip install llama-index-llms-huggingface-api\n",
    "\n",
    "%pip install -U bitsandbytes==0.43.1\n",
    "\n",
    "%pip install ctransformers\n",
    "\n",
    "%pip install llama-cpp-python\n",
    "\n",
    "%pip install torch torchvision torchaudio\n",
    "\n",
    "%pip install accelerate\n",
    "\n",
    "%pip install transformers\n",
    "\n",
    "%pip install pymilvus>=2.4.2\n",
    "\n",
    "%pip install einops\n",
    "\n",
    "%pip install huggingface_hub\n",
    "\n",
    "%pip install 'git+https://github.com/FlagOpen/FlagEmbedding.git'\n",
    "\n",
    "\n",
    "# Imports & environment\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple\n",
    "from importlib.metadata import version, PackageNotFoundError\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ.setdefault(\"TOKENIZERS_PARALLELISM\", \"false\")\n",
    "\n",
    "# LlamaIndex core\n",
    "from llama_index.core import VectorStoreIndex, StorageContext, Settings\n",
    "\n",
    "# Docling reader & node parser (handle modular import variations)\n",
    "try:\n",
    "\tfrom llama_index.readers.docling import DoclingReader\n",
    "except ModuleNotFoundError:\n",
    "\tfrom llama_index.readers.docling.base import DoclingReader\n",
    "\n",
    "try:\n",
    "\tfrom llama_index.node_parser.docling import DoclingNodeParser\n",
    "except ModuleNotFoundError:\n",
    "\tfrom llama_index.node_parser.docling.base import DoclingNodeParser\n",
    "\n",
    "# Vector DBs\n",
    "from llama_index.vector_stores.milvus import MilvusVectorStore\n",
    "from pymilvus import connections\n",
    "\n",
    "\n",
    "# Embeddings (Nomic text embedding via HF model id)\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "# Reranker (BAAI/bge-reranker-large)\n",
    "from llama_index.postprocessor.flag_embedding_reranker import (\n",
    "    FlagEmbeddingReranker,\n",
    ")\n",
    "\n",
    "# LLM: Granite via IBM watsonx\n",
    "# from llama_index.llms.watsonx import WatsonxLLM\n",
    "# from ibm_watsonx_ai.foundation_models.utils.enums import ModelTypes\n",
    "\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "\n",
    "# Docling\n",
    "from docling.document_converter import DocumentConverter, PdfFormatOption\n",
    "from docling.datamodel.pipeline_options import PdfPipelineOptions\n",
    "from docling.datamodel.base_models import InputFormat\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "from llama_index.core import Document\n",
    "from llama_index.core.node_parser import SentenceSplitter, TokenTextSplitter\n",
    "\n",
    "# Create an index over the documents\n",
    "from llama_index.core import VectorStoreIndex, StorageContext\n",
    "from llama_index.vector_stores.milvus import MilvusVectorStore\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "from typing import List, Literal, Optional, Dict\n",
    "import re\n",
    "from llama_index.core import Document\n",
    "from llama_index.core.node_parser import SentenceSplitter, TokenTextSplitter\n",
    "\n",
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.schema import TextNode  # optional, if you want nodes\n",
    "\n",
    "\n",
    "\n",
    "#!hf auth login --token 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\n",
    "!huggingface-cli login --token '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'\n",
    "\n",
    "\n",
    "#!huggingface-cli download tensorblock/granite-7b-lab-GGUF --include \"granite-7b-lab-Q4_K_M.gguf\" --local-dir ./models\n",
    "\n",
    "from llama_cpp import Llama\n",
    "\n",
    "llm = Llama(\n",
    "    model_path=\"./models/granite-7b-lab-Q4_K_M.gguf\",\n",
    "    n_ctx=4096,\n",
    "    n_threads=8,     # Adjust to your CPU cores\n",
    "    n_gpu_layers=50  # If GPU available\n",
    ")\n",
    "\n",
    "# 1. Set up Nomic embedding model\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"nomic-ai/nomic-embed-text-v1.5\", trust_remote_code=True)\n",
    "\n",
    "\n",
    "print(\"Imports OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b78c869",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EventId</th>\n",
       "      <th>EventTemplate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>E1</td>\n",
       "      <td>############################## _getSysMsgList</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>E2</td>\n",
       "      <td>(DiskStore.Normal:&lt;*&gt;) &lt;*&gt; &lt;*&gt;.&lt;*&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>E3</td>\n",
       "      <td>(ImportBailout.Error:&lt;*&gt;) Asked to exit for Di...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>E4</td>\n",
       "      <td>**** [BroadcomBluetoothHostController][SetupCo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>E5</td>\n",
       "      <td>**** [BroadcomBluetoothHostControllerUSBTransp...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  EventId                                      EventTemplate\n",
       "0      E1      ############################## _getSysMsgList\n",
       "1      E2                 (DiskStore.Normal:<*>) <*> <*>.<*>\n",
       "2      E3  (ImportBailout.Error:<*>) Asked to exit for Di...\n",
       "3      E4  **** [BroadcomBluetoothHostController][SetupCo...\n",
       "4      E5  **** [BroadcomBluetoothHostControllerUSBTransp..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "template_df= pd.read_csv('/Users/varunraste/Downloads/RAG_demo/Mac_data_logs/Mac_2k.log_templates.csv')\n",
    "template_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f837505a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LineId</th>\n",
       "      <th>Month</th>\n",
       "      <th>Date</th>\n",
       "      <th>Time</th>\n",
       "      <th>User</th>\n",
       "      <th>Component</th>\n",
       "      <th>PID</th>\n",
       "      <th>Address</th>\n",
       "      <th>Content</th>\n",
       "      <th>EventId</th>\n",
       "      <th>EventTemplate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Jul</td>\n",
       "      <td>1</td>\n",
       "      <td>09:00:55</td>\n",
       "      <td>calvisitor-10-105-160-95</td>\n",
       "      <td>kernel</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IOThunderboltSwitch&lt;0&gt;(0x0)::listenerCallback ...</td>\n",
       "      <td>E252</td>\n",
       "      <td>IOThunderboltSwitch&lt;&lt;*&gt;&gt;(&lt;*&gt;)::listenerCallbac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Jul</td>\n",
       "      <td>1</td>\n",
       "      <td>09:01:05</td>\n",
       "      <td>calvisitor-10-105-160-95</td>\n",
       "      <td>com.apple.CDScheduler</td>\n",
       "      <td>43</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Thermal pressure state: 1 Memory pressure stat...</td>\n",
       "      <td>E323</td>\n",
       "      <td>Thermal pressure state: &lt;*&gt; Memory pressure st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Jul</td>\n",
       "      <td>1</td>\n",
       "      <td>09:01:06</td>\n",
       "      <td>calvisitor-10-105-160-95</td>\n",
       "      <td>QQ</td>\n",
       "      <td>10018</td>\n",
       "      <td>NaN</td>\n",
       "      <td>FA||Url||taskID[2019352994] dealloc</td>\n",
       "      <td>E216</td>\n",
       "      <td>FA||Url||taskID[&lt;*&gt;] dealloc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Jul</td>\n",
       "      <td>1</td>\n",
       "      <td>09:02:26</td>\n",
       "      <td>calvisitor-10-105-160-95</td>\n",
       "      <td>kernel</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ARPT: 620701.011328: AirPort_Brcm43xx::syncPow...</td>\n",
       "      <td>E128</td>\n",
       "      <td>ARPT: &lt;*&gt;.&lt;*&gt;: AirPort_&lt;*&gt;::syncPowerState: WW...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Jul</td>\n",
       "      <td>1</td>\n",
       "      <td>09:02:26</td>\n",
       "      <td>authorMacBook-Pro</td>\n",
       "      <td>kernel</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ARPT: 620702.879952: AirPort_Brcm43xx::platfor...</td>\n",
       "      <td>E124</td>\n",
       "      <td>ARPT: &lt;*&gt;.&lt;*&gt;: AirPort_&lt;*&gt;::platformWoWEnable:...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   LineId Month  Date      Time                      User  \\\n",
       "0       1   Jul     1  09:00:55  calvisitor-10-105-160-95   \n",
       "1       2   Jul     1  09:01:05  calvisitor-10-105-160-95   \n",
       "2       3   Jul     1  09:01:06  calvisitor-10-105-160-95   \n",
       "3       4   Jul     1  09:02:26  calvisitor-10-105-160-95   \n",
       "4       5   Jul     1  09:02:26         authorMacBook-Pro   \n",
       "\n",
       "               Component    PID Address  \\\n",
       "0                 kernel      0     NaN   \n",
       "1  com.apple.CDScheduler     43     NaN   \n",
       "2                     QQ  10018     NaN   \n",
       "3                 kernel      0     NaN   \n",
       "4                 kernel      0     NaN   \n",
       "\n",
       "                                             Content EventId  \\\n",
       "0  IOThunderboltSwitch<0>(0x0)::listenerCallback ...    E252   \n",
       "1  Thermal pressure state: 1 Memory pressure stat...    E323   \n",
       "2                FA||Url||taskID[2019352994] dealloc    E216   \n",
       "3  ARPT: 620701.011328: AirPort_Brcm43xx::syncPow...    E128   \n",
       "4  ARPT: 620702.879952: AirPort_Brcm43xx::platfor...    E124   \n",
       "\n",
       "                                       EventTemplate  \n",
       "0  IOThunderboltSwitch<<*>>(<*>)::listenerCallbac...  \n",
       "1  Thermal pressure state: <*> Memory pressure st...  \n",
       "2                       FA||Url||taskID[<*>] dealloc  \n",
       "3  ARPT: <*>.<*>: AirPort_<*>::syncPowerState: WW...  \n",
       "4  ARPT: <*>.<*>: AirPort_<*>::platformWoWEnable:...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df= pd.read_csv('/Users/varunraste/Downloads/RAG_demo/Mac_data_logs/Mac_2k.log_structured.csv')\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8bc0c55d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructing 'timestamp' column...\n",
      "Null timestamps count: 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Month</th>\n",
       "      <th>Date</th>\n",
       "      <th>Time</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Jul</td>\n",
       "      <td>1</td>\n",
       "      <td>09:00:55</td>\n",
       "      <td>2025-07-01 09:00:55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jul</td>\n",
       "      <td>1</td>\n",
       "      <td>09:01:05</td>\n",
       "      <td>2025-07-01 09:01:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Jul</td>\n",
       "      <td>1</td>\n",
       "      <td>09:01:06</td>\n",
       "      <td>2025-07-01 09:01:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Jul</td>\n",
       "      <td>1</td>\n",
       "      <td>09:02:26</td>\n",
       "      <td>2025-07-01 09:02:26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Jul</td>\n",
       "      <td>1</td>\n",
       "      <td>09:02:26</td>\n",
       "      <td>2025-07-01 09:02:26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Jul</td>\n",
       "      <td>1</td>\n",
       "      <td>09:03:11</td>\n",
       "      <td>2025-07-01 09:03:11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Month  Date      Time           timestamp\n",
       "0   Jul     1  09:00:55 2025-07-01 09:00:55\n",
       "1   Jul     1  09:01:05 2025-07-01 09:01:05\n",
       "2   Jul     1  09:01:06 2025-07-01 09:01:06\n",
       "3   Jul     1  09:02:26 2025-07-01 09:02:26\n",
       "4   Jul     1  09:02:26 2025-07-01 09:02:26\n",
       "5   Jul     1  09:03:11 2025-07-01 09:03:11"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cell 3: Build timestamp (use Year column if present else YEAR_GUESS)\n",
    "YEAR_GUESS = 2025   # adjust if your logs span different year\n",
    "month_col = 'Month'\n",
    "date_col = 'Date'\n",
    "time_col = 'Time'\n",
    "year_col = 'Year' if 'Year' in df.columns else None\n",
    "\n",
    "def build_ts(row):\n",
    "    try:\n",
    "        year_val = int(row[year_col]) if (year_col and pd.notna(row[year_col])) else YEAR_GUESS\n",
    "        month_val = row[month_col] if month_col in df.columns else \"\"\n",
    "        date_val = int(row[date_col]) if (date_col in df.columns and pd.notna(row[date_col])) else None\n",
    "        time_val = row[time_col] if (time_col in df.columns and pd.notna(row[time_col])) else \"00:00:00\"\n",
    "        ts_str = f\"{month_val} {date_val} {year_val} {time_val}\"\n",
    "        return pd.to_datetime(ts_str, errors='coerce')\n",
    "    except Exception:\n",
    "        return pd.NaT\n",
    "\n",
    "print(\"Constructing 'timestamp' column...\")\n",
    "df['timestamp'] = df.apply(build_ts, axis=1)\n",
    "print(\"Null timestamps count:\", df['timestamp'].isna().sum())\n",
    "display(df[['Month','Date','Time','timestamp']].head(6))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a0f0c8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "469f69fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After removing consecutive duplicates:\n",
      "Remaining rows: 1977\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LineId</th>\n",
       "      <th>Month</th>\n",
       "      <th>Date</th>\n",
       "      <th>Time</th>\n",
       "      <th>User</th>\n",
       "      <th>Component</th>\n",
       "      <th>PID</th>\n",
       "      <th>Address</th>\n",
       "      <th>Content</th>\n",
       "      <th>EventId</th>\n",
       "      <th>EventTemplate</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1283</td>\n",
       "      <td>Jul</td>\n",
       "      <td>5</td>\n",
       "      <td>16:24:44</td>\n",
       "      <td>airbears2-10-142-108-38</td>\n",
       "      <td>QQ</td>\n",
       "      <td>10018</td>\n",
       "      <td>NaN</td>\n",
       "      <td>button report: 0x8002bdf</td>\n",
       "      <td>E161</td>\n",
       "      <td>button report: &lt;*&gt;</td>\n",
       "      <td>2025-07-05 16:24:44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1284</td>\n",
       "      <td>Jul</td>\n",
       "      <td>5</td>\n",
       "      <td>16:26:28</td>\n",
       "      <td>airbears2-10-142-108-38</td>\n",
       "      <td>corecaptured</td>\n",
       "      <td>36150</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CCLogTap::profileRemoved, Owner: com.apple.iok...</td>\n",
       "      <td>E177</td>\n",
       "      <td>CCLogTap::profileRemoved, Owner: com.apple.iok...</td>\n",
       "      <td>2025-07-05 16:26:28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1285</td>\n",
       "      <td>Jul</td>\n",
       "      <td>5</td>\n",
       "      <td>16:33:10</td>\n",
       "      <td>airbears2-10-142-108-38</td>\n",
       "      <td>locationd</td>\n",
       "      <td>82</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Location icon should now be in state 'Inactive'</td>\n",
       "      <td>E259</td>\n",
       "      <td>Location icon should now be in state 'Inactive'</td>\n",
       "      <td>2025-07-05 16:33:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1286</td>\n",
       "      <td>Jul</td>\n",
       "      <td>5</td>\n",
       "      <td>16:37:58</td>\n",
       "      <td>airbears2-10-142-108-38</td>\n",
       "      <td>syslogd</td>\n",
       "      <td>44</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ASL Sender Statistics</td>\n",
       "      <td>E153</td>\n",
       "      <td>ASL Sender Statistics</td>\n",
       "      <td>2025-07-05 16:37:58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1287</td>\n",
       "      <td>Jul</td>\n",
       "      <td>5</td>\n",
       "      <td>16:47:16</td>\n",
       "      <td>airbears2-10-142-108-38</td>\n",
       "      <td>Safari</td>\n",
       "      <td>9852</td>\n",
       "      <td>NaN</td>\n",
       "      <td>KeychainGetICDPStatus: keychain: -25300</td>\n",
       "      <td>E256</td>\n",
       "      <td>KeychainGetICDPStatus: keychain: &lt;*&gt;</td>\n",
       "      <td>2025-07-05 16:47:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1288</td>\n",
       "      <td>Jul</td>\n",
       "      <td>5</td>\n",
       "      <td>16:47:16</td>\n",
       "      <td>airbears2-10-142-108-38</td>\n",
       "      <td>Safari</td>\n",
       "      <td>9852</td>\n",
       "      <td>NaN</td>\n",
       "      <td>KeychainGetICDPStatus: status: off</td>\n",
       "      <td>E257</td>\n",
       "      <td>KeychainGetICDPStatus: status: off</td>\n",
       "      <td>2025-07-05 16:47:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1289</td>\n",
       "      <td>Jul</td>\n",
       "      <td>5</td>\n",
       "      <td>16:47:58</td>\n",
       "      <td>airbears2-10-142-108-38</td>\n",
       "      <td>locationd</td>\n",
       "      <td>82</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Location icon should now be in state 'Active'</td>\n",
       "      <td>E258</td>\n",
       "      <td>Location icon should now be in state 'Active'</td>\n",
       "      <td>2025-07-05 16:47:58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1290</td>\n",
       "      <td>Jul</td>\n",
       "      <td>5</td>\n",
       "      <td>16:58:24</td>\n",
       "      <td>airbears2-10-142-108-38</td>\n",
       "      <td>imagent</td>\n",
       "      <td>355</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;IMMacNotificationCenterManager: 0x7fdcc9d1638...</td>\n",
       "      <td>E106</td>\n",
       "      <td>&lt;IMMacNotificationCenterManager: &lt;*&gt;&gt;: Updatin...</td>\n",
       "      <td>2025-07-05 16:58:24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1291</td>\n",
       "      <td>Jul</td>\n",
       "      <td>5</td>\n",
       "      <td>16:58:25</td>\n",
       "      <td>airbears2-10-142-108-38</td>\n",
       "      <td>WindowServer</td>\n",
       "      <td>184</td>\n",
       "      <td>NaN</td>\n",
       "      <td>device_generate_lock_screen_screenshot: authw ...</td>\n",
       "      <td>E200</td>\n",
       "      <td>device_generate_lock_screen_screenshot: authw ...</td>\n",
       "      <td>2025-07-05 16:58:25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1292</td>\n",
       "      <td>Jul</td>\n",
       "      <td>5</td>\n",
       "      <td>16:58:39</td>\n",
       "      <td>airbears2-10-142-108-38</td>\n",
       "      <td>kernel</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ARPT: 728046.456828: wl0: setup_keepalive: Loc...</td>\n",
       "      <td>E145</td>\n",
       "      <td>ARPT: &lt;*&gt;.&lt;*&gt;: wl0: setup_keepalive: Local IP:...</td>\n",
       "      <td>2025-07-05 16:58:39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1293</td>\n",
       "      <td>Jul</td>\n",
       "      <td>5</td>\n",
       "      <td>17:01:33</td>\n",
       "      <td>airbears2-10-142-108-38</td>\n",
       "      <td>com.apple.AddressBook.InternetAccountsBridge</td>\n",
       "      <td>36221</td>\n",
       "      <td>NaN</td>\n",
       "      <td>dnssd_clientstub ConnectToServer: connect()-&gt; ...</td>\n",
       "      <td>E203</td>\n",
       "      <td>dnssd_clientstub ConnectToServer: connect()-&gt; ...</td>\n",
       "      <td>2025-07-05 17:01:33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1294</td>\n",
       "      <td>Jul</td>\n",
       "      <td>5</td>\n",
       "      <td>17:02:52</td>\n",
       "      <td>airbears2-10-142-108-38</td>\n",
       "      <td>kernel</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AppleCamIn::systemWakeCall - messageType = 0xE...</td>\n",
       "      <td>E120</td>\n",
       "      <td>AppleCamIn::systemWakeCall - messageType = &lt;*&gt;</td>\n",
       "      <td>2025-07-05 17:02:52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1295</td>\n",
       "      <td>Jul</td>\n",
       "      <td>5</td>\n",
       "      <td>17:03:07</td>\n",
       "      <td>airbears2-10-142-108-38</td>\n",
       "      <td>corecaptured</td>\n",
       "      <td>36224</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CCFile::captureLog</td>\n",
       "      <td>E167</td>\n",
       "      <td>CCFile::captureLog</td>\n",
       "      <td>2025-07-05 17:03:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1296</td>\n",
       "      <td>Jul</td>\n",
       "      <td>5</td>\n",
       "      <td>17:03:12</td>\n",
       "      <td>airbears2-10-142-108-38</td>\n",
       "      <td>kernel</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ARPT: 728173.580097: wl0: setup_keepalive: int...</td>\n",
       "      <td>E144</td>\n",
       "      <td>ARPT: &lt;*&gt;.&lt;*&gt;: wl0: setup_keepalive: interval ...</td>\n",
       "      <td>2025-07-05 17:03:12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1297</td>\n",
       "      <td>Jul</td>\n",
       "      <td>5</td>\n",
       "      <td>17:03:12</td>\n",
       "      <td>airbears2-10-142-108-38</td>\n",
       "      <td>kernel</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ARPT: 728173.580149: wl0: MDNS: IPV4 Addr: 10....</td>\n",
       "      <td>E141</td>\n",
       "      <td>ARPT: &lt;*&gt;.&lt;*&gt;: wl0: MDNS: IPV4 Addr: &lt;*&gt;</td>\n",
       "      <td>2025-07-05 17:03:12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>701</td>\n",
       "      <td>Jul</td>\n",
       "      <td>3</td>\n",
       "      <td>21:30:55</td>\n",
       "      <td>airbears2-10-142-110-255</td>\n",
       "      <td>kernel</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Sandbox: com.apple.Addres(33959) deny(1) netwo...</td>\n",
       "      <td>E300</td>\n",
       "      <td>Sandbox: &lt;*&gt;(&lt;*&gt;) deny(&lt;*&gt;) network-outbound /...</td>\n",
       "      <td>2025-07-03 21:30:55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>702</td>\n",
       "      <td>Jul</td>\n",
       "      <td>3</td>\n",
       "      <td>21:35:36</td>\n",
       "      <td>airbears2-10-142-110-255</td>\n",
       "      <td>locationd</td>\n",
       "      <td>82</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Location icon should now be in state 'Inactive'</td>\n",
       "      <td>E259</td>\n",
       "      <td>Location icon should now be in state 'Inactive'</td>\n",
       "      <td>2025-07-03 21:35:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>703</td>\n",
       "      <td>Jul</td>\n",
       "      <td>3</td>\n",
       "      <td>21:41:47</td>\n",
       "      <td>airbears2-10-142-110-255</td>\n",
       "      <td>com.apple.WebKit.WebContent</td>\n",
       "      <td>32778</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[21:41:47.568] &lt;&lt;&lt;&lt; Boss &gt;&gt;&gt;&gt; figPlaybackBossP...</td>\n",
       "      <td>E12</td>\n",
       "      <td>[&lt;*&gt;:&lt;*&gt;:&lt;*&gt;.&lt;*&gt;] &lt;&lt;&lt;&lt; Boss &gt;&gt;&gt;&gt; figPlaybackBo...</td>\n",
       "      <td>2025-07-03 21:41:47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>704</td>\n",
       "      <td>Jul</td>\n",
       "      <td>3</td>\n",
       "      <td>22:16:48</td>\n",
       "      <td>airbears2-10-142-110-255</td>\n",
       "      <td>WindowServer</td>\n",
       "      <td>184</td>\n",
       "      <td>NaN</td>\n",
       "      <td>send_datagram_available_ping: pid 445 failed t...</td>\n",
       "      <td>E305</td>\n",
       "      <td>send_datagram_available_ping: pid &lt;*&gt; failed t...</td>\n",
       "      <td>2025-07-03 22:16:48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>705</td>\n",
       "      <td>Jul</td>\n",
       "      <td>3</td>\n",
       "      <td>22:32:26</td>\n",
       "      <td>airbears2-10-142-110-255</td>\n",
       "      <td>WeChat</td>\n",
       "      <td>24144</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jemmytest</td>\n",
       "      <td>E253</td>\n",
       "      <td>jemmytest</td>\n",
       "      <td>2025-07-03 22:32:26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    LineId Month  Date      Time                      User  \\\n",
       "0     1283   Jul     5  16:24:44   airbears2-10-142-108-38   \n",
       "1     1284   Jul     5  16:26:28   airbears2-10-142-108-38   \n",
       "2     1285   Jul     5  16:33:10   airbears2-10-142-108-38   \n",
       "3     1286   Jul     5  16:37:58   airbears2-10-142-108-38   \n",
       "4     1287   Jul     5  16:47:16   airbears2-10-142-108-38   \n",
       "5     1288   Jul     5  16:47:16   airbears2-10-142-108-38   \n",
       "6     1289   Jul     5  16:47:58   airbears2-10-142-108-38   \n",
       "7     1290   Jul     5  16:58:24   airbears2-10-142-108-38   \n",
       "8     1291   Jul     5  16:58:25   airbears2-10-142-108-38   \n",
       "9     1292   Jul     5  16:58:39   airbears2-10-142-108-38   \n",
       "10    1293   Jul     5  17:01:33   airbears2-10-142-108-38   \n",
       "11    1294   Jul     5  17:02:52   airbears2-10-142-108-38   \n",
       "12    1295   Jul     5  17:03:07   airbears2-10-142-108-38   \n",
       "13    1296   Jul     5  17:03:12   airbears2-10-142-108-38   \n",
       "14    1297   Jul     5  17:03:12   airbears2-10-142-108-38   \n",
       "15     701   Jul     3  21:30:55  airbears2-10-142-110-255   \n",
       "16     702   Jul     3  21:35:36  airbears2-10-142-110-255   \n",
       "17     703   Jul     3  21:41:47  airbears2-10-142-110-255   \n",
       "18     704   Jul     3  22:16:48  airbears2-10-142-110-255   \n",
       "19     705   Jul     3  22:32:26  airbears2-10-142-110-255   \n",
       "\n",
       "                                       Component    PID Address  \\\n",
       "0                                             QQ  10018     NaN   \n",
       "1                                   corecaptured  36150     NaN   \n",
       "2                                      locationd     82     NaN   \n",
       "3                                        syslogd     44     NaN   \n",
       "4                                         Safari   9852     NaN   \n",
       "5                                         Safari   9852     NaN   \n",
       "6                                      locationd     82     NaN   \n",
       "7                                        imagent    355     NaN   \n",
       "8                                   WindowServer    184     NaN   \n",
       "9                                         kernel      0     NaN   \n",
       "10  com.apple.AddressBook.InternetAccountsBridge  36221     NaN   \n",
       "11                                        kernel      0     NaN   \n",
       "12                                  corecaptured  36224     NaN   \n",
       "13                                        kernel      0     NaN   \n",
       "14                                        kernel      0     NaN   \n",
       "15                                        kernel      0     NaN   \n",
       "16                                     locationd     82     NaN   \n",
       "17                   com.apple.WebKit.WebContent  32778     NaN   \n",
       "18                                  WindowServer    184     NaN   \n",
       "19                                        WeChat  24144     NaN   \n",
       "\n",
       "                                              Content EventId  \\\n",
       "0                            button report: 0x8002bdf    E161   \n",
       "1   CCLogTap::profileRemoved, Owner: com.apple.iok...    E177   \n",
       "2     Location icon should now be in state 'Inactive'    E259   \n",
       "3                               ASL Sender Statistics    E153   \n",
       "4             KeychainGetICDPStatus: keychain: -25300    E256   \n",
       "5                  KeychainGetICDPStatus: status: off    E257   \n",
       "6       Location icon should now be in state 'Active'    E258   \n",
       "7   <IMMacNotificationCenterManager: 0x7fdcc9d1638...    E106   \n",
       "8   device_generate_lock_screen_screenshot: authw ...    E200   \n",
       "9   ARPT: 728046.456828: wl0: setup_keepalive: Loc...    E145   \n",
       "10  dnssd_clientstub ConnectToServer: connect()-> ...    E203   \n",
       "11  AppleCamIn::systemWakeCall - messageType = 0xE...    E120   \n",
       "12                                 CCFile::captureLog    E167   \n",
       "13  ARPT: 728173.580097: wl0: setup_keepalive: int...    E144   \n",
       "14  ARPT: 728173.580149: wl0: MDNS: IPV4 Addr: 10....    E141   \n",
       "15  Sandbox: com.apple.Addres(33959) deny(1) netwo...    E300   \n",
       "16    Location icon should now be in state 'Inactive'    E259   \n",
       "17  [21:41:47.568] <<<< Boss >>>> figPlaybackBossP...     E12   \n",
       "18  send_datagram_available_ping: pid 445 failed t...    E305   \n",
       "19                                          jemmytest    E253   \n",
       "\n",
       "                                        EventTemplate           timestamp  \n",
       "0                                  button report: <*> 2025-07-05 16:24:44  \n",
       "1   CCLogTap::profileRemoved, Owner: com.apple.iok... 2025-07-05 16:26:28  \n",
       "2     Location icon should now be in state 'Inactive' 2025-07-05 16:33:10  \n",
       "3                               ASL Sender Statistics 2025-07-05 16:37:58  \n",
       "4                KeychainGetICDPStatus: keychain: <*> 2025-07-05 16:47:16  \n",
       "5                  KeychainGetICDPStatus: status: off 2025-07-05 16:47:16  \n",
       "6       Location icon should now be in state 'Active' 2025-07-05 16:47:58  \n",
       "7   <IMMacNotificationCenterManager: <*>>: Updatin... 2025-07-05 16:58:24  \n",
       "8   device_generate_lock_screen_screenshot: authw ... 2025-07-05 16:58:25  \n",
       "9   ARPT: <*>.<*>: wl0: setup_keepalive: Local IP:... 2025-07-05 16:58:39  \n",
       "10  dnssd_clientstub ConnectToServer: connect()-> ... 2025-07-05 17:01:33  \n",
       "11     AppleCamIn::systemWakeCall - messageType = <*> 2025-07-05 17:02:52  \n",
       "12                                 CCFile::captureLog 2025-07-05 17:03:07  \n",
       "13  ARPT: <*>.<*>: wl0: setup_keepalive: interval ... 2025-07-05 17:03:12  \n",
       "14           ARPT: <*>.<*>: wl0: MDNS: IPV4 Addr: <*> 2025-07-05 17:03:12  \n",
       "15  Sandbox: <*>(<*>) deny(<*>) network-outbound /... 2025-07-03 21:30:55  \n",
       "16    Location icon should now be in state 'Inactive' 2025-07-03 21:35:36  \n",
       "17  [<*>:<*>:<*>.<*>] <<<< Boss >>>> figPlaybackBo... 2025-07-03 21:41:47  \n",
       "18  send_datagram_available_ping: pid <*> failed t... 2025-07-03 22:16:48  \n",
       "19                                          jemmytest 2025-07-03 22:32:26  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Remove consecutive duplicates of 'Content' for the same 'User' from the original df\n",
    "df = df.sort_values(['User','timestamp']).reset_index(drop=True)\n",
    "\n",
    "# Mark rows where Content is same as previous row for the same User\n",
    "df['is_dup_consec'] = df.groupby('User')['Content'].shift() == df['Content']\n",
    "\n",
    "# Keep only non-duplicate rows\n",
    "df = df[~df['is_dup_consec']].drop(columns=['is_dup_consec']).reset_index(drop=True)\n",
    "\n",
    "print(\"After removing consecutive duplicates:\")\n",
    "print(\"Remaining rows:\", len(df))\n",
    "display(df.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68348cf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorting by User and timestamp...\n",
      "Sample rows with diff_seconds (first 10):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>prev_ts</th>\n",
       "      <th>diff_seconds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>airbears2-10-142-108-38</td>\n",
       "      <td>2025-07-05 16:24:44</td>\n",
       "      <td>NaT</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>airbears2-10-142-108-38</td>\n",
       "      <td>2025-07-05 16:26:28</td>\n",
       "      <td>2025-07-05 16:24:44</td>\n",
       "      <td>104.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>airbears2-10-142-108-38</td>\n",
       "      <td>2025-07-05 16:33:10</td>\n",
       "      <td>2025-07-05 16:26:28</td>\n",
       "      <td>402.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>airbears2-10-142-108-38</td>\n",
       "      <td>2025-07-05 16:37:58</td>\n",
       "      <td>2025-07-05 16:33:10</td>\n",
       "      <td>288.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>airbears2-10-142-108-38</td>\n",
       "      <td>2025-07-05 16:47:16</td>\n",
       "      <td>2025-07-05 16:37:58</td>\n",
       "      <td>558.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>airbears2-10-142-108-38</td>\n",
       "      <td>2025-07-05 16:47:16</td>\n",
       "      <td>2025-07-05 16:47:16</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>airbears2-10-142-108-38</td>\n",
       "      <td>2025-07-05 16:47:58</td>\n",
       "      <td>2025-07-05 16:47:16</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>airbears2-10-142-108-38</td>\n",
       "      <td>2025-07-05 16:58:24</td>\n",
       "      <td>2025-07-05 16:47:58</td>\n",
       "      <td>626.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>airbears2-10-142-108-38</td>\n",
       "      <td>2025-07-05 16:58:25</td>\n",
       "      <td>2025-07-05 16:58:24</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>airbears2-10-142-108-38</td>\n",
       "      <td>2025-07-05 16:58:39</td>\n",
       "      <td>2025-07-05 16:58:25</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      User           timestamp             prev_ts  \\\n",
       "0  airbears2-10-142-108-38 2025-07-05 16:24:44                 NaT   \n",
       "1  airbears2-10-142-108-38 2025-07-05 16:26:28 2025-07-05 16:24:44   \n",
       "2  airbears2-10-142-108-38 2025-07-05 16:33:10 2025-07-05 16:26:28   \n",
       "3  airbears2-10-142-108-38 2025-07-05 16:37:58 2025-07-05 16:33:10   \n",
       "4  airbears2-10-142-108-38 2025-07-05 16:47:16 2025-07-05 16:37:58   \n",
       "5  airbears2-10-142-108-38 2025-07-05 16:47:16 2025-07-05 16:47:16   \n",
       "6  airbears2-10-142-108-38 2025-07-05 16:47:58 2025-07-05 16:47:16   \n",
       "7  airbears2-10-142-108-38 2025-07-05 16:58:24 2025-07-05 16:47:58   \n",
       "8  airbears2-10-142-108-38 2025-07-05 16:58:25 2025-07-05 16:58:24   \n",
       "9  airbears2-10-142-108-38 2025-07-05 16:58:39 2025-07-05 16:58:25   \n",
       "\n",
       "   diff_seconds  \n",
       "0           0.0  \n",
       "1         104.0  \n",
       "2         402.0  \n",
       "3         288.0  \n",
       "4         558.0  \n",
       "5           0.0  \n",
       "6          42.0  \n",
       "7         626.0  \n",
       "8           1.0  \n",
       "9          14.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cell 4: Sort by User + timestamp and compute inter-event gaps\n",
    "USER_COL = 'User'  # change if your column name differs\n",
    "print(\"Sorting by\", USER_COL, \"and timestamp...\")\n",
    "df_sorted = df.sort_values([USER_COL, 'timestamp']).reset_index(drop=True)\n",
    "\n",
    "# previous timestamp per user and gap in seconds\n",
    "df_sorted['prev_ts'] = df_sorted.groupby(USER_COL)['timestamp'].shift(1)\n",
    "df_sorted['diff_seconds'] = (df_sorted['timestamp'] - df_sorted['prev_ts']).dt.total_seconds().fillna(0)\n",
    "\n",
    "print(\"Sample rows with diff_seconds (first 10):\")\n",
    "display(df_sorted[[USER_COL,'timestamp','prev_ts','diff_seconds']].head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "858d8b49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LineId</th>\n",
       "      <th>Month</th>\n",
       "      <th>Date</th>\n",
       "      <th>Time</th>\n",
       "      <th>User</th>\n",
       "      <th>Component</th>\n",
       "      <th>PID</th>\n",
       "      <th>Address</th>\n",
       "      <th>Content</th>\n",
       "      <th>EventId</th>\n",
       "      <th>EventTemplate</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>prev_ts</th>\n",
       "      <th>diff_seconds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1283</td>\n",
       "      <td>Jul</td>\n",
       "      <td>5</td>\n",
       "      <td>16:24:44</td>\n",
       "      <td>airbears2-10-142-108-38</td>\n",
       "      <td>QQ</td>\n",
       "      <td>10018</td>\n",
       "      <td>NaN</td>\n",
       "      <td>button report: 0x8002bdf</td>\n",
       "      <td>E161</td>\n",
       "      <td>button report: &lt;*&gt;</td>\n",
       "      <td>2025-07-05 16:24:44</td>\n",
       "      <td>NaT</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1284</td>\n",
       "      <td>Jul</td>\n",
       "      <td>5</td>\n",
       "      <td>16:26:28</td>\n",
       "      <td>airbears2-10-142-108-38</td>\n",
       "      <td>corecaptured</td>\n",
       "      <td>36150</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CCLogTap::profileRemoved, Owner: com.apple.iok...</td>\n",
       "      <td>E177</td>\n",
       "      <td>CCLogTap::profileRemoved, Owner: com.apple.iok...</td>\n",
       "      <td>2025-07-05 16:26:28</td>\n",
       "      <td>2025-07-05 16:24:44</td>\n",
       "      <td>104.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1285</td>\n",
       "      <td>Jul</td>\n",
       "      <td>5</td>\n",
       "      <td>16:33:10</td>\n",
       "      <td>airbears2-10-142-108-38</td>\n",
       "      <td>locationd</td>\n",
       "      <td>82</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Location icon should now be in state 'Inactive'</td>\n",
       "      <td>E259</td>\n",
       "      <td>Location icon should now be in state 'Inactive'</td>\n",
       "      <td>2025-07-05 16:33:10</td>\n",
       "      <td>2025-07-05 16:26:28</td>\n",
       "      <td>402.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1286</td>\n",
       "      <td>Jul</td>\n",
       "      <td>5</td>\n",
       "      <td>16:37:58</td>\n",
       "      <td>airbears2-10-142-108-38</td>\n",
       "      <td>syslogd</td>\n",
       "      <td>44</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ASL Sender Statistics</td>\n",
       "      <td>E153</td>\n",
       "      <td>ASL Sender Statistics</td>\n",
       "      <td>2025-07-05 16:37:58</td>\n",
       "      <td>2025-07-05 16:33:10</td>\n",
       "      <td>288.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1287</td>\n",
       "      <td>Jul</td>\n",
       "      <td>5</td>\n",
       "      <td>16:47:16</td>\n",
       "      <td>airbears2-10-142-108-38</td>\n",
       "      <td>Safari</td>\n",
       "      <td>9852</td>\n",
       "      <td>NaN</td>\n",
       "      <td>KeychainGetICDPStatus: keychain: -25300</td>\n",
       "      <td>E256</td>\n",
       "      <td>KeychainGetICDPStatus: keychain: &lt;*&gt;</td>\n",
       "      <td>2025-07-05 16:47:16</td>\n",
       "      <td>2025-07-05 16:37:58</td>\n",
       "      <td>558.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   LineId Month  Date      Time                     User     Component    PID  \\\n",
       "0    1283   Jul     5  16:24:44  airbears2-10-142-108-38            QQ  10018   \n",
       "1    1284   Jul     5  16:26:28  airbears2-10-142-108-38  corecaptured  36150   \n",
       "2    1285   Jul     5  16:33:10  airbears2-10-142-108-38     locationd     82   \n",
       "3    1286   Jul     5  16:37:58  airbears2-10-142-108-38       syslogd     44   \n",
       "4    1287   Jul     5  16:47:16  airbears2-10-142-108-38        Safari   9852   \n",
       "\n",
       "  Address                                            Content EventId  \\\n",
       "0     NaN                           button report: 0x8002bdf    E161   \n",
       "1     NaN  CCLogTap::profileRemoved, Owner: com.apple.iok...    E177   \n",
       "2     NaN    Location icon should now be in state 'Inactive'    E259   \n",
       "3     NaN                              ASL Sender Statistics    E153   \n",
       "4     NaN            KeychainGetICDPStatus: keychain: -25300    E256   \n",
       "\n",
       "                                       EventTemplate           timestamp  \\\n",
       "0                                 button report: <*> 2025-07-05 16:24:44   \n",
       "1  CCLogTap::profileRemoved, Owner: com.apple.iok... 2025-07-05 16:26:28   \n",
       "2    Location icon should now be in state 'Inactive' 2025-07-05 16:33:10   \n",
       "3                              ASL Sender Statistics 2025-07-05 16:37:58   \n",
       "4               KeychainGetICDPStatus: keychain: <*> 2025-07-05 16:47:16   \n",
       "\n",
       "              prev_ts  diff_seconds  \n",
       "0                 NaT           0.0  \n",
       "1 2025-07-05 16:24:44         104.0  \n",
       "2 2025-07-05 16:26:28         402.0  \n",
       "3 2025-07-05 16:33:10         288.0  \n",
       "4 2025-07-05 16:37:58         558.0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sorted.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "99e18606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session numbering complete. Sample columns:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>diff_seconds</th>\n",
       "      <th>new_session</th>\n",
       "      <th>session_id_local</th>\n",
       "      <th>row_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>airbears2-10-142-108-38</td>\n",
       "      <td>2025-07-05 16:24:44</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>airbears2-10-142-108-38</td>\n",
       "      <td>2025-07-05 16:26:28</td>\n",
       "      <td>104.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>airbears2-10-142-108-38</td>\n",
       "      <td>2025-07-05 16:33:10</td>\n",
       "      <td>402.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>airbears2-10-142-108-38</td>\n",
       "      <td>2025-07-05 16:37:58</td>\n",
       "      <td>288.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>airbears2-10-142-108-38</td>\n",
       "      <td>2025-07-05 16:47:16</td>\n",
       "      <td>558.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>airbears2-10-142-108-38</td>\n",
       "      <td>2025-07-05 16:47:16</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>airbears2-10-142-108-38</td>\n",
       "      <td>2025-07-05 16:47:58</td>\n",
       "      <td>42.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>airbears2-10-142-108-38</td>\n",
       "      <td>2025-07-05 16:58:24</td>\n",
       "      <td>626.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>airbears2-10-142-108-38</td>\n",
       "      <td>2025-07-05 16:58:25</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>airbears2-10-142-108-38</td>\n",
       "      <td>2025-07-05 16:58:39</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>airbears2-10-142-108-38</td>\n",
       "      <td>2025-07-05 17:01:33</td>\n",
       "      <td>174.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>airbears2-10-142-108-38</td>\n",
       "      <td>2025-07-05 17:02:52</td>\n",
       "      <td>79.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>airbears2-10-142-108-38</td>\n",
       "      <td>2025-07-05 17:03:07</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>airbears2-10-142-108-38</td>\n",
       "      <td>2025-07-05 17:03:12</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>airbears2-10-142-108-38</td>\n",
       "      <td>2025-07-05 17:03:12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>airbears2-10-142-110-255</td>\n",
       "      <td>2025-07-03 21:30:55</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>airbears2-10-142-110-255</td>\n",
       "      <td>2025-07-03 21:35:36</td>\n",
       "      <td>281.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>airbears2-10-142-110-255</td>\n",
       "      <td>2025-07-03 21:41:47</td>\n",
       "      <td>371.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>airbears2-10-142-110-255</td>\n",
       "      <td>2025-07-03 22:16:48</td>\n",
       "      <td>2101.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>airbears2-10-142-110-255</td>\n",
       "      <td>2025-07-03 22:32:26</td>\n",
       "      <td>938.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        User           timestamp  diff_seconds  new_session  \\\n",
       "0    airbears2-10-142-108-38 2025-07-05 16:24:44           0.0            0   \n",
       "1    airbears2-10-142-108-38 2025-07-05 16:26:28         104.0            0   \n",
       "2    airbears2-10-142-108-38 2025-07-05 16:33:10         402.0            0   \n",
       "3    airbears2-10-142-108-38 2025-07-05 16:37:58         288.0            0   \n",
       "4    airbears2-10-142-108-38 2025-07-05 16:47:16         558.0            0   \n",
       "5    airbears2-10-142-108-38 2025-07-05 16:47:16           0.0            0   \n",
       "6    airbears2-10-142-108-38 2025-07-05 16:47:58          42.0            0   \n",
       "7    airbears2-10-142-108-38 2025-07-05 16:58:24         626.0            0   \n",
       "8    airbears2-10-142-108-38 2025-07-05 16:58:25           1.0            0   \n",
       "9    airbears2-10-142-108-38 2025-07-05 16:58:39          14.0            0   \n",
       "10   airbears2-10-142-108-38 2025-07-05 17:01:33         174.0            0   \n",
       "11   airbears2-10-142-108-38 2025-07-05 17:02:52          79.0            0   \n",
       "12   airbears2-10-142-108-38 2025-07-05 17:03:07          15.0            0   \n",
       "13   airbears2-10-142-108-38 2025-07-05 17:03:12           5.0            0   \n",
       "14   airbears2-10-142-108-38 2025-07-05 17:03:12           0.0            0   \n",
       "15  airbears2-10-142-110-255 2025-07-03 21:30:55           0.0            0   \n",
       "16  airbears2-10-142-110-255 2025-07-03 21:35:36         281.0            0   \n",
       "17  airbears2-10-142-110-255 2025-07-03 21:41:47         371.0            0   \n",
       "18  airbears2-10-142-110-255 2025-07-03 22:16:48        2101.0            1   \n",
       "19  airbears2-10-142-110-255 2025-07-03 22:32:26         938.0            0   \n",
       "\n",
       "    session_id_local  row_num  \n",
       "0                  0        1  \n",
       "1                  0        2  \n",
       "2                  0        3  \n",
       "3                  0        4  \n",
       "4                  0        5  \n",
       "5                  0        6  \n",
       "6                  0        7  \n",
       "7                  0        8  \n",
       "8                  0        9  \n",
       "9                  0       10  \n",
       "10                 0       11  \n",
       "11                 0       12  \n",
       "12                 0       13  \n",
       "13                 0       14  \n",
       "14                 0       15  \n",
       "15                 0        1  \n",
       "16                 0        2  \n",
       "17                 0        3  \n",
       "18                 1        1  \n",
       "19                 1        2  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Cell 3: mark new sessions (gap > 20 minutes), then number sessions per user starting at 1 and rank rows in session\n",
    "CUT_OFF_SECONDS = 20 * 60   # 20 minutes\n",
    "\n",
    "# new_session = 1 when it's the first row for a user OR diff > cutoff\n",
    "df_sorted['new_session'] = ((df_sorted['diff_seconds'] > CUT_OFF_SECONDS) | (~np.isfinite(df_sorted['diff_seconds']))).astype(int)\n",
    "\n",
    "# session id per user starting at 1\n",
    "df_sorted['session_id_local'] = df_sorted.groupby('User')['new_session'].cumsum().astype(int)  # starts at 0 or 1 depending on new_session for first row\n",
    "# ensure numbering starts at 1 for first session: if first row new_session was 1, cumsum gives 1; if diff was inf then new_session=1 -> good.\n",
    "\n",
    "# rank rows within each session (1,2,3...)\n",
    "df_sorted['row_num'] = df_sorted.groupby(['User','session_id_local']).cumcount() + 1\n",
    "\n",
    "print(\"Session numbering complete. Sample columns:\")\n",
    "display(df_sorted[['User','timestamp','diff_seconds','new_session','session_id_local','row_num']].head(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cb16cc71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LineId</th>\n",
       "      <th>Month</th>\n",
       "      <th>Date</th>\n",
       "      <th>Time</th>\n",
       "      <th>User</th>\n",
       "      <th>Component</th>\n",
       "      <th>PID</th>\n",
       "      <th>Address</th>\n",
       "      <th>Content</th>\n",
       "      <th>EventId</th>\n",
       "      <th>EventTemplate</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>prev_ts</th>\n",
       "      <th>diff_seconds</th>\n",
       "      <th>new_session</th>\n",
       "      <th>session_id_local</th>\n",
       "      <th>row_num</th>\n",
       "      <th>sessions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1283</td>\n",
       "      <td>Jul</td>\n",
       "      <td>5</td>\n",
       "      <td>16:24:44</td>\n",
       "      <td>airbears2-10-142-108-38</td>\n",
       "      <td>QQ</td>\n",
       "      <td>10018</td>\n",
       "      <td>NaN</td>\n",
       "      <td>button report: 0x8002bdf</td>\n",
       "      <td>E161</td>\n",
       "      <td>button report: &lt;*&gt;</td>\n",
       "      <td>2025-07-05 16:24:44</td>\n",
       "      <td>NaT</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1284</td>\n",
       "      <td>Jul</td>\n",
       "      <td>5</td>\n",
       "      <td>16:26:28</td>\n",
       "      <td>airbears2-10-142-108-38</td>\n",
       "      <td>corecaptured</td>\n",
       "      <td>36150</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CCLogTap::profileRemoved, Owner: com.apple.iok...</td>\n",
       "      <td>E177</td>\n",
       "      <td>CCLogTap::profileRemoved, Owner: com.apple.iok...</td>\n",
       "      <td>2025-07-05 16:26:28</td>\n",
       "      <td>2025-07-05 16:24:44</td>\n",
       "      <td>104.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1285</td>\n",
       "      <td>Jul</td>\n",
       "      <td>5</td>\n",
       "      <td>16:33:10</td>\n",
       "      <td>airbears2-10-142-108-38</td>\n",
       "      <td>locationd</td>\n",
       "      <td>82</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Location icon should now be in state 'Inactive'</td>\n",
       "      <td>E259</td>\n",
       "      <td>Location icon should now be in state 'Inactive'</td>\n",
       "      <td>2025-07-05 16:33:10</td>\n",
       "      <td>2025-07-05 16:26:28</td>\n",
       "      <td>402.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1286</td>\n",
       "      <td>Jul</td>\n",
       "      <td>5</td>\n",
       "      <td>16:37:58</td>\n",
       "      <td>airbears2-10-142-108-38</td>\n",
       "      <td>syslogd</td>\n",
       "      <td>44</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ASL Sender Statistics</td>\n",
       "      <td>E153</td>\n",
       "      <td>ASL Sender Statistics</td>\n",
       "      <td>2025-07-05 16:37:58</td>\n",
       "      <td>2025-07-05 16:33:10</td>\n",
       "      <td>288.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1287</td>\n",
       "      <td>Jul</td>\n",
       "      <td>5</td>\n",
       "      <td>16:47:16</td>\n",
       "      <td>airbears2-10-142-108-38</td>\n",
       "      <td>Safari</td>\n",
       "      <td>9852</td>\n",
       "      <td>NaN</td>\n",
       "      <td>KeychainGetICDPStatus: keychain: -25300</td>\n",
       "      <td>E256</td>\n",
       "      <td>KeychainGetICDPStatus: keychain: &lt;*&gt;</td>\n",
       "      <td>2025-07-05 16:47:16</td>\n",
       "      <td>2025-07-05 16:37:58</td>\n",
       "      <td>558.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   LineId Month  Date      Time                     User     Component    PID  \\\n",
       "0    1283   Jul     5  16:24:44  airbears2-10-142-108-38            QQ  10018   \n",
       "1    1284   Jul     5  16:26:28  airbears2-10-142-108-38  corecaptured  36150   \n",
       "2    1285   Jul     5  16:33:10  airbears2-10-142-108-38     locationd     82   \n",
       "3    1286   Jul     5  16:37:58  airbears2-10-142-108-38       syslogd     44   \n",
       "4    1287   Jul     5  16:47:16  airbears2-10-142-108-38        Safari   9852   \n",
       "\n",
       "  Address                                            Content EventId  \\\n",
       "0     NaN                           button report: 0x8002bdf    E161   \n",
       "1     NaN  CCLogTap::profileRemoved, Owner: com.apple.iok...    E177   \n",
       "2     NaN    Location icon should now be in state 'Inactive'    E259   \n",
       "3     NaN                              ASL Sender Statistics    E153   \n",
       "4     NaN            KeychainGetICDPStatus: keychain: -25300    E256   \n",
       "\n",
       "                                       EventTemplate           timestamp  \\\n",
       "0                                 button report: <*> 2025-07-05 16:24:44   \n",
       "1  CCLogTap::profileRemoved, Owner: com.apple.iok... 2025-07-05 16:26:28   \n",
       "2    Location icon should now be in state 'Inactive' 2025-07-05 16:33:10   \n",
       "3                              ASL Sender Statistics 2025-07-05 16:37:58   \n",
       "4               KeychainGetICDPStatus: keychain: <*> 2025-07-05 16:47:16   \n",
       "\n",
       "              prev_ts  diff_seconds  new_session  session_id_local  row_num  \\\n",
       "0                 NaT           0.0            0                 0        1   \n",
       "1 2025-07-05 16:24:44         104.0            0                 0        2   \n",
       "2 2025-07-05 16:26:28         402.0            0                 0        3   \n",
       "3 2025-07-05 16:33:10         288.0            0                 0        4   \n",
       "4 2025-07-05 16:37:58         558.0            0                 0        5   \n",
       "\n",
       "   sessions  \n",
       "0         1  \n",
       "1         1  \n",
       "2         1  \n",
       "3         1  \n",
       "4         1  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# New column 'sessions': increments by 1 whenever row_num == 1 (start of a new session) per User\n",
    "df_new= df_sorted[df_sorted['row_num']==1].reset_index().reset_index()[['level_0','index']]\n",
    "df_new.rename({'level_0':'sessions'}, axis=1, inplace=True)\n",
    "df_new.set_index('index', inplace=True)\n",
    "\n",
    "df_new.head()\n",
    "\n",
    "df_sorted= df_sorted.merge(df_new, how='left', left_index=True, right_index=True)\n",
    "df_sorted['sessions']= df_sorted['sessions'].ffill().astype(int)\n",
    "df_sorted['sessions']= df_sorted['sessions']+1\n",
    "\n",
    "df_sorted.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8bf82c2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sessions</th>\n",
       "      <th>Content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>button report: 0x8002bdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2</td>\n",
       "      <td>Sandbox: com.apple.Addres(33959) deny(1) netwo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>3</td>\n",
       "      <td>send_datagram_available_ping: pid 445 failed t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>4</td>\n",
       "      <td>NETWORK: requery, 0, 0, 0, 0, 320, items, fQue...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>5</td>\n",
       "      <td>com.apple.SoftwareUpdate.Activity: scheduler_e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1946</th>\n",
       "      <td>188</td>\n",
       "      <td>doSaveChannels@286: Will write to: /Library/Lo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1960</th>\n",
       "      <td>189</td>\n",
       "      <td>ChromeExistion main isUndetectWithCommand = 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1964</th>\n",
       "      <td>190</td>\n",
       "      <td>Location icon should now be in state 'Active'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1972</th>\n",
       "      <td>191</td>\n",
       "      <td>WARNING: hibernate_page_list_setall skipped 19...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1974</th>\n",
       "      <td>192</td>\n",
       "      <td>setting hostname to \"calvisitor-10-105-163-9.c...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>192 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      sessions                                            Content\n",
       "0            1                           button report: 0x8002bdf\n",
       "15           2  Sandbox: com.apple.Addres(33959) deny(1) netwo...\n",
       "18           3  send_datagram_available_ping: pid 445 failed t...\n",
       "20           4  NETWORK: requery, 0, 0, 0, 0, 320, items, fQue...\n",
       "29           5  com.apple.SoftwareUpdate.Activity: scheduler_e...\n",
       "...        ...                                                ...\n",
       "1946       188  doSaveChannels@286: Will write to: /Library/Lo...\n",
       "1960       189      ChromeExistion main isUndetectWithCommand = 1\n",
       "1964       190      Location icon should now be in state 'Active'\n",
       "1972       191  WARNING: hibernate_page_list_setall skipped 19...\n",
       "1974       192  setting hostname to \"calvisitor-10-105-163-9.c...\n",
       "\n",
       "[192 rows x 2 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sorted[df_sorted['row_num']==1][['sessions','Content']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "68877160",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_x/v81sxk6j38zdg2y882m0qmj40000gn/T/ipykernel_12806/3114995774.py:5: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda g: pd.Series({\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LineId</th>\n",
       "      <th>Month</th>\n",
       "      <th>Date</th>\n",
       "      <th>Time</th>\n",
       "      <th>User</th>\n",
       "      <th>Component</th>\n",
       "      <th>PID</th>\n",
       "      <th>Address</th>\n",
       "      <th>Content</th>\n",
       "      <th>EventId</th>\n",
       "      <th>EventTemplate</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>prev_ts</th>\n",
       "      <th>diff_seconds</th>\n",
       "      <th>new_session</th>\n",
       "      <th>session_id_local</th>\n",
       "      <th>row_num</th>\n",
       "      <th>sessions</th>\n",
       "      <th>starter_log</th>\n",
       "      <th>chain_log</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1283</td>\n",
       "      <td>Jul</td>\n",
       "      <td>5</td>\n",
       "      <td>16:24:44</td>\n",
       "      <td>airbears2-10-142-108-38</td>\n",
       "      <td>QQ</td>\n",
       "      <td>10018</td>\n",
       "      <td>NaN</td>\n",
       "      <td>button report: 0x8002bdf</td>\n",
       "      <td>E161</td>\n",
       "      <td>button report: &lt;*&gt;</td>\n",
       "      <td>2025-07-05 16:24:44</td>\n",
       "      <td>NaT</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>button report: 0x8002bdf</td>\n",
       "      <td>button report: 0x8002bdf -&gt; CCLogTap::profileR...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1284</td>\n",
       "      <td>Jul</td>\n",
       "      <td>5</td>\n",
       "      <td>16:26:28</td>\n",
       "      <td>airbears2-10-142-108-38</td>\n",
       "      <td>corecaptured</td>\n",
       "      <td>36150</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CCLogTap::profileRemoved, Owner: com.apple.iok...</td>\n",
       "      <td>E177</td>\n",
       "      <td>CCLogTap::profileRemoved, Owner: com.apple.iok...</td>\n",
       "      <td>2025-07-05 16:26:28</td>\n",
       "      <td>2025-07-05 16:24:44</td>\n",
       "      <td>104.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>button report: 0x8002bdf</td>\n",
       "      <td>button report: 0x8002bdf -&gt; CCLogTap::profileR...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1285</td>\n",
       "      <td>Jul</td>\n",
       "      <td>5</td>\n",
       "      <td>16:33:10</td>\n",
       "      <td>airbears2-10-142-108-38</td>\n",
       "      <td>locationd</td>\n",
       "      <td>82</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Location icon should now be in state 'Inactive'</td>\n",
       "      <td>E259</td>\n",
       "      <td>Location icon should now be in state 'Inactive'</td>\n",
       "      <td>2025-07-05 16:33:10</td>\n",
       "      <td>2025-07-05 16:26:28</td>\n",
       "      <td>402.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>button report: 0x8002bdf</td>\n",
       "      <td>button report: 0x8002bdf -&gt; CCLogTap::profileR...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1286</td>\n",
       "      <td>Jul</td>\n",
       "      <td>5</td>\n",
       "      <td>16:37:58</td>\n",
       "      <td>airbears2-10-142-108-38</td>\n",
       "      <td>syslogd</td>\n",
       "      <td>44</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ASL Sender Statistics</td>\n",
       "      <td>E153</td>\n",
       "      <td>ASL Sender Statistics</td>\n",
       "      <td>2025-07-05 16:37:58</td>\n",
       "      <td>2025-07-05 16:33:10</td>\n",
       "      <td>288.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>button report: 0x8002bdf</td>\n",
       "      <td>button report: 0x8002bdf -&gt; CCLogTap::profileR...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1287</td>\n",
       "      <td>Jul</td>\n",
       "      <td>5</td>\n",
       "      <td>16:47:16</td>\n",
       "      <td>airbears2-10-142-108-38</td>\n",
       "      <td>Safari</td>\n",
       "      <td>9852</td>\n",
       "      <td>NaN</td>\n",
       "      <td>KeychainGetICDPStatus: keychain: -25300</td>\n",
       "      <td>E256</td>\n",
       "      <td>KeychainGetICDPStatus: keychain: &lt;*&gt;</td>\n",
       "      <td>2025-07-05 16:47:16</td>\n",
       "      <td>2025-07-05 16:37:58</td>\n",
       "      <td>558.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>button report: 0x8002bdf</td>\n",
       "      <td>button report: 0x8002bdf -&gt; CCLogTap::profileR...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   LineId Month  Date      Time                     User     Component    PID  \\\n",
       "0    1283   Jul     5  16:24:44  airbears2-10-142-108-38            QQ  10018   \n",
       "1    1284   Jul     5  16:26:28  airbears2-10-142-108-38  corecaptured  36150   \n",
       "2    1285   Jul     5  16:33:10  airbears2-10-142-108-38     locationd     82   \n",
       "3    1286   Jul     5  16:37:58  airbears2-10-142-108-38       syslogd     44   \n",
       "4    1287   Jul     5  16:47:16  airbears2-10-142-108-38        Safari   9852   \n",
       "\n",
       "  Address                                            Content EventId  \\\n",
       "0     NaN                           button report: 0x8002bdf    E161   \n",
       "1     NaN  CCLogTap::profileRemoved, Owner: com.apple.iok...    E177   \n",
       "2     NaN    Location icon should now be in state 'Inactive'    E259   \n",
       "3     NaN                              ASL Sender Statistics    E153   \n",
       "4     NaN            KeychainGetICDPStatus: keychain: -25300    E256   \n",
       "\n",
       "                                       EventTemplate           timestamp  \\\n",
       "0                                 button report: <*> 2025-07-05 16:24:44   \n",
       "1  CCLogTap::profileRemoved, Owner: com.apple.iok... 2025-07-05 16:26:28   \n",
       "2    Location icon should now be in state 'Inactive' 2025-07-05 16:33:10   \n",
       "3                              ASL Sender Statistics 2025-07-05 16:37:58   \n",
       "4               KeychainGetICDPStatus: keychain: <*> 2025-07-05 16:47:16   \n",
       "\n",
       "              prev_ts  diff_seconds  new_session  session_id_local  row_num  \\\n",
       "0                 NaT           0.0            0                 0        1   \n",
       "1 2025-07-05 16:24:44         104.0            0                 0        2   \n",
       "2 2025-07-05 16:26:28         402.0            0                 0        3   \n",
       "3 2025-07-05 16:33:10         288.0            0                 0        4   \n",
       "4 2025-07-05 16:37:58         558.0            0                 0        5   \n",
       "\n",
       "   sessions               starter_log  \\\n",
       "0         1  button report: 0x8002bdf   \n",
       "1         1  button report: 0x8002bdf   \n",
       "2         1  button report: 0x8002bdf   \n",
       "3         1  button report: 0x8002bdf   \n",
       "4         1  button report: 0x8002bdf   \n",
       "\n",
       "                                           chain_log  \n",
       "0  button report: 0x8002bdf -> CCLogTap::profileR...  \n",
       "1  button report: 0x8002bdf -> CCLogTap::profileR...  \n",
       "2  button report: 0x8002bdf -> CCLogTap::profileR...  \n",
       "3  button report: 0x8002bdf -> CCLogTap::profileR...  \n",
       "4  button report: 0x8002bdf -> CCLogTap::profileR...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Group by User + sessions and create starter and chain\n",
    "session_info = (\n",
    "    df_sorted\n",
    "    .groupby(['sessions'], sort=False)\n",
    "    .apply(lambda g: pd.Series({\n",
    "        'starter_log': g.sort_values('row_num')['Content'].iloc[0],\n",
    "        'chain_log': \" -> \".join(g.sort_values('row_num')['Content'].astype(str).tolist())\n",
    "    }))\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "df_sorted = df_sorted.merge(session_info, how='left', on='sessions')\n",
    "df_sorted.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2d9071f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sorted_1= df_sorted[['sessions','Month','Date','Time','Component','PID', 'Content', 'EventTemplate', 'starter_log', 'chain_log']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b2f72853",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sorted_1= df_sorted_1.merge(df_sorted_1.groupby('starter_log')['sessions'].nunique(),on='starter_log', how='left').rename({'sessions_y':'starter_log_freq','sessions_x':'sessions'}, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "754146bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sorted_1.drop(columns=['EventTemplate'],axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a01807ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sessions</th>\n",
       "      <th>Month</th>\n",
       "      <th>Date</th>\n",
       "      <th>Time</th>\n",
       "      <th>Component</th>\n",
       "      <th>PID</th>\n",
       "      <th>Content</th>\n",
       "      <th>starter_log</th>\n",
       "      <th>chain_log</th>\n",
       "      <th>starter_log_freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Jul</td>\n",
       "      <td>5</td>\n",
       "      <td>16:24:44</td>\n",
       "      <td>QQ</td>\n",
       "      <td>10018</td>\n",
       "      <td>button report: 0x8002bdf</td>\n",
       "      <td>button report: 0x8002bdf</td>\n",
       "      <td>button report: 0x8002bdf -&gt; CCLogTap::profileR...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Jul</td>\n",
       "      <td>5</td>\n",
       "      <td>16:26:28</td>\n",
       "      <td>corecaptured</td>\n",
       "      <td>36150</td>\n",
       "      <td>CCLogTap::profileRemoved, Owner: com.apple.iok...</td>\n",
       "      <td>button report: 0x8002bdf</td>\n",
       "      <td>button report: 0x8002bdf -&gt; CCLogTap::profileR...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Jul</td>\n",
       "      <td>5</td>\n",
       "      <td>16:33:10</td>\n",
       "      <td>locationd</td>\n",
       "      <td>82</td>\n",
       "      <td>Location icon should now be in state 'Inactive'</td>\n",
       "      <td>button report: 0x8002bdf</td>\n",
       "      <td>button report: 0x8002bdf -&gt; CCLogTap::profileR...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Jul</td>\n",
       "      <td>5</td>\n",
       "      <td>16:37:58</td>\n",
       "      <td>syslogd</td>\n",
       "      <td>44</td>\n",
       "      <td>ASL Sender Statistics</td>\n",
       "      <td>button report: 0x8002bdf</td>\n",
       "      <td>button report: 0x8002bdf -&gt; CCLogTap::profileR...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Jul</td>\n",
       "      <td>5</td>\n",
       "      <td>16:47:16</td>\n",
       "      <td>Safari</td>\n",
       "      <td>9852</td>\n",
       "      <td>KeychainGetICDPStatus: keychain: -25300</td>\n",
       "      <td>button report: 0x8002bdf</td>\n",
       "      <td>button report: 0x8002bdf -&gt; CCLogTap::profileR...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sessions Month  Date      Time     Component    PID  \\\n",
       "0         1   Jul     5  16:24:44            QQ  10018   \n",
       "1         1   Jul     5  16:26:28  corecaptured  36150   \n",
       "2         1   Jul     5  16:33:10     locationd     82   \n",
       "3         1   Jul     5  16:37:58       syslogd     44   \n",
       "4         1   Jul     5  16:47:16        Safari   9852   \n",
       "\n",
       "                                             Content  \\\n",
       "0                           button report: 0x8002bdf   \n",
       "1  CCLogTap::profileRemoved, Owner: com.apple.iok...   \n",
       "2    Location icon should now be in state 'Inactive'   \n",
       "3                              ASL Sender Statistics   \n",
       "4            KeychainGetICDPStatus: keychain: -25300   \n",
       "\n",
       "                starter_log  \\\n",
       "0  button report: 0x8002bdf   \n",
       "1  button report: 0x8002bdf   \n",
       "2  button report: 0x8002bdf   \n",
       "3  button report: 0x8002bdf   \n",
       "4  button report: 0x8002bdf   \n",
       "\n",
       "                                           chain_log  starter_log_freq  \n",
       "0  button report: 0x8002bdf -> CCLogTap::profileR...                 1  \n",
       "1  button report: 0x8002bdf -> CCLogTap::profileR...                 1  \n",
       "2  button report: 0x8002bdf -> CCLogTap::profileR...                 1  \n",
       "3  button report: 0x8002bdf -> CCLogTap::profileR...                 1  \n",
       "4  button report: 0x8002bdf -> CCLogTap::profileR...                 1  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sorted_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5f291398",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sorted_1.to_csv('/Users/varunraste/Downloads/RAG_demo/Mac_data_logs/Mac_2k_log_sessions.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "db39fb67",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sorted_2=df_sorted_1.head(50).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "808ac5a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared 50 Document objects.\n",
      "\n",
      "Sample doc text (truncated):\n",
      " <STARTER> button report: 0x8002bdf </STARTER>\n",
      "<SEQUENCE> button report: 0x8002bdf -> CCLogTap::profileRemoved, Owner: com.apple.iokit.IO80211Family, Name: IO80211AWDLPeerManager -> Location icon should now be in state 'Inactive' -> ASL Sender Statistics -> KeychainGetICDPStatus: keychain: -25300 -> \n",
      "\n",
      "Sample metadata:\n",
      " {'sessions': '1', 'month': 'Jul', 'date': '5', 'time': '16:24:44', 'component': 'QQ', 'pid': '10018', 'starter_log': 'button report: 0x8002bdf', 'starter_log_freq': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_x/v81sxk6j38zdg2y882m0qmj40000gn/T/ipykernel_12806/2457732122.py:34: DeprecationWarning: Call to deprecated function (or staticmethod) extra_info. ('extra_info' is deprecated, use 'metadata' instead.) -- Deprecated since version 0.12.2.\n",
      "  print(\"\\nSample metadata:\\n\", docs[0].extra_info)\n"
     ]
    }
   ],
   "source": [
    "# Cell 2  build LlamaIndex Documents from the exact columns you have\n",
    "# Each document contains starter + chain text and minimal metadata from your CSV.\n",
    "from llama_index.core import Document\n",
    "  # modern import alias\n",
    "docs = []\n",
    "\n",
    "for _, r in df_sorted_2.iterrows():\n",
    "    # Use only the available columns you listed\n",
    "    session_num = str(r.get('sessions', ''))\n",
    "    starter = str(r.get('starter_log', '') or '')\n",
    "    chain = str(r.get('chain_log', '') or '')\n",
    "    content_sample = str(r.get('Content', '') or '')  # fallback if chain missing\n",
    "\n",
    "    # Build text: prefer starter + chain, fallback to Content if needed\n",
    "    text = f\"<STARTER> {starter} </STARTER>\\n<SEQUENCE> {chain or content_sample} </SEQUENCE>\"\n",
    "\n",
    "    metadata = {\n",
    "        \"sessions\": session_num,\n",
    "        \"month\": str(r.get('Month','')),\n",
    "        \"date\": str(r.get('Date','')),\n",
    "        \"time\": str(r.get('Time','')),\n",
    "        \"component\": str(r.get('Component','')),\n",
    "        \"pid\": str(r.get('PID','')),\n",
    "        \"starter_log\": starter,\n",
    "        \"starter_log_freq\": r.get('starter_log_freq', None)\n",
    "    }\n",
    "    # remove empty values\n",
    "    metadata = {k:v for k,v in metadata.items() if v not in (\"\", None)}\n",
    "    docs.append(Document(text=text, extra_info=metadata))\n",
    "\n",
    "print(\"Prepared\", len(docs), \"Document objects.\")\n",
    "# show one sample\n",
    "print(\"\\nSample doc text (truncated):\\n\", docs[0].text[:300])\n",
    "print(\"\\nSample metadata:\\n\", docs[0].extra_info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1dbecc0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/varunraste/Downloads/RAG_demo/.venv/lib/python3.11/site-packages/milvus_lite/__init__.py:15: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import DistributionNotFound, get_distribution\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 1) build node parser (split by '->' so chunks align with event boundaries)\n",
    "parser = SentenceSplitter(chunk_size=512, chunk_overlap=64, separator=\"->\")\n",
    "\n",
    "# 3) Milvus-Lite vector store (local file). dim=768 for Nomic v1.5\n",
    "vector_store = MilvusVectorStore(\n",
    "    uri=\"/Users/varunraste/Downloads/RAG_demo/milvus_lite_2.db\", \n",
    "    collection_name=\"rag_docs_v4\",\n",
    "    dim=768,\n",
    "    overwrite=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "90a03772",
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "28c7b394",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1758098677.557673 3003314 fork_posix.cc:71] Other threads are currently calling into gRPC, skipping fork() handlers\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index built and persisted to Milvus Lite (collection: rag_docs_v4).\n"
     ]
    }
   ],
   "source": [
    "# 4) Build index (this will chunk documents, embed, and insert into Milvus)\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents=docs,\n",
    "    storage_context=storage_context,\n",
    "    embed_model=embed_model,\n",
    "    node_parser=parser\n",
    ")\n",
    "\n",
    "print(\"Index built and persisted to Milvus Lite (collection: rag_docs_v4).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6d7985e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_question = \"I am getting this error in the log 'NETWORK: requery, 0, 0, 0, 0, 320, items, fQueryRetries, 1, fLastRetryTimestamp, 520841203.7' What could be issues and next steps ? \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ac5671e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Override: stricter prompt (exact sections), no post-processing, verbose debug\n",
    "from typing import Any, Dict, Iterable, List, Tuple, Optional\n",
    "import re\n",
    "import difflib\n",
    "\n",
    "\n",
    "def rag_chat_simple(\n",
    "    user_query: str,\n",
    "    base_k: int = 6,\n",
    "    reranking_top_k: int = 3,\n",
    "    llm_callable=None,\n",
    "    verbose: bool = True,\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"RAG chat with a stricter prompt to reduce hallucinations.\n",
    "\n",
    "    - Prints top chunks, predicted section, allowed components, final prompt\n",
    "    - Uses Predicted Next Logs explicitly when present\n",
    "    - Returns raw LLM output (no post-processing)\n",
    "    \"\"\"\n",
    "\n",
    "    # --- helpers ---\n",
    "    seq_pat = re.compile(r\"<SEQUENCE>\\s*(.*?)\\s*</SEQUENCE>\", re.DOTALL | re.IGNORECASE)\n",
    "\n",
    "    def normalize_text(value: Optional[str]) -> str:\n",
    "        if not value:\n",
    "            return \"\"\n",
    "        return \" \".join(value.strip().strip('\"').strip(\"'\").lower().split())\n",
    "\n",
    "    def fuzzy_ratio(a: str, b: str) -> float:\n",
    "        return difflib.SequenceMatcher(None, a, b).ratio()\n",
    "\n",
    "    def get_node_text(node: Any) -> Optional[str]:\n",
    "        try:\n",
    "            if hasattr(node, \"text\") and isinstance(getattr(node, \"text\"), str):\n",
    "                return getattr(node, \"text\")\n",
    "            if hasattr(node, \"get_content\") and callable(getattr(node, \"get_content\")):\n",
    "                return node.get_content()\n",
    "            inner = getattr(node, \"node\", None)\n",
    "            if inner is not None:\n",
    "                if hasattr(inner, \"text\") and isinstance(getattr(inner, \"text\"), str):\n",
    "                    return getattr(inner, \"text\")\n",
    "                if hasattr(inner, \"get_content\") and callable(getattr(inner, \"get_content\")):\n",
    "                    return inner.get_content()\n",
    "        except Exception:\n",
    "            return None\n",
    "        return None\n",
    "\n",
    "    def get_node_metadata(node: Any) -> Dict[str, Any]:\n",
    "        try:\n",
    "            if hasattr(node, \"metadata\") and isinstance(getattr(node, \"metadata\"), dict):\n",
    "                return dict(getattr(node, \"metadata\"))\n",
    "            inner = getattr(node, \"node\", None)\n",
    "            if inner is not None and hasattr(inner, \"metadata\"):\n",
    "                try:\n",
    "                    return dict(getattr(inner, \"metadata\"))\n",
    "                except Exception:\n",
    "                    return {}\n",
    "        except Exception:\n",
    "            return {}\n",
    "        return {}\n",
    "\n",
    "    def extract_sequence_block(text: Optional[str]) -> Optional[str]:\n",
    "        if not text:\n",
    "            return None\n",
    "        m = seq_pat.search(text)\n",
    "        return m.group(1).strip() if m else None\n",
    "\n",
    "    def parse_sequence_steps(sequence_block: Optional[str]) -> List[str]:\n",
    "        if not sequence_block:\n",
    "            return []\n",
    "        parts = [p.strip().strip('\"').strip(\"'\") for p in sequence_block.split(\"->\")]\n",
    "        return [p for p in parts if p]\n",
    "\n",
    "    def best_position_for_query(steps: List[str], q: str) -> Tuple[int, bool, float, str]:\n",
    "        if not steps:\n",
    "            return 1_000_000, False, 0.0, \"\"\n",
    "        qn = normalize_text(q)\n",
    "        for i, step in enumerate(steps):\n",
    "            if normalize_text(step) == qn:\n",
    "                return i + 1, True, 1.0, step\n",
    "        best_i, best_sim, best_step = 0, -1.0, steps[0]\n",
    "        for i, step in enumerate(steps):\n",
    "            sim = fuzzy_ratio(normalize_text(step), qn)\n",
    "            if sim > best_sim:\n",
    "                best_i, best_sim, best_step = i, sim, step\n",
    "        return best_i + 1, False, best_sim, best_step\n",
    "\n",
    "    def rerank_nodes_by_sequence_query(nodes: Iterable[Any], q: str, top_k: int) -> List[Dict[str, Any]]:\n",
    "        scored: List[Dict[str, Any]] = []\n",
    "        for node in nodes:\n",
    "            text = get_node_text(node)\n",
    "            seq_block = extract_sequence_block(text)\n",
    "            if not seq_block:\n",
    "                continue\n",
    "            steps = parse_sequence_steps(seq_block)\n",
    "            pos, is_exact, sim, matched = best_position_for_query(steps, q)\n",
    "            scored.append({\n",
    "                \"node\": node,\n",
    "                \"position\": pos,\n",
    "                \"is_exact\": is_exact,\n",
    "                \"similarity\": sim,\n",
    "                \"matched_step\": matched,\n",
    "                \"steps\": steps,\n",
    "                \"sequence_text\": seq_block,\n",
    "                \"metadata\": get_node_metadata(node),\n",
    "            })\n",
    "        scored.sort(key=lambda d: (d[\"position\"], -d[\"similarity\"]))\n",
    "        return scored[:top_k]\n",
    "\n",
    "    def predict_next_steps_from_top_nodes(top_items: List[Dict[str, Any]]) -> List[str]:\n",
    "        seen = set()\n",
    "        predictions: List[str] = []\n",
    "        for it in top_items:\n",
    "            steps = it.get(\"steps\", [])\n",
    "            pos = max(1, int(it.get(\"position\", 1)))\n",
    "            tail = steps[pos - 1 if pos >= len(steps) else pos:]\n",
    "            if not tail and steps:\n",
    "                tail = [steps[-1]]\n",
    "            if not tail:\n",
    "                continue\n",
    "            seq = \" -> \".join(tail)\n",
    "            if seq not in seen:\n",
    "                seen.add(seq)\n",
    "                predictions.append(seq)\n",
    "        return predictions\n",
    "\n",
    "    def build_allowed_components(top_items: List[Dict[str, Any]]) -> List[str]:\n",
    "        comps: List[str] = []\n",
    "        seen_c = set()\n",
    "        for it in top_items:\n",
    "            meta = it.get(\"metadata\", {}) or {}\n",
    "            comp = meta.get(\"component\")\n",
    "            if comp is not None:\n",
    "                comp_s = str(comp)\n",
    "                if comp_s not in seen_c:\n",
    "                    seen_c.add(comp_s)\n",
    "                    comps.append(comp_s)\n",
    "        return comps\n",
    "\n",
    "    def format_context_from_top_nodes(top_items: List[Dict[str, Any]]) -> str:\n",
    "        blocks = []\n",
    "        for idx, it in enumerate(top_items, start=1):\n",
    "            meta = it.get(\"metadata\", {})\n",
    "            meta_str = \", \".join(f\"{k}: {v}\" for k, v in meta.items()) if isinstance(meta, dict) else str(meta)\n",
    "            block = (\n",
    "                f\"[CHUNK {idx}]\\n\"\n",
    "                f\"METADATA: {meta_str}\\n\"\n",
    "                f\"SEQUENCE: <SEQUENCE> {it.get('sequence_text', '')} </SEQUENCE>\\n\"\n",
    "            )\n",
    "            blocks.append(block)\n",
    "        return \"\\n\".join(blocks)\n",
    "\n",
    "    def format_pred_section(pred_steps: List[str]) -> str:\n",
    "        if not pred_steps:\n",
    "            return \"\"\n",
    "        return \"\\n\".join(f\"- {seq}\" for seq in pred_steps)\n",
    "\n",
    "    def build_sre_prompt(context: str, pred_section: str, question: str, allowed_components: List[str]) -> str:\n",
    "        allowed_comps_str = \", \".join(allowed_components) if allowed_components else \"(none)\"\n",
    "        return f\"\"\"\n",
    "You are an expert SRE whose job is to debug MaC logs. Use ONLY the provided context chunks and the Predicted Next Logs below (do not hallucinate).\n",
    "\n",
    "Context Chunks:\n",
    "{context}\n",
    "\n",
    "Predicted Next Logs (if any):\n",
    "{pred_section}\n",
    "\n",
    "Instructions:\n",
    "- If Predicted Next Logs is non-empty, base NEXT-PREDICTIONS primarily on these items.\n",
    "- If it is empty, use <SEQUENCE> content in the context to reason about next predictions. Do NOT mention that predictions were empty.\n",
    "- Keep the answer concise, structured, and grounded strictly in the provided data.\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Output format (MANDATORY): Return EXACTLY the following six numbered sections with these exact headings and nothing else. Do NOT add any other headings, labels, or prose before/after.\n",
    "1) RCA: 1-2 sentences describing the root cause\n",
    "2) COMPONENTS: list the component(s) responsible (choose only from: {allowed_comps_str}; if none apply, write 'UNKNOWN')\n",
    "3) SUGGESTED FIXES: exactly 3 concrete, actionable steps (imperative voice)\n",
    "4) NEXT-PREDICTIONS: 2-4 likely next events with brief rationale; if Predicted Next Logs is non-empty, derive strictly from it, otherwise from <SEQUENCE>\n",
    "5) SUMMARY: 1-2 sentences summarizing RCA, component, and the next actions\n",
    "6) CONFIDENCE: only a single integer 0-100 (no % sign, no extra words)\n",
    "\"\"\".strip()\n",
    "\n",
    "    # --- auto-detected retrieval ---\n",
    "    nodes = None\n",
    "    try:\n",
    "        global retriever  # type: ignore[name-defined]\n",
    "        if 'retriever' in globals():\n",
    "            r = globals()['retriever']\n",
    "            if hasattr(r, 'retrieve') and callable(getattr(r, 'retrieve')):\n",
    "                res = r.retrieve(user_query)\n",
    "                nodes = list(res)[:base_k]\n",
    "            elif callable(r):\n",
    "                try:\n",
    "                    nodes = list(r(user_query, base_k))\n",
    "                except TypeError:\n",
    "                    nodes = list(r(user_query))[:base_k]\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    if nodes is None:\n",
    "        try:\n",
    "            if 'query_engine' in globals():\n",
    "                qe = globals()['query_engine']\n",
    "                if hasattr(qe, 'query') and callable(getattr(qe, 'query')):\n",
    "                    resp = qe.query(user_query)\n",
    "                    src = getattr(resp, 'source_nodes', None)\n",
    "                    if src is not None:\n",
    "                        nodes = list(src)[:base_k]\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    if nodes is None:\n",
    "        try:\n",
    "            if 'index' in globals():\n",
    "                idx = globals()['index']\n",
    "                if hasattr(idx, 'as_retriever') and callable(getattr(idx, 'as_retriever')):\n",
    "                    r = idx.as_retriever(similarity_top_k=base_k)\n",
    "                    res = r.retrieve(user_query)\n",
    "                    nodes = list(res)[:base_k]\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    if nodes is None:\n",
    "        raise RuntimeError(\"No retriever/query_engine/index found in globals. Define a global `retriever`, `query_engine`, or `index`.\")\n",
    "\n",
    "    # --- rerank, predict, prompt, llm ---\n",
    "    top_items = rerank_nodes_by_sequence_query(nodes, q=user_query, top_k=reranking_top_k)\n",
    "    predictions = predict_next_steps_from_top_nodes(top_items)\n",
    "    context_text = format_context_from_top_nodes(top_items)\n",
    "    pred_text = format_pred_section(predictions)\n",
    "    allowed_components = build_allowed_components(top_items)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"[DEBUG] Top reranked chunks (earliest position first; ties by similarity):\")\n",
    "        for i, it in enumerate(top_items, start=1):\n",
    "            print(f\"  - Rank {i}: position={it['position']}, is_exact={it['is_exact']}, similarity={it['similarity']:.3f}\")\n",
    "            print(f\"    matched_step: {it['matched_step']}\")\n",
    "            print(f\"    metadata: {it.get('metadata', {})}\")\n",
    "            print(f\"    sequence: <SEQUENCE> {it.get('sequence_text','')} </SEQUENCE>\")\n",
    "        print(\"\\n[DEBUG] Predicted Next Logs:\")\n",
    "        if predictions:\n",
    "            for p in predictions:\n",
    "                print(f\"  - {p}\")\n",
    "        else:\n",
    "            print(\"  (none)\")\n",
    "        print(\"\\n[DEBUG] Allowed Components:\", allowed_components if allowed_components else \"(none)\")\n",
    "\n",
    "    prompt = build_sre_prompt(\n",
    "        context=context_text,\n",
    "        pred_section=pred_text,\n",
    "        question=user_query,\n",
    "        allowed_components=allowed_components,\n",
    "    )\n",
    "\n",
    "    if verbose:\n",
    "        print(\"\\n[DEBUG] Final Prompt:\\n\")\n",
    "        print(prompt)\n",
    "\n",
    "    if llm_callable is None:\n",
    "        raise ValueError(\"llm_callable must be provided: callable(prompt, max_tokens, temperature) -> OpenAI-like dict\")\n",
    "\n",
    "    try:\n",
    "        llm_resp = llm_callable(prompt, max_tokens=1200, temperature=0.0)\n",
    "        llm_text = llm_resp[\"choices\"][0][\"text\"].strip()\n",
    "    except Exception as e:\n",
    "        llm_text = f\"LLM call failed: {e}\"\n",
    "\n",
    "    if verbose:\n",
    "        print(\"\\n[DEBUG] Answer (raw LLM output):\\n\")\n",
    "        print(llm_text)\n",
    "\n",
    "    return {\n",
    "        \"prompt\": prompt,\n",
    "        \"top_nodes\": top_items,\n",
    "        \"predictions\": predictions,\n",
    "        \"answer\": llm_text,\n",
    "    }\n",
    "\n",
    "# Example:\n",
    "# out = rag_chat_simple(user_query=\"a\", base_k=6, reranking_top_k=3, llm_callable=my_llm, verbose=True)\n",
    "# print(out[\"answer\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3ba3aff8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ggml_metal_free: deallocating\n",
      "ggml_metal_mem_pool_free: freeing memory pool, num heaps = 0 (total = 0)\n",
      "ggml_metal_mem_pool_free: freeing memory pool, num heaps = 0 (total = 0)\n",
      "ggml_metal_mem_pool_free: freeing memory pool, num heaps = 0 (total = 0)\n",
      "ggml_metal_mem_pool_free: freeing memory pool, num heaps = 0 (total = 0)\n",
      "ggml_metal_mem_pool_free: freeing memory pool, num heaps = 0 (total = 0)\n",
      "ggml_metal_mem_pool_free: freeing memory pool, num heaps = 0 (total = 0)\n",
      "ggml_metal_mem_pool_free: freeing memory pool, num heaps = 0 (total = 0)\n",
      "ggml_metal_mem_pool_free: freeing memory pool, num heaps = 0 (total = 0)\n",
      "llama_model_load_from_file_impl: using device Metal (Apple M3) - 11093 MiB free\n",
      "llama_model_loader: loaded meta data with 37 key-value pairs and 291 tensors from ./models/granite-7b-lab-Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = Instructlab Granite 7b Lab\n",
      "llama_model_loader: - kv   3:                           general.finetune str              = lab\n",
      "llama_model_loader: - kv   4:                           general.basename str              = instructlab-granite\n",
      "llama_model_loader: - kv   5:                         general.size_label str              = 7B\n",
      "llama_model_loader: - kv   6:                            general.license str              = apache-2.0\n",
      "llama_model_loader: - kv   7:                   general.base_model.count u32              = 1\n",
      "llama_model_loader: - kv   8:                  general.base_model.0.name str              = Granite 7b Base\n",
      "llama_model_loader: - kv   9:          general.base_model.0.organization str              = Ibm\n",
      "llama_model_loader: - kv  10:              general.base_model.0.repo_url str              = https://huggingface.co/ibm/granite-7b...\n",
      "llama_model_loader: - kv  11:                               general.tags arr[str,6]       = [\"granite\", \"ibm\", \"lab\", \"labrador\",...\n",
      "llama_model_loader: - kv  12:                          general.languages arr[str,1]       = [\"en\"]\n",
      "llama_model_loader: - kv  13:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv  14:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv  15:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv  16:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv  17:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  18:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv  19:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  20:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  21:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  22:                           llama.vocab_size u32              = 32008\n",
      "llama_model_loader: - kv  23:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  24:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  25:                         tokenizer.ggml.pre str              = default\n",
      "llama_model_loader: - kv  26:                      tokenizer.ggml.tokens arr[str,32008]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  27:                      tokenizer.ggml.scores arr[f32,32008]   = [-1000.000000, -1000.000000, -1000.00...\n",
      "llama_model_loader: - kv  28:                  tokenizer.ggml.token_type arr[i32,32008]   = [3, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  29:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  30:                tokenizer.ggml.eos_token_id u32              = 32000\n",
      "llama_model_loader: - kv  31:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  32:            tokenizer.ggml.padding_token_id u32              = 32001\n",
      "llama_model_loader: - kv  33:               tokenizer.ggml.add_bos_token bool             = false\n",
      "llama_model_loader: - kv  34:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  35:                    tokenizer.chat_template str              = {% for message in messages %}{% if me...\n",
      "llama_model_loader: - kv  36:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "print_info: file format = GGUF V3 (latest)\n",
      "print_info: file type   = Q4_K - Medium\n",
      "print_info: file size   = 3.80 GiB (4.84 BPW) \n",
      "init_tokenizer: initializing tokenizer for type 1\n",
      "load: control token:  32004 '<|system|>' is not marked as EOG\n",
      "load: control token:  32003 '<|assistant|>' is not marked as EOG\n",
      "load: control token:  32002 '<|user|>' is not marked as EOG\n",
      "load: control token:  32001 '<|pad|>' is not marked as EOG\n",
      "load: control token:      1 '<s>' is not marked as EOG\n",
      "load: control token:      0 '<unk>' is not marked as EOG\n",
      "load: control token:      2 '</s>' is not marked as EOG\n",
      "load: printing all EOG tokens:\n",
      "load:   - 32000 ('<|endoftext|>')\n",
      "load: special tokens cache size = 8\n",
      "load: token to piece cache size = 0.1684 MB\n",
      "print_info: arch             = llama\n",
      "print_info: vocab_only       = 0\n",
      "print_info: n_ctx_train      = 4096\n",
      "print_info: n_embd           = 4096\n",
      "print_info: n_layer          = 32\n",
      "print_info: n_head           = 32\n",
      "print_info: n_head_kv        = 32\n",
      "print_info: n_rot            = 128\n",
      "print_info: n_swa            = 0\n",
      "print_info: is_swa_any       = 0\n",
      "print_info: n_embd_head_k    = 128\n",
      "print_info: n_embd_head_v    = 128\n",
      "print_info: n_gqa            = 1\n",
      "print_info: n_embd_k_gqa     = 4096\n",
      "print_info: n_embd_v_gqa     = 4096\n",
      "print_info: f_norm_eps       = 0.0e+00\n",
      "print_info: f_norm_rms_eps   = 1.0e-05\n",
      "print_info: f_clamp_kqv      = 0.0e+00\n",
      "print_info: f_max_alibi_bias = 0.0e+00\n",
      "print_info: f_logit_scale    = 0.0e+00\n",
      "print_info: f_attn_scale     = 0.0e+00\n",
      "print_info: n_ff             = 11008\n",
      "print_info: n_expert         = 0\n",
      "print_info: n_expert_used    = 0\n",
      "print_info: causal attn      = 1\n",
      "print_info: pooling type     = 0\n",
      "print_info: rope type        = 0\n",
      "print_info: rope scaling     = linear\n",
      "print_info: freq_base_train  = 10000.0\n",
      "print_info: freq_scale_train = 1\n",
      "print_info: n_ctx_orig_yarn  = 4096\n",
      "print_info: rope_finetuned   = unknown\n",
      "print_info: model type       = 7B\n",
      "print_info: model params     = 6.74 B\n",
      "print_info: general.name     = Instructlab Granite 7b Lab\n",
      "print_info: vocab type       = SPM\n",
      "print_info: n_vocab          = 32008\n",
      "print_info: n_merges         = 0\n",
      "print_info: BOS token        = 1 '<s>'\n",
      "print_info: EOS token        = 32000 '<|endoftext|>'\n",
      "print_info: EOT token        = 32000 '<|endoftext|>'\n",
      "print_info: UNK token        = 0 '<unk>'\n",
      "print_info: PAD token        = 32001 '<|pad|>'\n",
      "print_info: LF token         = 13 '<0x0A>'\n",
      "print_info: EOG token        = 32000 '<|endoftext|>'\n",
      "print_info: max token length = 48\n",
      "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
      "load_tensors: layer   0 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer   1 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer   2 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer   3 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer   4 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer   5 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer   6 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer   7 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer   8 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer   9 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  10 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  11 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  12 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  13 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  14 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  15 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  16 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  17 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  18 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  19 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  20 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  21 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  22 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  23 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  24 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  25 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  26 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  27 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  28 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  29 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  30 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  31 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  32 assigned to device Metal, is_swa = 0\n",
      "load_tensors: tensor 'token_embd.weight' (q4_K) (and 0 others) cannot be used with preferred buffer type CPU_REPACK, using CPU instead\n",
      "ggml_backend_metal_log_allocated_size: allocated buffer, size =  3891.30 MiB, ( 9181.33 / 16384.02)\n",
      "load_tensors: offloading 32 repeating layers to GPU\n",
      "load_tensors: offloading output layer to GPU\n",
      "load_tensors: offloaded 33/33 layers to GPU\n",
      "load_tensors: Metal_Mapped model buffer size =  3891.29 MiB\n",
      "load_tensors:   CPU_Mapped model buffer size =    70.33 MiB\n",
      "..................................................................................................\n",
      "llama_context: constructing llama_context\n",
      "llama_context: n_seq_max     = 1\n",
      "llama_context: n_ctx         = 4096\n",
      "llama_context: n_ctx_per_seq = 4096\n",
      "llama_context: n_batch       = 512\n",
      "llama_context: n_ubatch      = 512\n",
      "llama_context: causal_attn   = 1\n",
      "llama_context: flash_attn    = 0\n",
      "llama_context: kv_unified    = false\n",
      "llama_context: freq_base     = 10000.0\n",
      "llama_context: freq_scale    = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M3\n",
      "ggml_metal_init: picking default device: Apple M3\n",
      "ggml_metal_init: GPU name:   Apple M3\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction   = true\n",
      "ggml_metal_init: simdgroup matrix mul. = true\n",
      "ggml_metal_init: has residency sets    = true\n",
      "ggml_metal_init: has bfloat            = true\n",
      "ggml_metal_init: use bfloat            = false\n",
      "ggml_metal_init: hasUnifiedMemory      = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 17179.89 MB\n",
      "ggml_metal_init: loaded kernel_add                                    0x169737670 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_fuse_2                             0x16748efd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_fuse_3                             0x167491300 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_fuse_4                             0x167f2bb00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_fuse_5                             0x167f2bf60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_fuse_6                             0x167f2c330 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_fuse_7                             0x1697369f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_fuse_8                             0x16748e860 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_row_c4                             0x1697359c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_row_c4_fuse_2                      0x16748cf40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_row_c4_fuse_3                      0x16748bef0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_row_c4_fuse_4                      0x16988a8f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_row_c4_fuse_5                      0x167f2c700 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_row_c4_fuse_6                      0x169734630 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_row_c4_fuse_7                      0x16988acb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_row_c4_fuse_8                      0x16988af70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sub                                    0x167f2caa0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sub_row_c4                             0x169733620 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul                                    0x169737220 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_row_c4                             0x16988b280 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_div                                    0x167f2cf00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_div_row_c4                             0x167f2d2a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_id                                 0x167f2d600 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_f32                             0x167f2d9d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_f16                             0x4a9a3e740 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_i32                             0x167f2dda0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_i16                             0x16748af10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_scale                                  0x16748a300 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_scale_4                                0x167489940 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_clamp                                  0x16988b7c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_tanh                                   0x167486250 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_relu                                   0x4a9a3ea00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sigmoid                                0x167484c20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu                                   0x167484210 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_4                                 0x167482d90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_erf                               0x4a9a3ecc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_erf_4                             0x16747c000 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_quick                             0x16988ba80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_quick_4                           0x4a9a3f050 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_silu                                   0x4a9a3f310 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_silu_4                                 0x10cf24260 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_elu                                    0x167f2e200 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_abs                                    0x167f2e5d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sgn                                    0x10ce27db0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_step                                   0x4a9a3f5d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_hardswish                              0x16747f190 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_hardsigmoid                            0x16747f630 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_exp                                    0x10ce240f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f16                           0x167f2e9a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f16_4                         0x10cf24590 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f32                           0x167f2ed00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f32_4                         0x4a9a3f890 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                          0x4a9a3fb50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x16747fe80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_f32                           0x4a9a3fe10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                           0x16747e540 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                          0x167f2f0d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                          0x16747d890 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_0                          0x4a9a400d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_1                          0x4a9a40390 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q8_0                          0x16747b3c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_mxfp4                         0x167f2f530 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                          0x4a9a40650 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                          0x4a9a40910 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                          0x16988bdd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                          0x16988c230 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                          0x1674ac830 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x10cf248f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x10cf24bb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x10cf24ef0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x16988c590 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x16988ca30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x1674b1690 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x4a87c9d10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x4a87cab70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x4a87cd2e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_i32                           0x16988cd40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_rows_f32                           0x1674af2d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_rows_f16                           0x16988d0a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)\n",
      "ggml_metal_init: loaded kernel_set_rows_q8_0                          0x16988d470 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_rows_q4_0                          0x1674a50c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_rows_q4_1                          0x4a9a40bd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_rows_q5_0                          0x16988d8d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_rows_q5_1                          0x16988dc70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_rows_iq4_nl                        0x16988e0d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rms_norm                               0x1674a7800 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rms_norm_mul                           0x16988e430 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rms_norm_mul_add                       0x4a9a40e90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_l2_norm                                0x167f2f8d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_group_norm                             0x167f2fca0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_norm                                   0x16988e7b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_ssm_conv_f32                           0x167f30070 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_ssm_scan_f32                           0x16988eb80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_ssm_scan_f32_group                     0x16988efe0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rwkv_wkv6_f32                          0x167f30440 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rwkv_wkv7_f32                          0x16988f340 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x16988f7a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f32_f32_c4                      0x4a9a41150 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x4a9a41410 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f32_c4                      0x1674a65b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x4a9a416d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x4a9a41990 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x4a9a41c50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x4a9a41f10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x16988fce0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x169890190 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x169890450 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x1674a25f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_mxfp4_f32                       0x16749be70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x169890760 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x4a9a421d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x169890bc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x167f308a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x167f30de0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x169890f90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x167f310a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x167f31440 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x167f31810 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x4a9a42490 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x4a9a42750 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x4a9a42a10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x167f31d50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x4a9a42cd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x1674a0630 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x167f32010 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x167f323d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x1698912f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x167f32770 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x1698916c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x167f32ad0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x16749c670 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x16749b080 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x169891a90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_mxfp4_f32_r1_2              0x4a9a42f90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_mxfp4_f32_r1_3              0x167f32f30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_mxfp4_f32_r1_4              0x167f331f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_mxfp4_f32_r1_5              0x4a9a43250 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x167f335c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x169891fd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x169892330 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x167498a90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x4a9a43510 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x167499430 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x1674961d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x167f33990 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x167f33d60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x167f341c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x167f34590 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x169892700 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x106c12620 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x167f349f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x167f34d50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x167f35150 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x4a9a437d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x4a9a43a90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x167f354b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x167f35880 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x4a9a43d50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x167f35ce0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x4a9a44010 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x167f36040 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x4a9a442d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x167495d50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x167f36410 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x167494790 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x4a9a44590 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x167490a20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x167f367e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x16748c1f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x4a9a44850 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x167489090 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x4a9a44b10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x4a9a44dd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x10cf25480 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_mxfp4_f32                    0x10cf257e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13ce53f70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x10ce283f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x4a9a45090 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x167487c90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x167f36e20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x10ce286b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13ce542d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x167f370e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x169892aa0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x4a9a45350 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x4a9a45610 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x4a9a458d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x4a9a45b90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x169892e70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x167f374b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x167f377c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x1698932d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x1698936a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x167f37b20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x169893a70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x167480960 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_mxfp4_f32                       0x1674ad0d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_mxfp4_f32                       0x167f38040 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x1674aded0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x167f38300 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x167f386d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x167f38aa0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x4a87c77f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x4a8725590 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x10cf25c40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x4a9a45e50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x4a9a46110 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13ce546d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x167f38e00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x10ce28970 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x1674a6fc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x4a9a463d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_map0_f16                     0x167f391d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_map1_f32                     0x169893ed0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_f32_f16                      0x10ce28c80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_f16_f16                      0x4a9a46690 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f16                     0x169894190 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f16                     0x1698947b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f16                     0x167f39710 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f16                     0x4a9a46950 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f16                     0x4a9a46c10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_mxfp4_f16                    0x167f39a40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f16                     0x16748b770 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f16                     0x169894a70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f16                     0x16748f930 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f16                     0x167f39de0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f16                     0x4a9a46ed0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f16                  0x169894da0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f16                   0x169895170 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f16                  0x4a9a47190 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f16                    0x167f3a180 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f16                    0x1698954d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f16                    0x169895870 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f16                    0x4a9a47450 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f16                   0x4a9a47710 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f16                   0x16748fbf0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_norm_f32                          0x4a9a479d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_norm_f16                          0x167486d30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_multi_f32                         0x167f3a4e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_multi_f16                         0x167f3a940 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_vision_f32                        0x167f3ad10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_vision_f16                        0x1674815d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_neox_f32                          0x167f3b0e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_neox_f16                          0x167f3b440 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_f16                             0x167f3b8a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_f32                             0x167481890 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_ext_f16                         0x4a9a47c90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_ext_f32                         0x4a879bc70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x167f3bc70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x4a9a47f50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_upscale_f32                            0x4a87c80f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pad_f32                                0x4a9a48210 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x169895db0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x167f3bfd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_arange_f32                             0x1698960b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x167f3c430 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x4a87cd700 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_leaky_relu_f32                         0x4a87c7bc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x167f3c790 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13ce54a30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x169896510 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x169896870 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x167f3cb20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h192                0x4a87c8550 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_hk192_hv128         0x4a87ce6a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x4a9a484d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_hk576_hv512         0x4a87ce960 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x167f3cfc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x4a9a48790 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x4a9a48a50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x169896bd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x169897030 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h192               0x4a87cec20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_hk192_hv128        0x1698973d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x1698977a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_hk576_hv512        0x4a87ceee0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x169897ce0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x4a87cf1a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x4a87cf560 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x4a87cf930 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x4a87cfd00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h192               0x4a9a48d10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_hk192_hv128        0x4a9a491e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x167f3d360 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_hk576_hv512        0x4a9a494a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x4a87d00d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x4a87d04a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x4a9a49760 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x4a9a49b20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x4a9a49ff0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h192               0x4a9a4a2b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_hk192_hv128        0x13ce54df0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x4a9a4a570 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_hk576_hv512        0x167f3d6c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x169897fa0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x167f3db60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x169898300 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x169898710 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x167f3e050 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h192               0x169898ab0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_hk192_hv128        0x169898f10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x1698992e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_hk576_hv512        0x167f3e310 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13ce55100 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x1698996b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x106c12aa0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13ce554d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13ce55b10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h192               0x10cf260a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_hk192_hv128        0x10cf26590 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x4a87d0870 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_hk576_hv512        0x167f3e620 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h64             0x4a87d0bd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h64            0x169899a10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h64            0x4a9a4a930 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h64            0x167f3e9e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h64            0x169899de0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h64            0x16989a240 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h96             0x4a9a4ac40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h96            0x4a9a4afc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h96            0x4a9a4b3b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h96            0x4a9a4b930 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h96            0x4a9a4bbf0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h96            0x167f3ef20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x167f3f1e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x16989a610 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x167f3f510 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x4a9a4bf00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x16989a9e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x4a9a4c2d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h192            0x4a9a4c6b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h192           0x16989adb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h192           0x16989b180 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h192           0x167f3f8e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h192           0x16989b5e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h192           0x16989b940 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_hk192_hv128      0x16989bce0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_hk192_hv128      0x4a87d1030 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_hk192_hv128      0x4a87d13e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_hk192_hv128      0x4a87d17b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_hk192_hv128      0x167f3fc40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_hk192_hv128      0x4a9a4cb50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x4a9a4d080 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x4a9a4d340 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x4a87d1b10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x4a9a4d670 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x16989c040 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x167f40010 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_hk576_hv512      0x167f40470 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_hk576_hv512      0x167f409b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_hk576_hv512      0x16989c4a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_hk576_hv512      0x16989c870 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_hk576_hv512      0x167f40d30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_hk576_hv512      0x167f40ff0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_f32                                0x4a87d1ee0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_i32                                0x4a87d2340 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                            0x16989cbd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                            0x4a9a4d9d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f32                            0x4a9a4de00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                            0x16989cfa0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x4a9a4e4e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x4a87d2710 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x16989d4e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x4a9a4e8f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x4a87d2ae0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x10cf26850 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x16989d7a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x16989dbb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x16989df10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x4a9a4ebb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x16989e3b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x16989e700 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x4a87d2eb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x16989ea60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x10cf26b10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x167f413c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_concat                                 0x4a87d3280 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sqr                                    0x16989ee20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sqrt                                   0x167f41720 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sin                                    0x4a87d3650 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cos                                    0x4a87d3ab0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_neg                                    0x167f41af0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_reglu                                  0x4a9a4ee70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_geglu                                  0x106c12e40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_swiglu                                 0x16989f230 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_swiglu_oai                             0x16989f590 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_geglu_erf                              0x4a87d3f10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_geglu_quick                            0x16989f960 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sum_rows                               0x4a9a4f180 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mean                                   0x4a87d4290 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_argmax                                 0x4a9a4f7f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x4a9a4fca0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x4a87d4660 | th_max = 1024 | th_width =   32\n",
      "set_abort_callback: call\n",
      "llama_context:        CPU  output buffer size =     0.12 MiB\n",
      "create_memory: n_ctx = 4096 (padded)\n",
      "llama_kv_cache_unified: layer   0: dev = Metal\n",
      "llama_kv_cache_unified: layer   1: dev = Metal\n",
      "llama_kv_cache_unified: layer   2: dev = Metal\n",
      "llama_kv_cache_unified: layer   3: dev = Metal\n",
      "llama_kv_cache_unified: layer   4: dev = Metal\n",
      "llama_kv_cache_unified: layer   5: dev = Metal\n",
      "llama_kv_cache_unified: layer   6: dev = Metal\n",
      "llama_kv_cache_unified: layer   7: dev = Metal\n",
      "llama_kv_cache_unified: layer   8: dev = Metal\n",
      "llama_kv_cache_unified: layer   9: dev = Metal\n",
      "llama_kv_cache_unified: layer  10: dev = Metal\n",
      "llama_kv_cache_unified: layer  11: dev = Metal\n",
      "llama_kv_cache_unified: layer  12: dev = Metal\n",
      "llama_kv_cache_unified: layer  13: dev = Metal\n",
      "llama_kv_cache_unified: layer  14: dev = Metal\n",
      "llama_kv_cache_unified: layer  15: dev = Metal\n",
      "llama_kv_cache_unified: layer  16: dev = Metal\n",
      "llama_kv_cache_unified: layer  17: dev = Metal\n",
      "llama_kv_cache_unified: layer  18: dev = Metal\n",
      "llama_kv_cache_unified: layer  19: dev = Metal\n",
      "llama_kv_cache_unified: layer  20: dev = Metal\n",
      "llama_kv_cache_unified: layer  21: dev = Metal\n",
      "llama_kv_cache_unified: layer  22: dev = Metal\n",
      "llama_kv_cache_unified: layer  23: dev = Metal\n",
      "llama_kv_cache_unified: layer  24: dev = Metal\n",
      "llama_kv_cache_unified: layer  25: dev = Metal\n",
      "llama_kv_cache_unified: layer  26: dev = Metal\n",
      "llama_kv_cache_unified: layer  27: dev = Metal\n",
      "llama_kv_cache_unified: layer  28: dev = Metal\n",
      "llama_kv_cache_unified: layer  29: dev = Metal\n",
      "llama_kv_cache_unified: layer  30: dev = Metal\n",
      "llama_kv_cache_unified: layer  31: dev = Metal\n",
      "llama_kv_cache_unified:      Metal KV buffer size =  2048.00 MiB\n",
      "llama_kv_cache_unified: size = 2048.00 MiB (  4096 cells,  32 layers,  1/1 seqs), K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\n",
      "llama_context: enumerating backends\n",
      "llama_context: backend_ptrs.size() = 3\n",
      "llama_context: max_nodes = 2328\n",
      "llama_context: worst-case: n_tokens = 512, n_seqs = 1, n_outputs = 0\n",
      "graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  1, n_outputs =  512\n",
      "graph_reserve: reserving a graph for ubatch with n_tokens =    1, n_seqs =  1, n_outputs =    1\n",
      "graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  1, n_outputs =  512\n",
      "llama_context:      Metal compute buffer size =   312.01 MiB\n",
      "llama_context:        CPU compute buffer size =    32.01 MiB\n",
      "llama_context: graph nodes  = 1126\n",
      "llama_context: graph splits = 2\n",
      "Metal : EMBED_LIBRARY = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | REPACK = 1 | \n",
      "Model metadata: {'tokenizer.chat_template': \"{% for message in messages %}{% if message['role'] == 'system' %}{{'<|system|>'+ '\\n' + message['content'] + '\\n'}}{% elif message['role'] == 'user' %}{{'<|user|>' + '\\n' + message['content'] + '\\n'}}{% elif message['role'] == 'assistant' %}{{'<|assistant|>' + '\\n' + message['content'] + '<|endoftext|>' + ('' if loop.last else '\\n')}}{% endif %}{% endfor %}\", 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.add_bos_token': 'false', 'tokenizer.ggml.padding_token_id': '32001', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '32000', 'tokenizer.ggml.pre': 'default', 'llama.embedding_length': '4096', 'llama.vocab_size': '32008', 'llama.attention.head_count_kv': '32', 'general.size_label': '7B', 'general.base_model.0.name': 'Granite 7b Base', 'llama.block_count': '32', 'general.base_model.0.organization': 'Ibm', 'general.base_model.count': '1', 'general.quantization_version': '2', 'llama.rope.dimension_count': '128', 'general.license': 'apache-2.0', 'general.base_model.0.repo_url': 'https://huggingface.co/ibm/granite-7b-base', 'llama.attention.head_count': '32', 'llama.context_length': '4096', 'general.file_type': '15', 'general.finetune': 'lab', 'llama.feed_forward_length': '11008', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'general.architecture': 'llama', 'general.basename': 'instructlab-granite', 'llama.rope.freq_base': '10000.000000', 'tokenizer.ggml.bos_token_id': '1', 'general.type': 'model', 'tokenizer.ggml.model': 'llama', 'general.name': 'Instructlab Granite 7b Lab'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {% for message in messages %}{% if message['role'] == 'system' %}{{'<|system|>'+ '\n",
      "' + message['content'] + '\n",
      "'}}{% elif message['role'] == 'user' %}{{'<|user|>' + '\n",
      "' + message['content'] + '\n",
      "'}}{% elif message['role'] == 'assistant' %}{{'<|assistant|>' + '\n",
      "' + message['content'] + '<|endoftext|>' + ('' if loop.last else '\n",
      "')}}{% endif %}{% endfor %}\n",
      "Using chat eos_token: <|endoftext|>\n",
      "Using chat bos_token: <s>\n"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "llm.close()\n",
    "llm = Llama(\n",
    "    model_path=\"./models/granite-7b-lab-Q4_K_M.gguf\",\n",
    "    n_ctx=4096,\n",
    "    n_threads=8,     # Adjust to your CPU cores\n",
    "    n_gpu_layers=50  # If GPU available\n",
    ")\n",
    "llm.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1e8b7efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strict prompt + optional stop sequences, no post-processing\n",
    "from typing import Any, Dict, Iterable, List, Tuple, Optional\n",
    "import re\n",
    "import difflib\n",
    "\n",
    "\n",
    "def rag_chat_simple(\n",
    "    user_query: str,\n",
    "    base_k: int = 4,\n",
    "    reranking_top_k: int = 3,\n",
    "    llm_callable=None,\n",
    "    verbose: bool = True,\n",
    "    max_tokens: int = 500,\n",
    "    temperature: float = 0.0,\n",
    "    max_sequence_chars: int = 500,\n",
    "    max_predictions: int = 3,\n",
    "    allow_query_engine: bool = False,\n",
    "    stop_sequences: Optional[List[str]] = None,\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"RAG chat with strict BEGIN/END prompt and optional stop sequences.\n",
    "\n",
    "    - No post-processing. Uses prompt constraints and stop tokens to avoid duplicates.\n",
    "    - Keeps debug prints for verification.\n",
    "    \"\"\"\n",
    "\n",
    "    # --- helpers ---\n",
    "    seq_pat = re.compile(r\"<SEQUENCE>\\s*(.*?)\\s*</SEQUENCE>\", re.DOTALL | re.IGNORECASE)\n",
    "\n",
    "    def normalize_text(value: Optional[str]) -> str:\n",
    "        if not value:\n",
    "            return \"\"\n",
    "        return \" \".join(value.strip().strip('\"').strip(\"'\").lower().split())\n",
    "\n",
    "    def fuzzy_ratio(a: str, b: str) -> float:\n",
    "        return difflib.SequenceMatcher(None, a, b).ratio()\n",
    "\n",
    "    def get_node_text(node: Any) -> Optional[str]:\n",
    "        try:\n",
    "            if hasattr(node, \"text\") and isinstance(getattr(node, \"text\"), str):\n",
    "                return getattr(node, \"text\")\n",
    "            if hasattr(node, \"get_content\") and callable(getattr(node, \"get_content\")):\n",
    "                return node.get_content()\n",
    "            inner = getattr(node, \"node\", None)\n",
    "            if inner is not None:\n",
    "                if hasattr(inner, \"text\") and isinstance(getattr(inner, \"text\"), str):\n",
    "                    return getattr(inner, \"text\")\n",
    "                if hasattr(inner, \"get_content\") and callable(getattr(inner, \"get_content\")):\n",
    "                    return inner.get_content()\n",
    "        except Exception:\n",
    "            return None\n",
    "        return None\n",
    "\n",
    "    def get_node_metadata(node: Any) -> Dict[str, Any]:\n",
    "        try:\n",
    "            if hasattr(node, \"metadata\") and isinstance(getattr(node, \"metadata\"), dict):\n",
    "                return dict(getattr(node, \"metadata\"))\n",
    "            inner = getattr(node, \"node\", None)\n",
    "            if inner is not None and hasattr(inner, \"metadata\"):\n",
    "                try:\n",
    "                    return dict(getattr(inner, \"metadata\"))\n",
    "                except Exception:\n",
    "                    return {}\n",
    "        except Exception:\n",
    "            return {}\n",
    "        return {}\n",
    "\n",
    "    def extract_sequence_block(text: Optional[str]) -> Optional[str]:\n",
    "        if not text:\n",
    "            return None\n",
    "        m = seq_pat.search(text)\n",
    "        return m.group(1).strip() if m else None\n",
    "\n",
    "    def parse_sequence_steps(sequence_block: Optional[str]) -> List[str]:\n",
    "        if not sequence_block:\n",
    "            return []\n",
    "        parts = [p.strip().strip('\"').strip(\"'\") for p in sequence_block.split(\"->\")]\n",
    "        return [p for p in parts if p]\n",
    "\n",
    "    def best_position_for_query(steps: List[str], q: str) -> Tuple[int, bool, float, str]:\n",
    "        if not steps:\n",
    "            return 1_000_000, False, 0.0, \"\"\n",
    "        qn = normalize_text(q)\n",
    "        for i, step in enumerate(steps):\n",
    "            if normalize_text(step) == qn:\n",
    "                return i + 1, True, 1.0, step\n",
    "        best_i, best_sim, best_step = 0, -1.0, steps[0]\n",
    "        for i, step in enumerate(steps):\n",
    "            sim = fuzzy_ratio(normalize_text(step), qn)\n",
    "            if sim > best_sim:\n",
    "                best_i, best_sim, best_step = i, sim, step\n",
    "        return best_i + 1, False, best_sim, best_step\n",
    "\n",
    "    def rerank_nodes_by_sequence_query(nodes: Iterable[Any], q: str, top_k: int) -> List[Dict[str, Any]]:\n",
    "        scored: List[Dict[str, Any]] = []\n",
    "        for node in nodes:\n",
    "            text = get_node_text(node)\n",
    "            seq_block = extract_sequence_block(text)\n",
    "            if not seq_block:\n",
    "                continue\n",
    "            steps = parse_sequence_steps(seq_block)\n",
    "            pos, is_exact, sim, matched = best_position_for_query(steps, q)\n",
    "            scored.append({\n",
    "                \"node\": node,\n",
    "                \"position\": pos,\n",
    "                \"is_exact\": is_exact,\n",
    "                \"similarity\": sim,\n",
    "                \"matched_step\": matched,\n",
    "                \"steps\": steps,\n",
    "                \"sequence_text\": seq_block,\n",
    "                \"metadata\": get_node_metadata(node),\n",
    "            })\n",
    "        scored.sort(key=lambda d: (d[\"position\"], -d[\"similarity\"]))\n",
    "        return scored[:top_k]\n",
    "\n",
    "    def predict_next_steps_from_top_nodes(top_items: List[Dict[str, Any]]) -> List[str]:\n",
    "        seen = set()\n",
    "        predictions: List[str] = []\n",
    "        for it in top_items:\n",
    "            steps = it.get(\"steps\", [])\n",
    "            pos = max(1, int(it.get(\"position\", 1)))\n",
    "            tail = steps[pos - 1 if pos >= len(steps) else pos:]\n",
    "            if not tail and steps:\n",
    "                tail = [steps[-1]]\n",
    "            if not tail:\n",
    "                continue\n",
    "            seq = \" -> \".join(tail)\n",
    "            if seq not in seen:\n",
    "                seen.add(seq)\n",
    "                predictions.append(seq)\n",
    "        return predictions[:max_predictions]\n",
    "\n",
    "    def build_allowed_components(top_items: List[Dict[str, Any]]) -> List[str]:\n",
    "        comps: List[str] = []\n",
    "        seen_c = set()\n",
    "        for it in top_items:\n",
    "            meta = it.get(\"metadata\", {}) or {}\n",
    "            comp = meta.get(\"component\")\n",
    "            if comp is not None:\n",
    "                comp_s = str(comp)\n",
    "                if comp_s not in seen_c:\n",
    "                    seen_c.add(comp_s)\n",
    "                    comps.append(comp_s)\n",
    "        return comps\n",
    "\n",
    "    def truncate_sequence_text(seq_text: str) -> str:\n",
    "        if seq_text and len(seq_text) > max_sequence_chars:\n",
    "            return seq_text[:max_sequence_chars] + \" ...\"\n",
    "        return seq_text\n",
    "\n",
    "    def format_context_from_top_nodes(top_items: List[Dict[str, Any]]) -> str:\n",
    "        blocks = []\n",
    "        for idx, it in enumerate(top_items, start=1):\n",
    "            meta = it.get(\"metadata\", {})\n",
    "            meta_str = \", \".join(f\"{k}: {v}\" for k, v in meta.items()) if isinstance(meta, dict) else str(meta)\n",
    "            seq_text = truncate_sequence_text(it.get('sequence_text', ''))\n",
    "            block = (\n",
    "                f\"[CHUNK {idx}]\\n\"\n",
    "                f\"METADATA: {meta_str}\\n\"\n",
    "                f\"SEQUENCE: <SEQUENCE> {seq_text} </SEQUENCE>\\n\"\n",
    "            )\n",
    "            blocks.append(block)\n",
    "        return \"\\n\".join(blocks)\n",
    "\n",
    "    def format_pred_section(pred_steps: List[str]) -> str:\n",
    "        if not pred_steps:\n",
    "            return \"\"\n",
    "        return \"\\n\".join(f\"- {seq}\" for seq in pred_steps)\n",
    "\n",
    "    def build_sre_prompt(context: str, pred_section: str, question: str, allowed_components: List[str]) -> str:\n",
    "        allowed_comps_str = \", \".join(allowed_components) if allowed_components else \"(none)\"\n",
    "        return f\"\"\"\n",
    "You are an expert SRE whose job is to debug MaC logs. Use ONLY the provided context chunks and the Predicted Next Logs below (do not hallucinate).\n",
    "\n",
    "CONTEXT CHUNKS\n",
    "{context}\n",
    "\n",
    "PREDICTED NEXT LOGS (if any)\n",
    "{pred_section}\n",
    "\n",
    "INSTRUCTIONS\n",
    "- If Predicted Next Logs is non-empty, base NEXT-PREDICTIONS primarily on these items.\n",
    "- If it is empty, use <SEQUENCE> content in the context to reason about next predictions. Do NOT mention that predictions were empty.\n",
    "- Keep the answer concise, structured, and grounded strictly in the provided data.\n",
    "\n",
    "QUESTION\n",
    "{question}\n",
    "\n",
    "BEGIN OUTPUT\n",
    "Return EXACTLY these sections with these headings and nothing else. Do not add any extra lines before or after section 6. Do NOT repeat content.\n",
    "1) RCA: 1-2 sentences describing the root cause\n",
    "2) COMPONENTS: component(s) responsible (choose only from: {allowed_comps_str}; if none apply, write 'UNKNOWN')\n",
    "3) SUGGESTED FIXES: exactly 3 concrete steps (imperative voice)\n",
    "4) NEXT-PREDICTIONS: 2-4 likely next events with rationale; use Predicted Next Logs when available, else <SEQUENCE>\n",
    "5) SUMMARY: 1-2 sentences summarizing RCA, component, and next actions\n",
    "6) CONFIDENCE: a single integer 0-100 (no % sign, no extra words)\n",
    "END OUTPUT\n",
    "\"\"\".strip()\n",
    "\n",
    "    # --- retrieval (no query_engine unless allowed) ---\n",
    "    nodes = None\n",
    "    try:\n",
    "        global retriever  # type: ignore[name-defined]\n",
    "        if 'retriever' in globals():\n",
    "            r = globals()['retriever']\n",
    "            if hasattr(r, 'retrieve') and callable(getattr(r, 'retrieve')):\n",
    "                res = r.retrieve(user_query)\n",
    "                nodes = list(res)[:base_k]\n",
    "            elif callable(r):\n",
    "                try:\n",
    "                    nodes = list(r(user_query, base_k))\n",
    "                except TypeError:\n",
    "                    nodes = list(r(user_query))[:base_k]\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    if nodes is None:\n",
    "        try:\n",
    "            if 'index' in globals():\n",
    "                idx = globals()['index']\n",
    "                if hasattr(idx, 'as_retriever') and callable(getattr(idx, 'as_retriever')):\n",
    "                    r = idx.as_retriever(similarity_top_k=base_k)\n",
    "                    res = r.retrieve(user_query)\n",
    "                    nodes = list(res)[:base_k]\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    if nodes is None and allow_query_engine:\n",
    "        try:\n",
    "            if 'query_engine' in globals():\n",
    "                qe = globals()['query_engine']\n",
    "                if hasattr(qe, 'query') and callable(getattr(qe, 'query')):\n",
    "                    resp = qe.query(user_query)\n",
    "                    src = getattr(resp, 'source_nodes', None)\n",
    "                    if src is not None:\n",
    "                        nodes = list(src)[:base_k]\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    if nodes is None:\n",
    "        raise RuntimeError(\"No retriever/index found (or query_engine allowed). Define global `retriever` or `index`.\")\n",
    "\n",
    "    # --- rerank, predict, prompt, llm ---\n",
    "    top_items = rerank_nodes_by_sequence_query(nodes, q=user_query, top_k=reranking_top_k)\n",
    "    predictions = predict_next_steps_from_top_nodes(top_items)\n",
    "    context_text = format_context_from_top_nodes(top_items)\n",
    "    pred_text = format_pred_section(predictions)\n",
    "    allowed_components = build_allowed_components(top_items)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"[DEBUG] Top reranked chunks (earliest position first; ties by similarity):\")\n",
    "        for i, it in enumerate(top_items, start=1):\n",
    "            print(f\"  - Rank {i}: position={it['position']}, is_exact={it['is_exact']}, similarity={it['similarity']:.3f}\")\n",
    "            print(f\"    matched_step: {it['matched_step']}\")\n",
    "            print(f\"    metadata: {it.get('metadata', {})}\")\n",
    "            print(f\"    sequence: <SEQUENCE> {it.get('sequence_text','')[:120]}{'...' if len(it.get('sequence_text',''))>120 else ''} </SEQUENCE>\")\n",
    "        print(\"\\n[DEBUG] Predicted Next Logs:\")\n",
    "        if predictions:\n",
    "            for p in predictions:\n",
    "                print(f\"  - {p}\")\n",
    "        else:\n",
    "            print(\"  (none)\")\n",
    "        print(\"\\n[DEBUG] Allowed Components:\", allowed_components if allowed_components else \"(none)\")\n",
    "\n",
    "    prompt = build_sre_prompt(\n",
    "        context=context_text,\n",
    "        pred_section=pred_text,\n",
    "        question=user_query,\n",
    "        allowed_components=allowed_components,\n",
    "    )\n",
    "\n",
    "    if verbose:\n",
    "        print(\"\\n[DEBUG] Final Prompt:\\n\")\n",
    "        print(prompt)\n",
    "\n",
    "    if llm_callable is None:\n",
    "        raise ValueError(\"llm_callable must be provided: callable(prompt, max_tokens, temperature, [stop]) -> OpenAI-like dict\")\n",
    "\n",
    "    # default stop sequences aim to prevent trailing duplication\n",
    "    if stop_sequences is None:\n",
    "        stop_sequences = [\"END OUTPUT\", \"\\n7)\", \"\\nConfidence:\", \"LLM Response\", \"[DEBUG]\"]\n",
    "\n",
    "    try:\n",
    "        # Try passing stop tokens if supported\n",
    "        llm_resp = None\n",
    "        try:\n",
    "            llm_resp = llm_callable(prompt, max_tokens=max_tokens, temperature=temperature, stop=stop_sequences)\n",
    "        except TypeError:\n",
    "            llm_resp = llm_callable(prompt, max_tokens=max_tokens, temperature=temperature)\n",
    "        llm_text = llm_resp[\"choices\"][0][\"text\"].strip()\n",
    "    except Exception as e:\n",
    "        llm_text = f\"LLM call failed: {e}\"\n",
    "\n",
    "    if verbose:\n",
    "        print(\"\\n[DEBUG] Answer (raw LLM output):\\n\")\n",
    "        print(llm_text)\n",
    "\n",
    "    return {\n",
    "        \"prompt\": prompt,\n",
    "        \"top_nodes\": top_items,\n",
    "        \"predictions\": predictions,\n",
    "        \"answer\": llm_text,\n",
    "    }\n",
    "\n",
    "# Example\n",
    "# out = rag_chat_simple(user_query=\"a\", base_k=4, reranking_top_k=3, llm_callable=my_llm, verbose=True)\n",
    "# print(out[\"answer\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "525cabaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ggml_metal_free: deallocating\n",
      "ggml_metal_mem_pool_free: freeing memory pool, num heaps = 0 (total = 0)\n",
      "ggml_metal_mem_pool_free: freeing memory pool, num heaps = 0 (total = 0)\n",
      "ggml_metal_mem_pool_free: freeing memory pool, num heaps = 0 (total = 0)\n",
      "ggml_metal_mem_pool_free: freeing memory pool, num heaps = 0 (total = 0)\n",
      "ggml_metal_mem_pool_free: freeing memory pool, num heaps = 0 (total = 0)\n",
      "ggml_metal_mem_pool_free: freeing memory pool, num heaps = 0 (total = 0)\n",
      "ggml_metal_mem_pool_free: freeing memory pool, num heaps = 0 (total = 0)\n",
      "ggml_metal_mem_pool_free: freeing memory pool, num heaps = 0 (total = 0)\n",
      "llama_model_load_from_file_impl: using device Metal (Apple M3) - 12381 MiB free\n",
      "llama_model_loader: loaded meta data with 37 key-value pairs and 291 tensors from ./models/granite-7b-lab-Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = Instructlab Granite 7b Lab\n",
      "llama_model_loader: - kv   3:                           general.finetune str              = lab\n",
      "llama_model_loader: - kv   4:                           general.basename str              = instructlab-granite\n",
      "llama_model_loader: - kv   5:                         general.size_label str              = 7B\n",
      "llama_model_loader: - kv   6:                            general.license str              = apache-2.0\n",
      "llama_model_loader: - kv   7:                   general.base_model.count u32              = 1\n",
      "llama_model_loader: - kv   8:                  general.base_model.0.name str              = Granite 7b Base\n",
      "llama_model_loader: - kv   9:          general.base_model.0.organization str              = Ibm\n",
      "llama_model_loader: - kv  10:              general.base_model.0.repo_url str              = https://huggingface.co/ibm/granite-7b...\n",
      "llama_model_loader: - kv  11:                               general.tags arr[str,6]       = [\"granite\", \"ibm\", \"lab\", \"labrador\",...\n",
      "llama_model_loader: - kv  12:                          general.languages arr[str,1]       = [\"en\"]\n",
      "llama_model_loader: - kv  13:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv  14:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv  15:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv  16:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv  17:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  18:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv  19:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  20:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  21:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  22:                           llama.vocab_size u32              = 32008\n",
      "llama_model_loader: - kv  23:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  24:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  25:                         tokenizer.ggml.pre str              = default\n",
      "llama_model_loader: - kv  26:                      tokenizer.ggml.tokens arr[str,32008]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  27:                      tokenizer.ggml.scores arr[f32,32008]   = [-1000.000000, -1000.000000, -1000.00...\n",
      "llama_model_loader: - kv  28:                  tokenizer.ggml.token_type arr[i32,32008]   = [3, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  29:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  30:                tokenizer.ggml.eos_token_id u32              = 32000\n",
      "llama_model_loader: - kv  31:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  32:            tokenizer.ggml.padding_token_id u32              = 32001\n",
      "llama_model_loader: - kv  33:               tokenizer.ggml.add_bos_token bool             = false\n",
      "llama_model_loader: - kv  34:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  35:                    tokenizer.chat_template str              = {% for message in messages %}{% if me...\n",
      "llama_model_loader: - kv  36:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "print_info: file format = GGUF V3 (latest)\n",
      "print_info: file type   = Q4_K - Medium\n",
      "print_info: file size   = 3.80 GiB (4.84 BPW) \n",
      "init_tokenizer: initializing tokenizer for type 1\n",
      "load: control token:  32004 '<|system|>' is not marked as EOG\n",
      "load: control token:  32003 '<|assistant|>' is not marked as EOG\n",
      "load: control token:  32002 '<|user|>' is not marked as EOG\n",
      "load: control token:  32001 '<|pad|>' is not marked as EOG\n",
      "load: control token:      1 '<s>' is not marked as EOG\n",
      "load: control token:      0 '<unk>' is not marked as EOG\n",
      "load: control token:      2 '</s>' is not marked as EOG\n",
      "load: printing all EOG tokens:\n",
      "load:   - 32000 ('<|endoftext|>')\n",
      "load: special tokens cache size = 8\n",
      "load: token to piece cache size = 0.1684 MB\n",
      "print_info: arch             = llama\n",
      "print_info: vocab_only       = 0\n",
      "print_info: n_ctx_train      = 4096\n",
      "print_info: n_embd           = 4096\n",
      "print_info: n_layer          = 32\n",
      "print_info: n_head           = 32\n",
      "print_info: n_head_kv        = 32\n",
      "print_info: n_rot            = 128\n",
      "print_info: n_swa            = 0\n",
      "print_info: is_swa_any       = 0\n",
      "print_info: n_embd_head_k    = 128\n",
      "print_info: n_embd_head_v    = 128\n",
      "print_info: n_gqa            = 1\n",
      "print_info: n_embd_k_gqa     = 4096\n",
      "print_info: n_embd_v_gqa     = 4096\n",
      "print_info: f_norm_eps       = 0.0e+00\n",
      "print_info: f_norm_rms_eps   = 1.0e-05\n",
      "print_info: f_clamp_kqv      = 0.0e+00\n",
      "print_info: f_max_alibi_bias = 0.0e+00\n",
      "print_info: f_logit_scale    = 0.0e+00\n",
      "print_info: f_attn_scale     = 0.0e+00\n",
      "print_info: n_ff             = 11008\n",
      "print_info: n_expert         = 0\n",
      "print_info: n_expert_used    = 0\n",
      "print_info: causal attn      = 1\n",
      "print_info: pooling type     = 0\n",
      "print_info: rope type        = 0\n",
      "print_info: rope scaling     = linear\n",
      "print_info: freq_base_train  = 10000.0\n",
      "print_info: freq_scale_train = 1\n",
      "print_info: n_ctx_orig_yarn  = 4096\n",
      "print_info: rope_finetuned   = unknown\n",
      "print_info: model type       = 7B\n",
      "print_info: model params     = 6.74 B\n",
      "print_info: general.name     = Instructlab Granite 7b Lab\n",
      "print_info: vocab type       = SPM\n",
      "print_info: n_vocab          = 32008\n",
      "print_info: n_merges         = 0\n",
      "print_info: BOS token        = 1 '<s>'\n",
      "print_info: EOS token        = 32000 '<|endoftext|>'\n",
      "print_info: EOT token        = 32000 '<|endoftext|>'\n",
      "print_info: UNK token        = 0 '<unk>'\n",
      "print_info: PAD token        = 32001 '<|pad|>'\n",
      "print_info: LF token         = 13 '<0x0A>'\n",
      "print_info: EOG token        = 32000 '<|endoftext|>'\n",
      "print_info: max token length = 48\n",
      "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
      "load_tensors: layer   0 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer   1 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer   2 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer   3 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer   4 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer   5 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer   6 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer   7 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer   8 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer   9 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  10 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  11 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  12 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  13 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  14 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  15 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  16 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  17 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  18 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  19 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  20 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  21 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  22 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  23 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  24 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  25 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  26 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  27 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  28 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  29 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  30 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  31 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  32 assigned to device Metal, is_swa = 0\n",
      "load_tensors: tensor 'token_embd.weight' (q4_K) (and 0 others) cannot be used with preferred buffer type CPU_REPACK, using CPU instead\n",
      "ggml_backend_metal_log_allocated_size: allocated buffer, size =  3891.30 MiB, ( 7893.33 / 16384.02)\n",
      "load_tensors: offloading 32 repeating layers to GPU\n",
      "load_tensors: offloading output layer to GPU\n",
      "load_tensors: offloaded 33/33 layers to GPU\n",
      "load_tensors: Metal_Mapped model buffer size =  3891.29 MiB\n",
      "load_tensors:   CPU_Mapped model buffer size =    70.33 MiB\n",
      "..................................................................................................\n",
      "llama_context: constructing llama_context\n",
      "llama_context: n_seq_max     = 1\n",
      "llama_context: n_ctx         = 4096\n",
      "llama_context: n_ctx_per_seq = 4096\n",
      "llama_context: n_batch       = 512\n",
      "llama_context: n_ubatch      = 512\n",
      "llama_context: causal_attn   = 1\n",
      "llama_context: flash_attn    = 0\n",
      "llama_context: kv_unified    = false\n",
      "llama_context: freq_base     = 10000.0\n",
      "llama_context: freq_scale    = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M3\n",
      "ggml_metal_init: picking default device: Apple M3\n",
      "ggml_metal_init: GPU name:   Apple M3\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction   = true\n",
      "ggml_metal_init: simdgroup matrix mul. = true\n",
      "ggml_metal_init: has residency sets    = true\n",
      "ggml_metal_init: has bfloat            = true\n",
      "ggml_metal_init: use bfloat            = false\n",
      "ggml_metal_init: hasUnifiedMemory      = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 17179.89 MB\n",
      "ggml_metal_init: loaded kernel_add                                    0x13cec75c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_fuse_2                             0x13cec7a20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_fuse_3                             0x10cea5440 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_fuse_4                             0x13cec7e80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_fuse_5                             0x10cea5700 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_fuse_6                             0x10cea5ac0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_fuse_7                             0x106c13860 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_fuse_8                             0x13cec8190 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_row_c4                             0x10cea5dd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_row_c4_fuse_2                      0x13cec8590 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_row_c4_fuse_3                      0x10cea6230 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_row_c4_fuse_4                      0x106c13b20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_row_c4_fuse_5                      0x13cec88f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_row_c4_fuse_6                      0x106c13e70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_row_c4_fuse_7                      0x10cea6600 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_row_c4_fuse_8                      0x10cea69d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sub                                    0x10cea6d30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sub_row_c4                             0x13cec8d50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul                                    0x10cea72d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_row_c4                             0x10cea7590 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_div                                    0x10cea78c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_div_row_c4                             0x106c143b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_id                                 0x10cea7c90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_f32                             0x106c146b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_f16                             0x10cea8060 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_i32                             0x106c14a80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_i16                             0x10cea83c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_scale                                  0x10cea8820 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_scale_4                                0x10cf2d3b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_clamp                                  0x4a84cae40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_tanh                                   0x10cea8bf0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_relu                                   0x10cf2d710 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sigmoid                                0x13cec90b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu                                   0x13cec9550 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_4                                 0x4a8282cd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_erf                               0x10cea8f50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_erf_4                             0x4a8282f90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_quick                             0x10cea9320 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_quick_4                           0x4a82832f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_silu                                   0x4a82836f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_silu_4                                 0x106c14de0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_elu                                    0x10cea9780 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_abs                                    0x4a8283a50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sgn                                    0x13cec98b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_step                                   0x10cea9ae0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_hardswish                              0x10cea9f40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_hardsigmoid                            0x13cec9c80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_exp                                    0x13ceca050 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f16                           0x106c151b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f16_4                         0x10ceaa3a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f32                           0x10ceaa6a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f32_4                         0x10ceaaa40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                          0x10ceaaea0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x106c15580 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_f32                           0x13ceca420 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                           0x10ceab270 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                          0x4a8283e20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                          0x10ceab5d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_0                          0x106c15950 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_1                          0x10ceab9a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q8_0                          0x4a82841f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_mxfp4                         0x10ceabd70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                          0x106c15d20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                          0x4a8284650 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13ceca7f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                          0x106c161f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13cecabc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13cecb020 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13cecb560 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x4a8284b90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13cecb820 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x4a8284e50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13cecbb50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x4a8285180 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13cecbf20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x4a8285550 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_i32                           0x4a82858b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_rows_f32                           0x13cecc3f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_rows_f16                           0x13cecc7c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)\n",
      "ggml_metal_init: loaded kernel_set_rows_q8_0                          0x4a84cb290 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_rows_q4_0                          0x4a8285d50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_rows_q4_1                          0x4a82860b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_rows_q5_0                          0x4a82865f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_rows_q5_1                          0x13ceccc20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_rows_iq4_nl                        0x106c16550 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rms_norm                               0x106c168a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rms_norm_mul                           0x106c16d00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rms_norm_mul_add                       0x106c17160 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_l2_norm                                0x10ceac140 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_group_norm                             0x13cecd030 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_norm                                   0x106c174c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_ssm_conv_f32                           0x106c178d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13cecd390 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_ssm_scan_f32_group                     0x13cecd7f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rwkv_wkv6_f32                          0x13cecdc50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rwkv_wkv7_f32                          0x10ceac510 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x10ceac8e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f32_f32_c4                      0x106c17c70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x10ceacd40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f32_c4                      0x4a82868b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x106c17fd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x10cead0a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x10cead500 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x106c18370 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13cecdf80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13cece350 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x4a8286bb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x106c18740 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_mxfp4_f32                       0x106c18ba0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x4a8286f80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x10cead860 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x106c12aa0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x10ceadcc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x106c12d60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x106c2e4c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x4a82873e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x4a8287780 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x10ceae090 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x4a8287ae0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13cece7b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x4a8287eb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x4a84cb730 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13ceceb50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13cecef20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x106c2e7c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x106c2eb20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x106c2ee70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13cecf280 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x106c2f2d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x4a8288210 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x4a82886f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x4a8288a90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x106c2f630 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_mxfp4_f32_r1_2              0x3ab427840 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_mxfp4_f32_r1_3              0x10ceae5d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_mxfp4_f32_r1_4              0x10ceae890 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_mxfp4_f32_r1_5              0x10ceaec50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x10ceaeff0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x10ceaf3c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x4a8288fd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x106c2f960 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x4a8289290 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x106c2fdc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x106c30190 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x106c30560 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13cecf7c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13cecfa80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x10ceaf900 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x10ceafbc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x10ceaff60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x10ceb0330 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x4a82896a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x106c30aa0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x106c30d60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13cecfd40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13ced0110 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x106c310c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x106c314d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x106c318a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13ced0570 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x4a8289aa0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x4a8289e70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x10ceb0690 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x10ceb0af0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x4a84cb9f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x4a828a240 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13ced0940 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13ced0d10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13ced10e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x106c31d00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x4a828a6a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x106c320a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x106c32440 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x10ceb0e50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_mxfp4_f32                    0x106c327a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x4a828aa40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x4a828ade0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x106c32ce0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x4a828b1b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x4a828b580 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x106c32fa0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13ced14b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13ced1880 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x106c333b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x106c33780 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13ced1ce0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13ced2040 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x106c33b50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13ced24a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x106c33f20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x106c342f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13ced2800 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x10ceb12b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x10ceb1680 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x10ceb1a50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13ced2b80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_mxfp4_f32                       0x10ceb1e20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_mxfp4_f32                       0x10ceb21f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x106c346c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x4a828b9e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x10ceb2650 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x4a828bd80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x4a828c120 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x106c34a20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x4a828c480 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x10ceb29a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x4a828c850 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x106c35060 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x106c35320 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x4a828cc20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13ced3020 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13ced32e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_map0_f16                     0x106c35620 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_map1_f32                     0x106c359f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_f32_f16                      0x13ced36e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_f16_f16                      0x4a828d1c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f16                     0x4a828d480 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f16                     0x13ced3ab0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f16                     0x13ced3f10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f16                     0x13ced4270 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f16                     0x106c35d50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_mxfp4_f16                    0x4a828d7e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f16                     0x4a828dbb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f16                     0x4a828df80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f16                     0x4a828e2e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f16                     0x13ced45f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f16                     0x4a828e6b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f16                  0x106c36120 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f16                   0x10ceb2d00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f16                  0x4a828ea80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f16                    0x4a828eee0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f16                    0x13ced49c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f16                    0x106c36580 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f16                    0x4a828f280 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f16                   0x13ced4d90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f16                   0x13ced51f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_norm_f32                          0x10ceb30d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_norm_f16                          0x4a828f620 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_multi_f32                         0x4a828fa80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_multi_f16                         0x106c36950 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_vision_f32                        0x4a828fde0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_vision_f16                        0x10ceb34a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_neox_f32                          0x10ceb3870 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_neox_f16                          0x10ceb3cd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_f16                             0x4a8290240 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_f32                             0x10ceb4030 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_ext_f16                         0x10ceb4490 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_ext_f32                         0x4a82905a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x4a8290a00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x4a8290dd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_upscale_f32                            0x106c36cb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pad_f32                                0x10ceb47f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x4a8291130 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x10ceb4c90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_arange_f32                             0x106c37110 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x4a8291500 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x106c374e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_leaky_relu_f32                         0x106c37840 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x10ceb4f90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x106c37ca0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13ced55c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13ced5990 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x106c38000 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h192                0x10ceb5360 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_hk192_hv128         0x13ced5cf0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x106c383d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_hk576_hv512         0x13ced60c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x4a8291960 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x4a8291d30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x10ceb58a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x10ceb5ba0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x4a8292100 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h192               0x10ceb60e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_hk192_hv128        0x106c38830 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x4a8292460 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_hk576_hv512        0x4a82928c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x106c38c00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x4a8292c90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x10ceb63a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x4a8292ff0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x4a8293450 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h192               0x10ceb6700 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_hk192_hv128        0x13ced66a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x10ceb6b10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_hk576_hv512        0x4a82937b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13ced6960 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x106c38f60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x4a8293c10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13ced6cc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x106c393c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h192               0x4a8293f70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_hk192_hv128        0x10ceb6e70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13ced7010 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_hk576_hv512        0x4a8294340 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x10ceb7240 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x4a8294710 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x4a8294b70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x10ceb7610 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13ced73e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h192               0x4a8294ed0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_hk192_hv128        0x4a8295330 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x4a8295690 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_hk576_hv512        0x106c39720 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x4a8295af0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x10ceb79e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x10ceb7e40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x10ceb8210 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x10ceb85e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h192               0x4a82961a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_hk192_hv128        0x4a84cbcb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13ced77b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_hk576_hv512        0x106c39af0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h64             0x106c39eb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h64            0x13ced7c10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h64            0x13ced7fe0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h64            0x106c3a3f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h64            0x13ced8340 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h64            0x106c3a770 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h96             0x106c3aa30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h96            0x10ceb89b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h96            0x10ceb8d10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h96            0x10ceb91b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h96            0x106c3ad80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h96            0x10ceb9510 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x10ceb9910 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x4a84cc070 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x4a84cc410 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x4a84cc7e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x4a84ccb40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x106c3b1e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h192            0x4a84ccfe0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h192           0x4a8296460 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h192           0x10cf2dbe0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h192           0x3ab427e80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h192           0x4a84cd3e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h192           0x4a8296720 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_hk192_hv128      0x10ceb9d70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_hk192_hv128      0x4a87c8550 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_hk192_hv128      0x4a87c80f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_hk192_hv128      0x3ab428140 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_hk192_hv128      0x10cf2df80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_hk192_hv128      0x10cf2e3e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x3ab428440 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13ced87a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x4a87cd2e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x4a84cd740 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x10cf2e780 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x10cf2eb50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_hk576_hv512      0x3ab428a80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_hk576_hv512      0x10cf2eeb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_hk576_hv512      0x3ab428d40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_hk576_hv512      0x13ced8b00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_hk576_hv512      0x13ced8f60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_hk576_hv512      0x10cf2f310 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_f32                                0x4a87cab70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_i32                                0x3ab429100 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                            0x3ab4293c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                            0x106c3b640 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f32                            0x4a87c9d10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                            0x4a84cdc30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x106c3b9a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13ced9330 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x4a84cdf90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x4a84ce350 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x4a87dc190 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x3ab4297d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x4a84ce6b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x3ab429b30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x4a84cea50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x4a87cd700 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x4a84ceeb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x10cf2f6b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x106c3bcf0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x4a8725590 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x13ced9790 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x10ceba140 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_concat                                 0x3ab429eb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sqr                                    0x4a84cf210 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sqrt                                   0x4a87c77f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sin                                    0x13ced9af0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cos                                    0x4a84cf670 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_neg                                    0x3ab42a310 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_reglu                                  0x106c3c0c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_geglu                                  0x106c3c520 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_swiglu                                 0x4a84cf9d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_swiglu_oai                             0x4a87d6b60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_geglu_erf                              0x10cf2fa10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_geglu_quick                            0x10ceba4a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sum_rows                               0x10cf30050 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mean                                   0x4a84cfce0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_argmax                                 0x13ced9e70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13ceda2d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x4a87d1010 | th_max = 1024 | th_width =   32\n",
      "set_abort_callback: call\n",
      "llama_context:        CPU  output buffer size =     0.12 MiB\n",
      "create_memory: n_ctx = 4096 (padded)\n",
      "llama_kv_cache_unified: layer   0: dev = Metal\n",
      "llama_kv_cache_unified: layer   1: dev = Metal\n",
      "llama_kv_cache_unified: layer   2: dev = Metal\n",
      "llama_kv_cache_unified: layer   3: dev = Metal\n",
      "llama_kv_cache_unified: layer   4: dev = Metal\n",
      "llama_kv_cache_unified: layer   5: dev = Metal\n",
      "llama_kv_cache_unified: layer   6: dev = Metal\n",
      "llama_kv_cache_unified: layer   7: dev = Metal\n",
      "llama_kv_cache_unified: layer   8: dev = Metal\n",
      "llama_kv_cache_unified: layer   9: dev = Metal\n",
      "llama_kv_cache_unified: layer  10: dev = Metal\n",
      "llama_kv_cache_unified: layer  11: dev = Metal\n",
      "llama_kv_cache_unified: layer  12: dev = Metal\n",
      "llama_kv_cache_unified: layer  13: dev = Metal\n",
      "llama_kv_cache_unified: layer  14: dev = Metal\n",
      "llama_kv_cache_unified: layer  15: dev = Metal\n",
      "llama_kv_cache_unified: layer  16: dev = Metal\n",
      "llama_kv_cache_unified: layer  17: dev = Metal\n",
      "llama_kv_cache_unified: layer  18: dev = Metal\n",
      "llama_kv_cache_unified: layer  19: dev = Metal\n",
      "llama_kv_cache_unified: layer  20: dev = Metal\n",
      "llama_kv_cache_unified: layer  21: dev = Metal\n",
      "llama_kv_cache_unified: layer  22: dev = Metal\n",
      "llama_kv_cache_unified: layer  23: dev = Metal\n",
      "llama_kv_cache_unified: layer  24: dev = Metal\n",
      "llama_kv_cache_unified: layer  25: dev = Metal\n",
      "llama_kv_cache_unified: layer  26: dev = Metal\n",
      "llama_kv_cache_unified: layer  27: dev = Metal\n",
      "llama_kv_cache_unified: layer  28: dev = Metal\n",
      "llama_kv_cache_unified: layer  29: dev = Metal\n",
      "llama_kv_cache_unified: layer  30: dev = Metal\n",
      "llama_kv_cache_unified: layer  31: dev = Metal\n",
      "llama_kv_cache_unified:      Metal KV buffer size =  2048.00 MiB\n",
      "llama_kv_cache_unified: size = 2048.00 MiB (  4096 cells,  32 layers,  1/1 seqs), K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\n",
      "llama_context: enumerating backends\n",
      "llama_context: backend_ptrs.size() = 3\n",
      "llama_context: max_nodes = 2328\n",
      "llama_context: worst-case: n_tokens = 512, n_seqs = 1, n_outputs = 0\n",
      "graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  1, n_outputs =  512\n",
      "graph_reserve: reserving a graph for ubatch with n_tokens =    1, n_seqs =  1, n_outputs =    1\n",
      "graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  1, n_outputs =  512\n",
      "llama_context:      Metal compute buffer size =   312.01 MiB\n",
      "llama_context:        CPU compute buffer size =    32.01 MiB\n",
      "llama_context: graph nodes  = 1126\n",
      "llama_context: graph splits = 2\n",
      "Metal : EMBED_LIBRARY = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | REPACK = 1 | \n",
      "Model metadata: {'tokenizer.chat_template': \"{% for message in messages %}{% if message['role'] == 'system' %}{{'<|system|>'+ '\\n' + message['content'] + '\\n'}}{% elif message['role'] == 'user' %}{{'<|user|>' + '\\n' + message['content'] + '\\n'}}{% elif message['role'] == 'assistant' %}{{'<|assistant|>' + '\\n' + message['content'] + '<|endoftext|>' + ('' if loop.last else '\\n')}}{% endif %}{% endfor %}\", 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.add_bos_token': 'false', 'tokenizer.ggml.padding_token_id': '32001', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '32000', 'tokenizer.ggml.pre': 'default', 'llama.embedding_length': '4096', 'llama.vocab_size': '32008', 'llama.attention.head_count_kv': '32', 'general.size_label': '7B', 'general.base_model.0.name': 'Granite 7b Base', 'llama.block_count': '32', 'general.base_model.0.organization': 'Ibm', 'general.base_model.count': '1', 'general.quantization_version': '2', 'llama.rope.dimension_count': '128', 'general.license': 'apache-2.0', 'general.base_model.0.repo_url': 'https://huggingface.co/ibm/granite-7b-base', 'llama.attention.head_count': '32', 'llama.context_length': '4096', 'general.file_type': '15', 'general.finetune': 'lab', 'llama.feed_forward_length': '11008', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'general.architecture': 'llama', 'general.basename': 'instructlab-granite', 'llama.rope.freq_base': '10000.000000', 'tokenizer.ggml.bos_token_id': '1', 'general.type': 'model', 'tokenizer.ggml.model': 'llama', 'general.name': 'Instructlab Granite 7b Lab'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {% for message in messages %}{% if message['role'] == 'system' %}{{'<|system|>'+ '\n",
      "' + message['content'] + '\n",
      "'}}{% elif message['role'] == 'user' %}{{'<|user|>' + '\n",
      "' + message['content'] + '\n",
      "'}}{% elif message['role'] == 'assistant' %}{{'<|assistant|>' + '\n",
      "' + message['content'] + '<|endoftext|>' + ('' if loop.last else '\n",
      "')}}{% endif %}{% endfor %}\n",
      "Using chat eos_token: <|endoftext|>\n",
      "Using chat bos_token: <s>\n"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "llm.close()\n",
    "llm = Llama(\n",
    "    model_path=\"./models/granite-7b-lab-Q4_K_M.gguf\",\n",
    "    n_ctx=4096,\n",
    "    n_threads=8,     # Adjust to your CPU cores\n",
    "    n_gpu_layers=50  # If GPU available\n",
    ")\n",
    "llm.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "af5c4a94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Top reranked chunks (earliest position first; ties by similarity):\n",
      "  - Rank 1: position=1, is_exact=False, similarity=0.710\n",
      "    matched_step: NETWORK: requery, 0, 0, 0, 0, 320, items, fQueryRetries, 1, fLastRetryTimestamp, 520841203.7\n",
      "    metadata: {'sessions': '4', 'month': 'Jul', 'date': '3', 'time': '23:16:10', 'component': 'QQ', 'pid': '10018', 'starter_log': 'NETWORK: requery, 0, 0, 0, 0, 320, items, fQueryRetries, 1, fLastRetryTimestamp, 520841203.7', 'starter_log_freq': 1}\n",
      "    sequence: <SEQUENCE> NETWORK: requery, 0, 0, 0, 0, 320, items, fQueryRetries, 1, fLastRetryTimestamp, 520841203.7 -> Location icon should now... </SEQUENCE>\n",
      "  - Rank 2: position=1, is_exact=False, similarity=0.710\n",
      "    matched_step: NETWORK: requery, 0, 0, 0, 0, 320, items, fQueryRetries, 1, fLastRetryTimestamp, 520841203.7\n",
      "    metadata: {'sessions': '4', 'month': 'Jul', 'date': '3', 'time': '23:07:40', 'component': 'locationd', 'pid': '82', 'starter_log': 'NETWORK: requery, 0, 0, 0, 0, 320, items, fQueryRetries, 1, fLastRetryTimestamp, 520841203.7', 'starter_log_freq': 1}\n",
      "    sequence: <SEQUENCE> NETWORK: requery, 0, 0, 0, 0, 320, items, fQueryRetries, 1, fLastRetryTimestamp, 520841203.7 -> Location icon should now... </SEQUENCE>\n",
      "  - Rank 3: position=1, is_exact=False, similarity=0.710\n",
      "    matched_step: NETWORK: requery, 0, 0, 0, 0, 320, items, fQueryRetries, 1, fLastRetryTimestamp, 520841203.7\n",
      "    metadata: {'sessions': '4', 'month': 'Jul', 'date': '3', 'time': '23:07:12', 'component': 'locationd', 'pid': '82', 'starter_log': 'NETWORK: requery, 0, 0, 0, 0, 320, items, fQueryRetries, 1, fLastRetryTimestamp, 520841203.7', 'starter_log_freq': 1}\n",
      "    sequence: <SEQUENCE> NETWORK: requery, 0, 0, 0, 0, 320, items, fQueryRetries, 1, fLastRetryTimestamp, 520841203.7 -> Location icon should now... </SEQUENCE>\n",
      "\n",
      "[DEBUG] Predicted Next Logs:\n",
      "  - Location icon should now be in state 'Active -> FA||Url||taskID[2019353410] dealloc -> dnssd_clientstub ConnectToServer: connect() -> No of tries: 1 -> [23:24:32.378] <<<< Boss >>>> figPlaybackBossPrerollCompleted: unexpected preroll-complete notification -> Location icon should now be in state 'Inactive -> KeychainGetICDPStatus: status: off -> ARPT: 697702.656868: AirPort_Brcm43xx::powerChange: System Sleep -> tcp_connection_tls_session_error_callback_imp 2210 tcp_connection_tls_session_handle_read_error.790 error 60\n",
      "\n",
      "[DEBUG] Allowed Components: ['QQ', 'locationd']\n",
      "\n",
      "[DEBUG] Final Prompt:\n",
      "\n",
      "You are an expert SRE whose job is to debug MaC logs. Use ONLY the provided context chunks and the Predicted Next Logs below (do not hallucinate).\n",
      "\n",
      "CONTEXT CHUNKS\n",
      "[CHUNK 1]\n",
      "METADATA: sessions: 4, month: Jul, date: 3, time: 23:16:10, component: QQ, pid: 10018, starter_log: NETWORK: requery, 0, 0, 0, 0, 320, items, fQueryRetries, 1, fLastRetryTimestamp, 520841203.7, starter_log_freq: 1\n",
      "SEQUENCE: <SEQUENCE> NETWORK: requery, 0, 0, 0, 0, 320, items, fQueryRetries, 1, fLastRetryTimestamp, 520841203.7 -> Location icon should now be in state 'Active' -> FA||Url||taskID[2019353410] dealloc -> dnssd_clientstub ConnectToServer: connect()-> No of tries: 1 -> [23:24:32.378] <<<< Boss >>>> figPlaybackBossPrerollCompleted: unexpected preroll-complete notification -> Location icon should now be in state 'Inactive' -> KeychainGetICDPStatus: status: off -> ARPT: 697702.656868: AirPort_Brcm43xx::powerChange: Syst ... </SEQUENCE>\n",
      "\n",
      "[CHUNK 2]\n",
      "METADATA: sessions: 4, month: Jul, date: 3, time: 23:07:40, component: locationd, pid: 82, starter_log: NETWORK: requery, 0, 0, 0, 0, 320, items, fQueryRetries, 1, fLastRetryTimestamp, 520841203.7, starter_log_freq: 1\n",
      "SEQUENCE: <SEQUENCE> NETWORK: requery, 0, 0, 0, 0, 320, items, fQueryRetries, 1, fLastRetryTimestamp, 520841203.7 -> Location icon should now be in state 'Active' -> FA||Url||taskID[2019353410] dealloc -> dnssd_clientstub ConnectToServer: connect()-> No of tries: 1 -> [23:24:32.378] <<<< Boss >>>> figPlaybackBossPrerollCompleted: unexpected preroll-complete notification -> Location icon should now be in state 'Inactive' -> KeychainGetICDPStatus: status: off -> ARPT: 697702.656868: AirPort_Brcm43xx::powerChange: Syst ... </SEQUENCE>\n",
      "\n",
      "[CHUNK 3]\n",
      "METADATA: sessions: 4, month: Jul, date: 3, time: 23:07:12, component: locationd, pid: 82, starter_log: NETWORK: requery, 0, 0, 0, 0, 320, items, fQueryRetries, 1, fLastRetryTimestamp, 520841203.7, starter_log_freq: 1\n",
      "SEQUENCE: <SEQUENCE> NETWORK: requery, 0, 0, 0, 0, 320, items, fQueryRetries, 1, fLastRetryTimestamp, 520841203.7 -> Location icon should now be in state 'Active' -> FA||Url||taskID[2019353410] dealloc -> dnssd_clientstub ConnectToServer: connect()-> No of tries: 1 -> [23:24:32.378] <<<< Boss >>>> figPlaybackBossPrerollCompleted: unexpected preroll-complete notification -> Location icon should now be in state 'Inactive' -> KeychainGetICDPStatus: status: off -> ARPT: 697702.656868: AirPort_Brcm43xx::powerChange: Syst ... </SEQUENCE>\n",
      "\n",
      "\n",
      "PREDICTED NEXT LOGS (if any)\n",
      "- Location icon should now be in state 'Active -> FA||Url||taskID[2019353410] dealloc -> dnssd_clientstub ConnectToServer: connect() -> No of tries: 1 -> [23:24:32.378] <<<< Boss >>>> figPlaybackBossPrerollCompleted: unexpected preroll-complete notification -> Location icon should now be in state 'Inactive -> KeychainGetICDPStatus: status: off -> ARPT: 697702.656868: AirPort_Brcm43xx::powerChange: System Sleep -> tcp_connection_tls_session_error_callback_imp 2210 tcp_connection_tls_session_handle_read_error.790 error 60\n",
      "\n",
      "INSTRUCTIONS\n",
      "- If Predicted Next Logs is non-empty, base NEXT-PREDICTIONS primarily on these items.\n",
      "- If it is empty, use <SEQUENCE> content in the context to reason about next predictions. Do NOT mention that predictions were empty.\n",
      "- Keep the answer concise, structured, and grounded strictly in the provided data.\n",
      "\n",
      "QUESTION\n",
      "I am getting this error in the log 'NETWORK: requery, 0, 0, 0, 0, 320, items, fQueryRetries, 1, fLastRetryTimestamp, 520841203.7' What could be issues and next steps ? \n",
      "\n",
      "BEGIN OUTPUT\n",
      "Return EXACTLY these sections with these headings and nothing else. Do not add any extra lines before or after section 6. Do NOT repeat content.\n",
      "1) RCA: 1-2 sentences describing the root cause\n",
      "2) COMPONENTS: component(s) responsible (choose only from: QQ, locationd; if none apply, write 'UNKNOWN')\n",
      "3) SUGGESTED FIXES: exactly 3 concrete steps (imperative voice)\n",
      "4) NEXT-PREDICTIONS: 2-4 likely next events with rationale; use Predicted Next Logs when available, else <SEQUENCE>\n",
      "5) SUMMARY: 1-2 sentences summarizing RCA, component, and next actions\n",
      "6) CONFIDENCE: a single integer 0-100 (no % sign, no extra words)\n",
      "END OUTPUT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =   10966.97 ms\n",
      "llama_perf_context_print: prompt eval time =   10966.07 ms /  1637 tokens (    6.70 ms per token,   149.28 tokens per second)\n",
      "llama_perf_context_print:        eval time =   34639.76 ms /   465 runs   (   74.49 ms per token,    13.42 tokens per second)\n",
      "llama_perf_context_print:       total time =   45828.46 ms /  2102 tokens\n",
      "llama_perf_context_print:    graphs reused =        450\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[DEBUG] Answer (raw LLM output):\n",
      "\n",
      "[SEQUENCE]\n",
      "NETWORK: requery, 0, 0, 0, 0, 320, items, fQueryRetries, 1, fLastRetryTimestamp, 520841203.7 -> Location icon should now be in state 'Active' -> FA||Url||taskID[2019353410] dealloc -> dnssd\\_clientstub ConnectToServer: connect()-> No of tries: 1 -> [23:24:32.378] <<<< Boss >>>> figPlaybackBossPrerollCompleted: unexpected preroll-complete notification -> Location icon should now be in state 'Inactive' -> KeychainGetICDPStatus: status: off -> ARPT: 697702.656868: AirPort\\_Brcm43xx::powerChange: Syst ...\n",
      "\n",
      "[RCA]\n",
      "The system encountered an issue while attempting to query items in the network.\n",
      "\n",
      "[COMPONENTS]\n",
      "The components responsible for this error are QQ and locationd.\n",
      "\n",
      "[SUGGESTED FIXES]\n",
      "1. Check the network connection and ensure that the necessary permissions are granted.\n",
      "2. Verify that the items being queried are valid and exist in the system.\n",
      "3. Review the system logs to identify any error messages or patterns related to the query process.\n",
      "\n",
      "[NEXT-PREDICTIONS]\n",
      "Based on the current state of the system, the next likely events include:\n",
      "1. The system will attempt to query items again after a short delay.\n",
      "2. The system will log an error message related to the query process.\n",
      "3. The system will notify the user of the issue and provide guidance on how to resolve it.\n",
      "\n",
      "[SUMMARY]\n",
      "The system is experiencing issues while querying items in the network, and the locationd component may also be affected. The suggested fixes include checking the network connection, verifying item validity, and reviewing system logs.\n",
      "\n",
      "[CONFIDENCE]\n",
      "90\n",
      "[SEQUENCE]\n",
      "NETWORK: requery, 0, 0, 0, 0, 320, items, fQueryRetries, 1, fLastRetryTimestamp, 520841203.7 -> Location icon should now be in state 'Active' -> FA||Url||taskID[2019353410] dealloc -> dnssd\\_clientstub ConnectToServer: connect()-> No of tries: 1 -> [23:24:32.378] <<<< Boss >>>> figPlaybackBossPrerollCompleted: unexpected preroll-complete notification -> Location icon should now be in state 'Inactive' -> KeychainGetICDPStatus: status: off -> ARPT: 697702.656868: AirPort\\_Brcm43xx::powerChange: Syst ...\n",
      "\n",
      "[RCA]\n",
      "The system encountered an issue while attempting to query items in the network.\n",
      "\n",
      "[COMPONENTS]\n",
      "The components responsible for this error are QQ and locationd.\n",
      "\n",
      "[SUGGESTED FIXES]\n",
      "1. Check the network connection and ensure that the necessary permissions are granted.\n",
      "2. Verify that the items being queried are valid and exist in the system.\n",
      "3. Review the system logs to identify any error messages or patterns related to the query process.\n",
      "\n",
      "[NEXT-PREDICTIONS]\n",
      "Based on the current state of the system, the next likely events include:\n",
      "1. The system will attempt to query items again after a short delay.\n",
      "2. The system will log an error message related to the query process.\n",
      "3. The system will notify the user of the issue and provide guidance on how to resolve it.\n",
      "\n",
      "[SUMMARY]\n",
      "The system is experiencing issues while querying items in the network, and the locationd component may also be affected. The suggested fixes include checking the network connection, verifying item validity, and reviewing system logs.\n",
      "\n",
      "[CONFIDENCE]\n",
      "90\n"
     ]
    }
   ],
   "source": [
    "out = rag_chat_simple(\n",
    "  user_query=user_question,\n",
    "  base_k=4,\n",
    "  reranking_top_k=3,\n",
    "  llm_callable=llm,\n",
    "  verbose=True,\n",
    "  max_tokens=600,\n",
    "  temperature=0.0)\n",
    "print(out[\"answer\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ee3dfe",
   "metadata": {},
   "source": [
    "# Alternate way\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bc1859bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<STARTER> NETWORK: requery, 0, 0, 0, 0, 320, items, fQueryRetries, 1, fLastRetryTimestamp, 520841203.7 </STARTER>\\n<SEQUENCE> NETWORK: requery, 0, 0, 0, 0, 320, items, fQueryRetries, 1, fLastRetryTimestamp, 520841203.7 -> Location icon should now be in state 'Active' -> FA||Url||taskID[2019353410] dealloc -> dnssd_clientstub ConnectToServer: connect()-> No of tries: 1 -> [23:24:32.378] <<<< Boss >>>> figPlaybackBossPrerollCompleted: unexpected preroll-complete notification -> Location icon should now be in state 'Inactive' -> KeychainGetICDPStatus: status: off -> ARPT: 697702.656868: AirPort_Brcm43xx::powerChange: System Sleep -> tcp_connection_tls_session_error_callback_imp 2210 tcp_connection_tls_session_handle_read_error.790 error 60 </SEQUENCE>\""
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes[0].get_content()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4bf66b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, math\n",
    "from collections import defaultdict, Counter\n",
    "from typing import List\n",
    "\n",
    "_RE_ARROW = re.compile(r'\\s*->\\s*')\n",
    "\n",
    "def split_chain_into_events(chain_text: str) -> List[str]:\n",
    "    \"\"\"Split by '->' (handling spaces). Fallback to other separators if needed.\"\"\"\n",
    "    if not chain_text or str(chain_text).strip() == \"\":\n",
    "        return []\n",
    "    txt = str(chain_text)\n",
    "    if '->' in txt:\n",
    "        parts = _RE_ARROW.split(txt)\n",
    "    elif '|||' in txt:\n",
    "        parts = [p.strip() for p in txt.split('|||')]\n",
    "    elif '\\n' in txt:\n",
    "        parts = [p.strip() for p in txt.splitlines()]\n",
    "    else:\n",
    "        parts = [p.strip() for p in txt.split(',')]\n",
    "    parts = [p for p in parts if p and str(p).strip() != \"\"]\n",
    "    return parts\n",
    "\n",
    "# Normalization: lowercase, remove punctuation (but keep alphanum + spaces)\n",
    "_norm_re = re.compile(r'[^a-z0-9 ]+')\n",
    "def normalize_text(s: str) -> str:\n",
    "    if s is None:\n",
    "        return \"\"\n",
    "    s2 = str(s).lower()\n",
    "    s2 = s2.replace('-', ' ')  # treat hyphen like space\n",
    "    s2 = _norm_re.sub(' ', s2)\n",
    "    s2 = re.sub(r'\\s+', ' ', s2).strip()\n",
    "    return s2\n",
    "\n",
    "def token_overlap_score(query: str, text: str) -> float:\n",
    "    \"\"\"\n",
    "    Returns overlap ratio between query tokens and text tokens.\n",
    "    Score = |intersection| / |query_tokens|\n",
    "    \"\"\"\n",
    "    q = normalize_text(query).split()\n",
    "    t = normalize_text(text).split()\n",
    "    if not q:\n",
    "        return 0.0\n",
    "    set_q = set(q)\n",
    "    set_t = set(t)\n",
    "    inter = set_q & set_t\n",
    "    return len(inter) / max(1, len(set_q))\n",
    "\n",
    "def first_pos_in_events(events: List[str], query: str, threshold: float = 0.5) -> int:\n",
    "    \"\"\"\n",
    "    Return 1-based index of first event whose token-overlap with query >= threshold.\n",
    "    Returns math.inf if none found.\n",
    "    \"\"\"\n",
    "    q = normalize_text(query)\n",
    "    if not events:\n",
    "        return math.inf\n",
    "    for i, ev in enumerate(events, start=1):\n",
    "        if token_overlap_score(q, ev) >= threshold:\n",
    "            return i\n",
    "    return math.inf\n",
    "\n",
    "def extract_next_events_from_events(events: List[str], query: str, max_next=1, threshold: float = 0.5) -> List[str]:\n",
    "    out = []\n",
    "    q = normalize_text(query)\n",
    "    for i, ev in enumerate(events):\n",
    "        if token_overlap_score(q, ev) >= threshold:\n",
    "            nxt_idx = i + 1\n",
    "            if nxt_idx < len(events):\n",
    "                end_idx = min(len(events), nxt_idx + max_next)\n",
    "                nxt = \" -> \".join(events[nxt_idx:end_idx])\n",
    "                if nxt:\n",
    "                    out.append(nxt)\n",
    "    return out\n",
    "\n",
    "\n",
    "from typing import Any, Dict, Iterable, List, Tuple\n",
    "import difflib\n",
    "\n",
    "\n",
    "def _normalize_text(value: str) -> str:\n",
    "    if value is None:\n",
    "        return \"\"\n",
    "    return \" \".join(value.strip().strip('\"').strip(\"'\").lower().split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f36e2c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: The final simple_rag_chat_final function (drop-in)\n",
    "import torch\n",
    "\n",
    "def _extract_text_and_meta(node):\n",
    "    \"\"\"\n",
    "    Defensive extraction for LlamaIndex node-like objects.\n",
    "    Returns (text, metadata_dict).\n",
    "    \"\"\"\n",
    "    text = \"\"\n",
    "    meta = {}\n",
    "    if hasattr(node, \"text\"):\n",
    "        text = getattr(node, \"text\") or \"\"\n",
    "    elif hasattr(node, \"get_text\"):\n",
    "        try:\n",
    "            text = node.get_text() or \"\"\n",
    "        except Exception:\n",
    "            text = \"\"\n",
    "    elif getattr(node, \"node\", None) is not None:\n",
    "        inner = getattr(node, \"node\")\n",
    "        text = getattr(inner, \"text\", \"\") or (getattr(inner, \"get_text\", lambda: \"\")() if hasattr(inner, \"get_text\") else \"\")\n",
    "    if hasattr(node, \"metadata\"):\n",
    "        meta = getattr(node, \"metadata\") or {}\n",
    "    elif hasattr(node, \"extra_info\"):\n",
    "        meta = getattr(node, \"extra_info\") or {}\n",
    "    elif getattr(node, \"node\", None) is not None:\n",
    "        inner = getattr(node, \"node\")\n",
    "        meta = getattr(inner, \"metadata\", {}) or getattr(inner, \"extra_info\", {}) or {}\n",
    "    if meta is None:\n",
    "        meta = {}\n",
    "    return str(text), {str(k): v for k, v in dict(meta).items()}\n",
    "\n",
    "def simple_rag_chat_final(\n",
    "    question: str,\n",
    "    index,                       # LlamaIndex VectorStoreIndex\n",
    "    llm,                         # Granite LLM callable (llama_cpp.Llama)\n",
    "    top_k: int = 5,\n",
    "    base_retrieval_k: int = 25,\n",
    "    use_reranker: bool = True,\n",
    "    reranker_model: str = \"BAAI/bge-ranker-base\",\n",
    "    fuzzy_threshold: float = 0.5,   # token overlap threshold (0..1) for matching\n",
    "    max_next_events: int = 1\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Minimal RAG that uses token-overlap fuzzy matching on your chain_log events.\n",
    "    Expects node metadata to include your CSV keys (sessions, starter_log, chain_log, starter_log_freq).\n",
    "    \"\"\"\n",
    "    q_norm = normalize_text(question)\n",
    "    retriever = index.as_retriever(similarity_top_k=base_retrieval_k)\n",
    "    nodes = retriever.retrieve(question)\n",
    "    if not nodes:\n",
    "        return {\"question\": question, \"chosen_sessions\": [], \"predicted_next_events\": [], \"context_used\": \"\", \"llm_response\": \"\", \"note\": \"no_nodes_retrieved\"}\n",
    "\n",
    "    # optional reranker (safe)\n",
    "    if use_reranker and len(nodes) > top_k:\n",
    "        try:\n",
    "            from llama_index.postprocessor.flag_embedding_reranker import FlagEmbeddingReranker\n",
    "            device = \"cuda\" if torch.cuda.is_available() else (\"mps\" if getattr(torch.backends, \"mps\", None) and torch.backends.mps.is_available() else \"cpu\")\n",
    "            reranker = FlagEmbeddingReranker(model=reranker_model, top_n=base_retrieval_k, device=device)\n",
    "            nodes = reranker.postprocess_nodes(nodes, query_str=question)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # aggregate nodes by session (use 'sessions' metadata; fallback to chain snippet)\n",
    "    session_nodes = defaultdict(list)\n",
    "    for node in nodes:\n",
    "        text, meta = _extract_text_and_meta(node)\n",
    "        sess = meta.get(\"sessions\")\n",
    "        # fallback keys\n",
    "        if sess is None:\n",
    "            sess = meta.get(\"session\") or meta.get(\"session_id\") or meta.get(\"session_uuid\")\n",
    "        # fallback surrogate\n",
    "        chain_text = meta.get(\"chain_log\") or meta.get(\"chain\") or meta.get(\"Content\") or text\n",
    "        starter_text = meta.get(\"starter_log\") or meta.get(\"starter\") or \"\"\n",
    "        starter_freq = meta.get(\"starter_log_freq\") or meta.get(\"starter_freq\") or 0\n",
    "        sess_key = str(sess) if sess is not None else f\"chain_surrogate::{abs(hash(chain_text))%10**9}\"\n",
    "        events = split_chain_into_events(chain_text)\n",
    "        pos = first_pos_in_events(events, question, threshold=fuzzy_threshold)\n",
    "        combo_count = int(starter_freq) if str(starter_freq).strip().isdigit() else 0\n",
    "        session_nodes[sess_key].append({\n",
    "            \"node\": node, \"text\": text, \"meta\": meta,\n",
    "            \"chain_text\": chain_text, \"events\": events,\n",
    "            \"pos\": pos, \"starter\": starter_text, \"starter_freq\": combo_count\n",
    "        })\n",
    "\n",
    "    # build session summaries\n",
    "    session_summary = []\n",
    "    for sid, nlist in session_nodes.items():\n",
    "        min_pos = min(n[\"pos\"] for n in nlist)\n",
    "        starter_match = any(token_overlap_score(question, (n.get(\"starter\",\"\") or \"\")) >= fuzzy_threshold for n in nlist)\n",
    "        max_freq = max(n.get(\"starter_freq\", 0) for n in nlist)\n",
    "        session_summary.append({\n",
    "            \"session_id\": sid,\n",
    "            \"nodes\": nlist,\n",
    "            \"min_pos\": min_pos,\n",
    "            \"starter_match\": starter_match,\n",
    "            \"starter_freq\": max_freq\n",
    "        })\n",
    "\n",
    "    # apply preference rules\n",
    "    starters = [s for s in session_summary if s[\"starter_match\"]]\n",
    "    if starters:\n",
    "        # prefer higher starter_freq then session_id for deterministic tie-break\n",
    "        starters.sort(key=lambda s: (-s[\"starter_freq\"], str(s[\"session_id\"])))\n",
    "        chosen_sessions_list = starters\n",
    "    else:\n",
    "        inseq = [s for s in session_summary if s[\"min_pos\"] != math.inf]\n",
    "        if not inseq:\n",
    "            # fallback: return top retriever nodes\n",
    "            flat = []\n",
    "            for node in nodes[:top_k]:\n",
    "                t, m = _extract_text_and_meta(node)\n",
    "                flat.append({\"node\": node, \"text\": t, \"meta\": m})\n",
    "            context = \"\\n\\n\".join([f\"[Context {i+1}] {f['text']}\" for i, f in enumerate(flat)])\n",
    "            try:\n",
    "                resp = llm(f\"Context:\\n{context}\\n\\nQuestion: {question}\\nAnswer briefly.\", max_tokens=600, temperature=0.0)\n",
    "                ans = resp[\"choices\"][0][\"text\"].strip()\n",
    "            except Exception as e:\n",
    "                ans = f\"LLM call failed: {e}\"\n",
    "            return {\"question\": question, \"chosen_sessions\": [], \"predicted_next_events\": [], \"context_used\": context, \"llm_response\": ans, \"note\": \"no_exact_matches_in_chains\"}\n",
    "        inseq.sort(key=lambda s: (s[\"min_pos\"], -s[\"starter_freq\"], str(s[\"session_id\"])))\n",
    "        chosen_sessions_list = inseq\n",
    "\n",
    "    # build context nodes from chosen sessions (earliest positions first), up to top_k chunks\n",
    "    context_nodes = []\n",
    "    for s in chosen_sessions_list:\n",
    "        sorted_nodes = sorted(s[\"nodes\"], key=lambda n: (n[\"pos\"]))\n",
    "        for n in sorted_nodes:\n",
    "            context_nodes.append(n)\n",
    "            if len(context_nodes) >= top_k:\n",
    "                break\n",
    "        if len(context_nodes) >= top_k:\n",
    "            break\n",
    "\n",
    "    context_parts = []\n",
    "    for i, n in enumerate(context_nodes, start=1):\n",
    "        md = n[\"meta\"]\n",
    "        meta_bits = []\n",
    "        if md.get(\"starter_log\"): meta_bits.append(f\"starter={md.get('starter_log')}\")\n",
    "        if md.get(\"sessions\"): meta_bits.append(f\"sessions={md.get('sessions')}\")\n",
    "        if md.get(\"Month\") and md.get(\"Date\") and md.get(\"Time\"):\n",
    "            meta_bits.append(f\"time={md.get('Month')} {md.get('Date')} {md.get('Time')}\")\n",
    "        meta_line = (\" | \" + \", \".join(meta_bits)) if meta_bits else \"\"\n",
    "        context_parts.append(f\"[Context {i}]{meta_line}:\\n{n['text']}\")\n",
    "    context = \"\\n\\n\".join(context_parts)\n",
    "\n",
    "    # predicted next events aggregation (across top chosen sessions)\n",
    "    next_counter = Counter()\n",
    "    for s in chosen_sessions_list[:max(1, len(chosen_sessions_list))]:\n",
    "        for n in s[\"nodes\"]:\n",
    "            events = n.get(\"events\", [])\n",
    "            nxts = extract_next_events_from_events(events, question, max_next=max_next_events, threshold=fuzzy_threshold)\n",
    "            for nxt in nxts:\n",
    "                next_counter[nxt] += 1\n",
    "    total_next = sum(next_counter.values())\n",
    "    predicted_next_events = [{\"event\": ev, \"count\": cnt, \"prob\": cnt/total_next if total_next>0 else 0.0} for ev, cnt in next_counter.most_common()]\n",
    "\n",
    "    # build prompt (RCA) and call LLM\n",
    "    pred_section = \"\"\n",
    "    if predicted_next_events:\n",
    "        pred_section = \"Predicted next logs (top):\\n\" + \"\\n\".join([f\"{i+1}. {p['event']} (count={p['count']}, prob={p['prob']:.2%})\" for i, p in enumerate(predicted_next_events[:10])]) + \"\\n\\n\"\n",
    "    prompt = f\"\"\"\n",
    "You are an expert SRE whose job is to debug MaC logs. Use ONLY the provided context chunks and predicted next logs (do not hallucinate).\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "{pred_section}\n",
    "\n",
    "In case if pred_section is blank then use your reasoning to explain the answer.\n",
    "In that situation , Do not talk abouut pred_section being blank.\n",
    "You can simply use <SEQUENCE> from the context to reason about next predictions.\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Provide:\n",
    "1) ROOT CAUSE (1-2 sentences)\n",
    "2) EVIDENCE: list which context chunks support your cause\n",
    "3) SUGGESTED FIXES (3 concrete actions)\n",
    "4) NEXT-PREDICTIONS explanation : Fetch this information from the top chunks and sequence embedded in them. Explain the sequence position and why these are likely next events.\n",
    "5) CONFIDENCE: <Only return the number between 0-100 indicating confidence, do not add any text>\n",
    "\n",
    "Make sure that answer should contain all the above 5 sections clearly marked by their bullet numbers.\n",
    "Do not add any text or instruction in output outside these sections.\n",
    "Strictly restrict answer to the 5 pointer section Dont mention anything apart from these sections.\n",
    "\n",
    "\"\"\"\n",
    "    try:\n",
    "        llm_resp = llm(prompt, max_tokens=1200, temperature=0.3)\n",
    "        llm_text = llm_resp[\"choices\"][0][\"text\"].strip()\n",
    "\n",
    "    except Exception as e:\n",
    "        llm_text = f\"LLM call failed: {e}\"\n",
    "\n",
    "    result = {\n",
    "        \"question\": question,\n",
    "        \"chosen_sessions\": [s[\"session_id\"] for s in chosen_sessions_list],\n",
    "        \"context_used\": context,\n",
    "        \"llm_response\": llm_text,\n",
    "        \"predicted_next_events\": predicted_next_events,\n",
    "        \"session_summary\": session_summary\n",
    "    }\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "af436e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_question = \"I am getting this log 'NETWORK: requery, 0, 0, 0, 0, 320, items, fQueryRetries, 1, fLastRetryTimestamp, 520841203.7'\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b1c5a94e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ggml_metal_free: deallocating\n",
      "ggml_metal_mem_pool_free: freeing memory pool, num heaps = 0 (total = 0)\n",
      "ggml_metal_mem_pool_free: freeing memory pool, num heaps = 0 (total = 0)\n",
      "ggml_metal_mem_pool_free: freeing memory pool, num heaps = 0 (total = 0)\n",
      "ggml_metal_mem_pool_free: freeing memory pool, num heaps = 0 (total = 0)\n",
      "ggml_metal_mem_pool_free: freeing memory pool, num heaps = 0 (total = 0)\n",
      "ggml_metal_mem_pool_free: freeing memory pool, num heaps = 0 (total = 0)\n",
      "ggml_metal_mem_pool_free: freeing memory pool, num heaps = 0 (total = 0)\n",
      "ggml_metal_mem_pool_free: freeing memory pool, num heaps = 0 (total = 0)\n",
      "llama_model_load_from_file_impl: using device Metal (Apple M3) - 12381 MiB free\n",
      "llama_model_loader: loaded meta data with 37 key-value pairs and 291 tensors from ./models/granite-7b-lab-Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = Instructlab Granite 7b Lab\n",
      "llama_model_loader: - kv   3:                           general.finetune str              = lab\n",
      "llama_model_loader: - kv   4:                           general.basename str              = instructlab-granite\n",
      "llama_model_loader: - kv   5:                         general.size_label str              = 7B\n",
      "llama_model_loader: - kv   6:                            general.license str              = apache-2.0\n",
      "llama_model_loader: - kv   7:                   general.base_model.count u32              = 1\n",
      "llama_model_loader: - kv   8:                  general.base_model.0.name str              = Granite 7b Base\n",
      "llama_model_loader: - kv   9:          general.base_model.0.organization str              = Ibm\n",
      "llama_model_loader: - kv  10:              general.base_model.0.repo_url str              = https://huggingface.co/ibm/granite-7b...\n",
      "llama_model_loader: - kv  11:                               general.tags arr[str,6]       = [\"granite\", \"ibm\", \"lab\", \"labrador\",...\n",
      "llama_model_loader: - kv  12:                          general.languages arr[str,1]       = [\"en\"]\n",
      "llama_model_loader: - kv  13:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv  14:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv  15:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv  16:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv  17:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  18:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv  19:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  20:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  21:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  22:                           llama.vocab_size u32              = 32008\n",
      "llama_model_loader: - kv  23:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  24:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  25:                         tokenizer.ggml.pre str              = default\n",
      "llama_model_loader: - kv  26:                      tokenizer.ggml.tokens arr[str,32008]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  27:                      tokenizer.ggml.scores arr[f32,32008]   = [-1000.000000, -1000.000000, -1000.00...\n",
      "llama_model_loader: - kv  28:                  tokenizer.ggml.token_type arr[i32,32008]   = [3, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  29:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  30:                tokenizer.ggml.eos_token_id u32              = 32000\n",
      "llama_model_loader: - kv  31:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  32:            tokenizer.ggml.padding_token_id u32              = 32001\n",
      "llama_model_loader: - kv  33:               tokenizer.ggml.add_bos_token bool             = false\n",
      "llama_model_loader: - kv  34:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  35:                    tokenizer.chat_template str              = {% for message in messages %}{% if me...\n",
      "llama_model_loader: - kv  36:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "print_info: file format = GGUF V3 (latest)\n",
      "print_info: file type   = Q4_K - Medium\n",
      "print_info: file size   = 3.80 GiB (4.84 BPW) \n",
      "init_tokenizer: initializing tokenizer for type 1\n",
      "load: control token:  32004 '<|system|>' is not marked as EOG\n",
      "load: control token:  32003 '<|assistant|>' is not marked as EOG\n",
      "load: control token:  32002 '<|user|>' is not marked as EOG\n",
      "load: control token:  32001 '<|pad|>' is not marked as EOG\n",
      "load: control token:      1 '<s>' is not marked as EOG\n",
      "load: control token:      0 '<unk>' is not marked as EOG\n",
      "load: control token:      2 '</s>' is not marked as EOG\n",
      "load: printing all EOG tokens:\n",
      "load:   - 32000 ('<|endoftext|>')\n",
      "load: special tokens cache size = 8\n",
      "load: token to piece cache size = 0.1684 MB\n",
      "print_info: arch             = llama\n",
      "print_info: vocab_only       = 0\n",
      "print_info: n_ctx_train      = 4096\n",
      "print_info: n_embd           = 4096\n",
      "print_info: n_layer          = 32\n",
      "print_info: n_head           = 32\n",
      "print_info: n_head_kv        = 32\n",
      "print_info: n_rot            = 128\n",
      "print_info: n_swa            = 0\n",
      "print_info: is_swa_any       = 0\n",
      "print_info: n_embd_head_k    = 128\n",
      "print_info: n_embd_head_v    = 128\n",
      "print_info: n_gqa            = 1\n",
      "print_info: n_embd_k_gqa     = 4096\n",
      "print_info: n_embd_v_gqa     = 4096\n",
      "print_info: f_norm_eps       = 0.0e+00\n",
      "print_info: f_norm_rms_eps   = 1.0e-05\n",
      "print_info: f_clamp_kqv      = 0.0e+00\n",
      "print_info: f_max_alibi_bias = 0.0e+00\n",
      "print_info: f_logit_scale    = 0.0e+00\n",
      "print_info: f_attn_scale     = 0.0e+00\n",
      "print_info: n_ff             = 11008\n",
      "print_info: n_expert         = 0\n",
      "print_info: n_expert_used    = 0\n",
      "print_info: causal attn      = 1\n",
      "print_info: pooling type     = 0\n",
      "print_info: rope type        = 0\n",
      "print_info: rope scaling     = linear\n",
      "print_info: freq_base_train  = 10000.0\n",
      "print_info: freq_scale_train = 1\n",
      "print_info: n_ctx_orig_yarn  = 4096\n",
      "print_info: rope_finetuned   = unknown\n",
      "print_info: model type       = 7B\n",
      "print_info: model params     = 6.74 B\n",
      "print_info: general.name     = Instructlab Granite 7b Lab\n",
      "print_info: vocab type       = SPM\n",
      "print_info: n_vocab          = 32008\n",
      "print_info: n_merges         = 0\n",
      "print_info: BOS token        = 1 '<s>'\n",
      "print_info: EOS token        = 32000 '<|endoftext|>'\n",
      "print_info: EOT token        = 32000 '<|endoftext|>'\n",
      "print_info: UNK token        = 0 '<unk>'\n",
      "print_info: PAD token        = 32001 '<|pad|>'\n",
      "print_info: LF token         = 13 '<0x0A>'\n",
      "print_info: EOG token        = 32000 '<|endoftext|>'\n",
      "print_info: max token length = 48\n",
      "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
      "load_tensors: layer   0 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer   1 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer   2 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer   3 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer   4 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer   5 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer   6 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer   7 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer   8 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer   9 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  10 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  11 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  12 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  13 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  14 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  15 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  16 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  17 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  18 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  19 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  20 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  21 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  22 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  23 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  24 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  25 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  26 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  27 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  28 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  29 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  30 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  31 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  32 assigned to device Metal, is_swa = 0\n",
      "load_tensors: tensor 'token_embd.weight' (q4_K) (and 0 others) cannot be used with preferred buffer type CPU_REPACK, using CPU instead\n",
      "ggml_backend_metal_log_allocated_size: allocated buffer, size =  3891.30 MiB, ( 7893.33 / 16384.02)\n",
      "load_tensors: offloading 32 repeating layers to GPU\n",
      "load_tensors: offloading output layer to GPU\n",
      "load_tensors: offloaded 33/33 layers to GPU\n",
      "load_tensors:   CPU_Mapped model buffer size =    70.33 MiB\n",
      "load_tensors: Metal_Mapped model buffer size =  3891.29 MiB\n",
      "..................................................................................................\n",
      "llama_context: constructing llama_context\n",
      "llama_context: n_seq_max     = 1\n",
      "llama_context: n_ctx         = 4096\n",
      "llama_context: n_ctx_per_seq = 4096\n",
      "llama_context: n_batch       = 512\n",
      "llama_context: n_ubatch      = 512\n",
      "llama_context: causal_attn   = 1\n",
      "llama_context: flash_attn    = 0\n",
      "llama_context: kv_unified    = false\n",
      "llama_context: freq_base     = 10000.0\n",
      "llama_context: freq_scale    = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M3\n",
      "ggml_metal_init: picking default device: Apple M3\n",
      "ggml_metal_init: GPU name:   Apple M3\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction   = true\n",
      "ggml_metal_init: simdgroup matrix mul. = true\n",
      "ggml_metal_init: has residency sets    = true\n",
      "ggml_metal_init: has bfloat            = true\n",
      "ggml_metal_init: use bfloat            = false\n",
      "ggml_metal_init: hasUnifiedMemory      = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 17179.89 MB\n",
      "ggml_metal_init: loaded kernel_add                                    0x150171720 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_fuse_2                             0x1575ce240 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_fuse_3                             0x15148fe60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_fuse_4                             0x157810f90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_fuse_5                             0x17e4fd390 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_fuse_6                             0x17e4fc9f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_fuse_7                             0x17e4fbd20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_fuse_8                             0x30c8c70b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_row_c4                             0x17e4fad30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_row_c4_fuse_2                      0x30c8c73b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_row_c4_fuse_3                      0x17e4fa5a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_row_c4_fuse_4                      0x17e4f9d70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_row_c4_fuse_5                      0x30c8c7790 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_row_c4_fuse_6                      0x11f02d8c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_row_c4_fuse_7                      0x11f02d4a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_row_c4_fuse_8                      0x1575dd1b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sub                                    0x1575dd470 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sub_row_c4                             0x30c8c7cd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul                                    0x30c8c7f90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_row_c4                             0x30c8c8310 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_div                                    0x30c8c86e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_div_row_c4                             0x30c8c8ab0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_id                                 0x17e4f93e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_f32                             0x150143af0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_f16                             0x157928460 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_i32                             0x17e4f8cb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_i16                             0x157928720 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_scale                                  0x1579289e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_scale_4                                0x1575cc2b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_clamp                                  0x17e4f7c30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_tanh                                   0x30c8c8e80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_relu                                   0x1575cf020 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sigmoid                                0x30c8c91e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu                                   0x1575c9020 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_4                                 0x157928ca0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_erf                               0x17e4f5370 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_erf_4                             0x17e4f4f80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_quick                             0x30c8c95b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_quick_4                           0x17e4f4a90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_silu                                   0x30c8c9a10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_silu_4                                 0x1575c92e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_elu                                    0x17e4f3f60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_abs                                    0x1579273d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sgn                                    0x30c8c9d70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_step                                   0x30c8ca1d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_hardswish                              0x11f02dcd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_hardsigmoid                            0x11f02df90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_exp                                    0x11f02a2e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f16                           0x11f02a5a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f16_4                         0x11f02cc30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f32                           0x157927690 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f32_4                         0x30c8ca590 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                          0x17e4f3510 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x11f02cef0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_f32                           0x157927950 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                           0x157927c10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                          0x157927ed0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                          0x11f02c410 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_0                          0x17e4f2c20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_1                          0x157926520 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q8_0                          0x30c8ca930 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_mxfp4                         0x30c8cacd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                          0x30c8cb0a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                          0x1579267e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                          0x11f02c6d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                          0x11f02e480 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                          0x30c8cb470 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x17e4f22c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x157926aa0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x157926d60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x30c8cb8d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x11f02e740 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x11f02ea00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x11f02ecc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x11f02ef80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x30c8cbc30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_i32                           0x11f02a990 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_rows_f32                           0x30c8cc0d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_rows_f16                           0x17e4f1940 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)\n",
      "ggml_metal_init: loaded kernel_set_rows_q8_0                          0x11f02ac50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_rows_q4_0                          0x157825960 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_rows_q4_1                          0x11f02af10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_rows_q5_0                          0x1579295c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_rows_q5_1                          0x17e4fc530 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_rows_iq4_nl                        0x11f02b1d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rms_norm                               0x11f02b490 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rms_norm_mul                           0x11f02b750 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rms_norm_mul_add                       0x11f02ba10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_l2_norm                                0x11f02be40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_group_norm                             0x30c8cc390 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_norm                                   0x17e4fb4f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_ssm_conv_f32                           0x157929880 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_ssm_scan_f32                           0x157929b40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_ssm_scan_f32_group                     0x157929e00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rwkv_wkv6_f32                          0x17e4f6e00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rwkv_wkv7_f32                          0x157825340 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x11f02f450 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f32_f32_c4                      0x11f02f710 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x30c8cc880 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f32_c4                      0x11f02f9d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x11f02fc90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x15792a0c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x11f02ff50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x157e44790 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x1556347f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x11f030510 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x157815ac0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x11f0307d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_mxfp4_f32                       0x30c8ccb40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x30c8cd0e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x30c8cd3a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x30c8cd660 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x11f030a90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x30c8cd920 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x3085e21f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x30c8cdd80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x11f030d50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x11f031010 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x157828630 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x15792a9c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x1578288f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x30c8ce1e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x30c8ce5b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x11f0312d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x157828bb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x11f031590 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x157828e70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x11f031850 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x11f031b10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x11f031dd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x11f032090 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x11f032350 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x157829130 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_mxfp4_f32_r1_2              0x1578293f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_mxfp4_f32_r1_3              0x11f032610 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_mxfp4_f32_r1_4              0x15781e1f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_mxfp4_f32_r1_5              0x15781e4b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x30c8ce910 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x15781e770 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x30c8cece0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x30c8cf140 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x15781ea30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x15781ecf0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x30c8cf5e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x30c8cf980 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x15781efb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x15781f270 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x30c8cfd50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x15781f610 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x11f0328d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x15781fb50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x11f032b90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x15792ac80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x15792af40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x15781fe10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x1578201d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x11f032e50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x11f033110 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x15792b200 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x11f0333d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x157820630 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x157820a00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x157820dd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x17e4f0cc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x17e4f0f80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x15792b4c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x17e404180 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x15792b780 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x15792c3e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x17e404440 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x15792c6a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x15792c960 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x17e404790 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x17e404bf0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_mxfp4_f32                    0x15792cc20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x17e404fc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x15792cee0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x15792d1a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x17e405500 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x17e4057c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x17e405ac0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x11f033690 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x11f033950 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x11f033c10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x11f033ed0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x17e405e20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x15792d460 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x15792d720 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x11f034190 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x17e4061f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x17e406650 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x15792d9e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x15792dca0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x15792e070 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x17e406a20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x30c8d00b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_mxfp4_f32                       0x30c8d0510 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_mxfp4_f32                       0x15792e440 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x15792e810 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x11f034450 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x11f034710 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x17e406e80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x17e4071e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x17e4075f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x17e407990 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x11f0349d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x11f034c90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x30c8d0870 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x30c8d0cd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x30c8d10a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x11f034f50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x30c8d1470 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_map0_f16                     0x30c8d17d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_map1_f32                     0x30c8d1bd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_f32_f16                      0x11f035360 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_f16_f16                      0x11f035700 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f16                     0x11f035ba0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f16                     0x11f035eb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f16                     0x15792ec70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f16                     0x17e407cf0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f16                     0x17e408150 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_mxfp4_f16                    0x17e408520 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f16                     0x17e4088f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f16                     0x11f036170 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f16                     0x15792f1b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f16                     0x11f0365d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f16                     0x17e408c50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f16                  0x11f036970 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f16                   0x17e4090b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f16                  0x11f036d40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f16                    0x11f037110 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f16                    0x17e409480 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f16                    0x17e409850 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f16                    0x17e409c20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f16                   0x11f0374e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f16                   0x11f0378b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_norm_f32                          0x17e409ff0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_norm_f16                          0x15792f470 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_multi_f32                         0x17e40a350 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_multi_f16                         0x15792f7d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_vision_f32                        0x17e40a7b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_vision_f16                        0x11f037c10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_neox_f32                          0x15792fb70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_neox_f16                          0x15792ffd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_f16                             0x1579303a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_f32                             0x30c8d1f70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_ext_f16                         0x17e40ac10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_ext_f32                         0x30c8d2340 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x157930700 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x157930b60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_upscale_f32                            0x11f0380e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pad_f32                                0x157930ec0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x157931320 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x30c8d26a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_arange_f32                             0x17e40aed0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x1579316f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x157931ac0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_leaky_relu_f32                         0x157931e90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x11f038540 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x17e40b1e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x17e40b680 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x1579322f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x17e40b9b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h192                0x17e40bd10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_hk192_hv128         0x17e40c110 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x157932650 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_hk576_hv512         0x30c8d2be0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x157932a50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x157932db0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x17e40c470 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x11f0388e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x17e40c870 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h192               0x157933180 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_hk192_hv128        0x11f038cb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x17e40cc40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_hk576_hv512        0x17e40d010 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x17e40d3e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x1579336c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x1579339c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x157933d90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x17e40d740 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h192               0x17e40dba0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_hk192_hv128        0x30c8d2ee0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x1579340f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_hk576_hv512        0x1579344f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x157934890 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x157934c60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x30c8d32b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x30c8d3710 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x30c8d3ae0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h192               0x30c8d3eb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_hk192_hv128        0x17e40df00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x157935030 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_hk576_hv512        0x30c8d43f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x17e40e2d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x17e40e730 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x30c8d46b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x17e40eb00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x30c8d4a70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h192               0x157935390 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_hk192_hv128        0x17e40ee60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x1579357f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_hk576_hv512        0x157935bc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x157935f90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x157936360 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x17e40f300 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x17e40f600 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x17e40f9d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h192               0x11f039010 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_hk192_hv128        0x11f0394b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x17e40ff10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_hk576_hv512        0x17e4101d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h64             0x17e410570 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h64            0x11f039850 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h64            0x17e4108d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h64            0x11df067f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h64            0x30c8d4e10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h64            0x30c8d51e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h96             0x11f039bb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h96            0x11f03a010 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h96            0x17e410e10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h96            0x17e4110d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h96            0x17e411470 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h96            0x17e411880 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x30c8d55b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x30c8d5870 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x17e411c90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x17e412030 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x30c8d5c40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x30c8d60a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h192            0x30c8d6470 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h192           0x17e412570 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h192           0x30c8d67d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h192           0x30c8d6c30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h192           0x17e412830 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h192           0x1579367f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_hk192_hv128      0x157936b90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_hk192_hv128      0x11f03a4a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_hk192_hv128      0x17e412b90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_hk192_hv128      0x17e412fa0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_hk192_hv128      0x157936ff0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_hk192_hv128      0x11f03a840 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x11f03ac10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x17e413400 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x30c8d7130 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x30c8d73f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x157937350 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x30c8d7750 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_hk576_hv512      0x30c8d7bb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_hk576_hv512      0x1579376f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_hk576_hv512      0x157937ac0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_hk576_hv512      0x157937e90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_hk576_hv512      0x157938260 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_hk576_hv512      0x30c8d8010 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_f32                                0x30c8d8370 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_i32                                0x11f03af70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                            0x17e413760 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                            0x30c8d8780 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f32                            0x11f03b410 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                            0x17e413bc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x17e413f20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x11f03b6d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x11f03bad0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x11f03bea0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x17e414380 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x17e414750 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x17e414b20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x11f03c200 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x17e414f80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x17e415350 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x17e4156b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x17e415b10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x17e415ee0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x30c8d8ae0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x17e416240 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x1579385c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_concat                                 0x17e4166a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sqr                                    0x17e416a70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sqrt                                   0x157938990 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sin                                    0x157938df0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cos                                    0x30c8d8ef0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_neg                                    0x157939150 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_reglu                                  0x157939520 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_geglu                                  0x157939980 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_swiglu                                 0x157939d50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_swiglu_oai                             0x17e416dd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_geglu_erf                              0x17e417230 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_geglu_quick                            0x30c8d92c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sum_rows                               0x30c8d9690 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mean                                   0x17e417590 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_argmax                                 0x30c8d99e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x15793a1b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x15793a510 | th_max = 1024 | th_width =   32\n",
      "set_abort_callback: call\n",
      "llama_context:        CPU  output buffer size =     0.12 MiB\n",
      "create_memory: n_ctx = 4096 (padded)\n",
      "llama_kv_cache_unified: layer   0: dev = Metal\n",
      "llama_kv_cache_unified: layer   1: dev = Metal\n",
      "llama_kv_cache_unified: layer   2: dev = Metal\n",
      "llama_kv_cache_unified: layer   3: dev = Metal\n",
      "llama_kv_cache_unified: layer   4: dev = Metal\n",
      "llama_kv_cache_unified: layer   5: dev = Metal\n",
      "llama_kv_cache_unified: layer   6: dev = Metal\n",
      "llama_kv_cache_unified: layer   7: dev = Metal\n",
      "llama_kv_cache_unified: layer   8: dev = Metal\n",
      "llama_kv_cache_unified: layer   9: dev = Metal\n",
      "llama_kv_cache_unified: layer  10: dev = Metal\n",
      "llama_kv_cache_unified: layer  11: dev = Metal\n",
      "llama_kv_cache_unified: layer  12: dev = Metal\n",
      "llama_kv_cache_unified: layer  13: dev = Metal\n",
      "llama_kv_cache_unified: layer  14: dev = Metal\n",
      "llama_kv_cache_unified: layer  15: dev = Metal\n",
      "llama_kv_cache_unified: layer  16: dev = Metal\n",
      "llama_kv_cache_unified: layer  17: dev = Metal\n",
      "llama_kv_cache_unified: layer  18: dev = Metal\n",
      "llama_kv_cache_unified: layer  19: dev = Metal\n",
      "llama_kv_cache_unified: layer  20: dev = Metal\n",
      "llama_kv_cache_unified: layer  21: dev = Metal\n",
      "llama_kv_cache_unified: layer  22: dev = Metal\n",
      "llama_kv_cache_unified: layer  23: dev = Metal\n",
      "llama_kv_cache_unified: layer  24: dev = Metal\n",
      "llama_kv_cache_unified: layer  25: dev = Metal\n",
      "llama_kv_cache_unified: layer  26: dev = Metal\n",
      "llama_kv_cache_unified: layer  27: dev = Metal\n",
      "llama_kv_cache_unified: layer  28: dev = Metal\n",
      "llama_kv_cache_unified: layer  29: dev = Metal\n",
      "llama_kv_cache_unified: layer  30: dev = Metal\n",
      "llama_kv_cache_unified: layer  31: dev = Metal\n",
      "llama_kv_cache_unified:      Metal KV buffer size =  2048.00 MiB\n",
      "llama_kv_cache_unified: size = 2048.00 MiB (  4096 cells,  32 layers,  1/1 seqs), K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\n",
      "llama_context: enumerating backends\n",
      "llama_context: backend_ptrs.size() = 3\n",
      "llama_context: max_nodes = 2328\n",
      "llama_context: worst-case: n_tokens = 512, n_seqs = 1, n_outputs = 0\n",
      "graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  1, n_outputs =  512\n",
      "graph_reserve: reserving a graph for ubatch with n_tokens =    1, n_seqs =  1, n_outputs =    1\n",
      "graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  1, n_outputs =  512\n",
      "llama_context:      Metal compute buffer size =   312.01 MiB\n",
      "llama_context:        CPU compute buffer size =    32.01 MiB\n",
      "llama_context: graph nodes  = 1126\n",
      "llama_context: graph splits = 2\n",
      "Metal : EMBED_LIBRARY = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | REPACK = 1 | \n",
      "Model metadata: {'tokenizer.chat_template': \"{% for message in messages %}{% if message['role'] == 'system' %}{{'<|system|>'+ '\\n' + message['content'] + '\\n'}}{% elif message['role'] == 'user' %}{{'<|user|>' + '\\n' + message['content'] + '\\n'}}{% elif message['role'] == 'assistant' %}{{'<|assistant|>' + '\\n' + message['content'] + '<|endoftext|>' + ('' if loop.last else '\\n')}}{% endif %}{% endfor %}\", 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.add_bos_token': 'false', 'tokenizer.ggml.padding_token_id': '32001', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '32000', 'tokenizer.ggml.pre': 'default', 'llama.embedding_length': '4096', 'llama.vocab_size': '32008', 'llama.attention.head_count_kv': '32', 'general.size_label': '7B', 'general.base_model.0.name': 'Granite 7b Base', 'llama.block_count': '32', 'general.base_model.0.organization': 'Ibm', 'general.base_model.count': '1', 'general.quantization_version': '2', 'llama.rope.dimension_count': '128', 'general.license': 'apache-2.0', 'general.base_model.0.repo_url': 'https://huggingface.co/ibm/granite-7b-base', 'llama.attention.head_count': '32', 'llama.context_length': '4096', 'general.file_type': '15', 'general.finetune': 'lab', 'llama.feed_forward_length': '11008', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'general.architecture': 'llama', 'general.basename': 'instructlab-granite', 'llama.rope.freq_base': '10000.000000', 'tokenizer.ggml.bos_token_id': '1', 'general.type': 'model', 'tokenizer.ggml.model': 'llama', 'general.name': 'Instructlab Granite 7b Lab'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {% for message in messages %}{% if message['role'] == 'system' %}{{'<|system|>'+ '\n",
      "' + message['content'] + '\n",
      "'}}{% elif message['role'] == 'user' %}{{'<|user|>' + '\n",
      "' + message['content'] + '\n",
      "'}}{% elif message['role'] == 'assistant' %}{{'<|assistant|>' + '\n",
      "' + message['content'] + '<|endoftext|>' + ('' if loop.last else '\n",
      "')}}{% endif %}{% endfor %}\n",
      "Using chat eos_token: <|endoftext|>\n",
      "Using chat bos_token: <s>\n"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "llm.close()\n",
    "llm = Llama(\n",
    "    model_path=\"./models/granite-7b-lab-Q4_K_M.gguf\",\n",
    "    n_ctx=4096,\n",
    "    n_threads=8,     # Adjust to your CPU cores\n",
    "    n_gpu_layers=50  # If GPU available\n",
    ")\n",
    "llm.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "65c48d9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =   15841.39 ms\n",
      "llama_perf_context_print: prompt eval time =   15840.69 ms /  2319 tokens (    6.83 ms per token,   146.40 tokens per second)\n",
      "llama_perf_context_print:        eval time =   81961.29 ms /  1038 runs   (   78.96 ms per token,    12.66 tokens per second)\n",
      "llama_perf_context_print:       total time =   99138.58 ms /  3357 tokens\n",
      "llama_perf_context_print:    graphs reused =       1005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LLM Response:\n",
      " [ROOT CAUSE]\n",
      "The root cause of this log is the network connectivity issue.\n",
      "\n",
      "[EVIDENCE]\n",
      "The evidence supporting this cause can be found in the following context chunks:\n",
      "\n",
      "- NETWORK: requery, 0, 0, 0, 0, 320, items, fQueryRetries, 1, fLastRetryTimestamp, 520841203.7\n",
      "- <SEQUENCE> NETWORK: requery, 0, 0, 0, 0, 320, items, fQueryRetries, 1, fLastRetryTimestamp, 520841203.7 -> Location icon should now be in state 'Active' -> FA||Url||taskID[2019353410] dealloc -> dnssd\\_clientstub ConnectToServer: connect()-> No of tries: 1 -> [23:24:32.378] <<<< Boss >>>> figPlaybackBossPrerollCompleted: unexpected preroll-complete notification -> Location icon should now be in state 'Inactive' -> KeychainGetICDPStatus: status: off -> ARPT: 697702.656868: AirPort\\_Brcm43xx::powerChange: System Sleep -> tcp\\_connection\\_tls\\_session\\_error\\_callback\\_imp 2210 tcp\\_connection\\_tls\\_session\\_handle\\_read\\_error.790 error 60\n",
      "\n",
      "[SUGGESTED FIXES]\n",
      "To resolve this issue, we can consider the following fixes:\n",
      "\n",
      "1. Check the network connection settings.\n",
      "2. Restart the network device.\n",
      "3. Update the network driver or firmware.\n",
      "\n",
      "[NEXT-PREDICTIONS EXPLANATION]\n",
      "The next predictions can be inferred from the top chunks and the sequence embedded in them. The sequence position and the reason for these events can be explained as follows:\n",
      "\n",
      "- The 'Location icon should now be in state 'Active'\" event is likely to occur after the network connectivity issue is resolved. This is because the location icon represents the network connection status, and once the connection is established, the icon should switch to the 'Active' state.\n",
      "- The 'FA||Url||taskID[2019353410] dealloc' event is likely to occur after the network connection is established. This is because the FA||Url||taskID[2019353410] reference might be associated with the network connection, and once the connection is no longer needed, the reference can be deallocated.\n",
      "- The 'dnssd\\_clientstub ConnectToServer: connect()-> No of tries: 1' event is likely to occur multiple times before the network connection is established. This is because the network connection might fail to establish, and the dnssd\\_clientstub will attempt to connect to the server again with the same settings.\n",
      "- The '[23:24:32.378] <<<< Boss >>>> figPlaybackBossPrerollCompleted: unexpected preroll-complete notification' event is likely to occur when the preroll-complete notification is received during the playback. This is because the preroll-complete notification might indicate that the playback has completed, and the boss should no longer be in the preroll state.\n",
      "- The 'KeychainGetICDPStatus: status: off' event is likely to occur when the ICDP status is turned off. This is because the ICDP status might be used to indicate the network connection status, and once it is turned off, the network connection might not be available.\n",
      "- The 'ARPT: 697702.656868: AirPort\\_Brcm43xx::powerChange: System Sleep' event is likely to occur when the AirPort device is powered off or switched to sleep mode. This is because the powerChange event might indicate that the AirPort device has been powered off or switched to sleep mode, and the network connection might not be available.\n",
      "- The 'tcp\\_connection\\_tls\\_session\\_error\\_callback\\_imp 2210 tcp\\_connection\\_tls\\_session\\_handle\\_read\\_error.790 error 60' event is likely to occur when the TLS session handle read error occurs during the network connection establishment. This is because the TLS session handle read error might indicate that the network connection has failed to establish, and the error needs to be handled.\n",
      "\n",
      "[CONFIDENCE]\n",
      "90%\n",
      "\n",
      "Confidence: 90%\n"
     ]
    }
   ],
   "source": [
    "llm_call = simple_rag_chat_final(question=user_question, index=index, llm=llm, top_k=5, base_retrieval_k=25, use_reranker=False, fuzzy_threshold=0.5)\n",
    "\n",
    "\n",
    "print(\"\\nLLM Response:\\n\", llm_call .get(\"llm_response\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
